<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://smashmath.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://smashmath.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-13T20:53:47+00:00</updated><id>https://smashmath.github.io/feed.xml</id><title type="html">smashmath</title><subtitle>i do math sometimes </subtitle><entry><title type="html">Constant Coefficient ODEs Made Simple with Linear Operators</title><link href="https://smashmath.github.io/blog/linalglinconstcoef/" rel="alternate" type="text/html" title="Constant Coefficient ODEs Made Simple with Linear Operators"/><published>2023-11-11T00:00:00+00:00</published><updated>2023-11-11T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/linalglinconstcoef</id><content type="html" xml:base="https://smashmath.github.io/blog/linalglinconstcoef/"><![CDATA[<p>Last year, I made a post about <a href="../linconstcoef/" target="_blank">this same topic</a>, but since then, I have developed a new way of understanding this topic through the lens of linear algebra which makes many of the seemingly arbitrary choices seem almost stupidly obvious. One time, I was discussing differential equations with a friend of mine who was taking a class in it at the time. He asked me, “why do we know that the solutions to these equations <em>have</em> to be exponentials?” My explanation was not great, but years later I presented the following explanation which I think was much better. In his words, it was “mindblowing”. I’m hoping you will feel similarly. <strong>It does, however, require linear algebra.</strong></p> <p>We will address the following questions in the context of constant coefficient linear ordinary differential equations:</p> \[a_ny^{(n)}+\ldots+a_1y'+a_0y=0\] <ol> <li>Why is our guess for the solution \(e^{\lambda t}\)?</li> <li>Why do we multiply by \(t\) when we have a repeated root of the characteristic polynomial?</li> <li>Why does an \(n\)th order equation have exactly \(n\) linearly independent homogeneous solutions?</li> </ol> <p>And as a bonus, I’m going to explain why the <strong>exponential response formula</strong> is actually absurdly trivial!</p> <h1 id="linear-algebra-recap">Linear Algebra Recap</h1> <p>Let us review some basic linear algebra facts and terminology.</p> <p>A <strong>subspace</strong> is a nonempty subset of a vector space that is closed under linear combinations.</p> <h2 id="linear-transformations">Linear Transformations</h2> <p>A linear transformation \(T\), is a function that preserves linear combinations.</p> <p>\begin{equation} T(c_1x_1+\ldots+c_kx_k)=c_1T(x_1)+\ldots+c_kT(x_k) \end{equation}</p> <p>The derivative \(\frac{d}{dt}\) is an example of a linear transformation. This is by the linearity of differentiation. That is, by the fact that we can pull out constants from derivatives, and that the derivative of a sum is just the sum of derivatives.</p> <p>If \(T(x)=y\), then we say that \(y\) is the <strong>image</strong> of \(x\) under \(T\) (often, we just say the image of \(x\)). The image is unique because \(T\) is a function. We call \(x\) the <strong>preimage</strong> of \(y\) under \(T\). A preimage is not necessarily unique.</p> <p>If the image of an element \(x\) is \(0\) (\(T(x)=0\)), then we say that \(x\) is in the <strong>kernel</strong> of \(T\) (\(x\in\ker(T)\)). The kernel is the set of all preimages of \(0\). We sometimes call the kernel the set of all <strong>homogeneous solutions</strong> to the equation \(T(x)=0\). The kernel is a subspace.</p> <h3 id="preimage-theorem">Preimage Theorem</h3> <p>Assume the equation \(T(x)=b\) (which is asking the question “what is a preimage of b?”) has at least one solution, \(x=x_p\). Then <em>every</em> solution is of the form \(x=x_p+x_h\) where \(x_h\) is an element in \(\ker(T)\). That is, <strong>all preimages of a linear transformation are off from a single particular preimage by something in the kernel</strong>.</p> <p><strong>Proof:</strong> If \(x_0\) is a vector of the form \(x_0=x_p+x_h\), then \(T(x_0)=T(x_p+x_h)=T(x_p)+T(x_h)=b+0=b\). Hence, \(x_0\) is also a preimage of \(b\), showing that any vector of this form is also a solution.</p> <p>Suppose we have some other solution \(x_1\). Then \(T(x_1-x_p)=T(x_1)-T(x_p)=b-b=0\). Hence, \(x_1-x_p\in\ker(T)\), so \(x_1-x_p=x_h\) for some \(x_h\in\ker(T)\), and thus \(x_1=x_p+x_h\), concluding the proof. \(\square\)</p> <p>This last step actually implies that a <strong>general solution</strong> to \(T(x)=b\) is an expression of the form</p> \[x=c_1x_1+\ldots+c_nx_n+x_p\] <p>where \(x_1,\ldots,x_n\) form a basis for \(\ker(T)\) and \(x_p\) is any particular solution. This is because if for any solution \(x-x_p\in\ker(T)\), then by establishing a basis \(x_1,\ldots,x_n\) we are saying that \(x-x_p\) can be written in the form \(c_1x_1+\ldots+c_nx_n\).</p> <p>For the derivative operator, the kernel is just the set of all constant functions. Any preimage under the operator is also just an antiderivative. And one of the important theorems of calculus is that any two antiderivatives differ by a constant, which is exactly what the preimage theorem states for this example.</p> <h2 id="eigenvectors">Eigenvectors</h2> <p>An <strong>eigenvector</strong> of \(T\) is a nonzero vector \(v\) for which \(T\) just scales \(v\). That is, \(T(v)=\lambda v\) for some \(\lambda\in F\). \(\lambda\) is called the <strong>eigenvalue</strong> of \(v\). Note that this means that</p> \[(T-\lambda I)v=0\implies v\in\ker(T-\lambda I)\] <p>As a very mundane but useful fact, if the eigenvalue of \(v\) is nonzero, then we can just divide both sides of \(T(v)=\lambda v\) by \(\lambda\) to get that \(T\left(\frac{v}{\lambda}\right)=v\implies\frac{v}{\lambda}\) is an easy preimage of \(v\).</p> <p>\begin{equation}\label{divide} T(v)=\lambda v\neq0\implies T\left(\frac{v}{\lambda}\right)=v \end{equation}</p> <p>Another important fact is that \(T^n(v)=\lambda^n v\).</p> <p>We call the set of all eigenvectors with eigenvalue \(\lambda\) the <strong>eigenspace of \(\lambda\)</strong>. The <strong>generalized eigenspace of \(\lambda\)</strong> is the set of all generalized eigenvectors which we denote \(E_\lambda\). Recall that generalized eigenvectors are all vectors \(w\) for which</p> \[(T-\lambda I)^kw=0\] <p>for some integer \(k\).</p> <h2 id="linear-operators-and-polynomials">Linear Operators and Polynomials</h2> <p>Given a polynomial \(p(x)=a_0+a_1x+\ldots+a_nx^n\), we define</p> <p>\begin{equation} p(T)=a_0I+a_1T+\ldots+a_nT^n \end{equation}</p> \[\implies p(T)x=a_0x+a_1T(x)+\ldots+a_nT^n(x)\] <p>Here are some insanely important facts</p> <ol> <li>Polynomials in an operator commute. That is, for all polynomials \(p,q\), \(p(T)q(T)=q(T)p(T)\). This includes the specific case of first order factors \((T-aI)(T-bI)=(T-bI)(T-aI)\).</li> <li>The kernel is just the eigenspace of \(0\). That is, finding the kernel of an operator can be done by finding all eigenvectors with eigenvalue zero.</li> <li>If \(v\) is an eigenvector of \(T\) with eigenvalue \(\lambda\), then \(v\) is also an eigenvector of \(p(T)\) with eigenvalue \(p(\lambda)\) (this is a very good thing to prove, and it’s a very short proof).</li> <li>\(p(T)\) can have eigenvectors that are not eigenvectors of \(T\). One example is that \(\begin{pmatrix}1\\0\end{pmatrix}\) is an eigenvector of \(J=\begin{pmatrix}\lambda&amp;1\\0&amp;\lambda\end{pmatrix}\), but \(\begin{pmatrix}0\\1\end{pmatrix}\) is an eigenvector of \((J-\lambda I)^2=\begin{pmatrix}0&amp;0\\0&amp;0\end{pmatrix}\) and not of \(J\).</li> <li>Combining the above three points tells us that finding all eigenvectors of \(T\) with eigenvalue \(\lambda\) such that \(p(\lambda)=0\) will definitely give a subset of the kernel of \(p(T)\). However, it may not necessarily give a full basis for \(\ker(p(T))\).</li> </ol> <h1 id="the-differential-operator">The Differential Operator</h1> <p>We are going to focus in on the differential operator \(D\) defined by</p> <p>\begin{equation} Dy=y’ \end{equation}</p> <p>This is a linear operator as mentioned before. As a note, we will omit the \(I\) when writing something like \((D-\lambda I)\). We will just write \((D-\lambda)\). Further, \(y^{(k)}=D^ky\).</p> <p>Okay, so now, let’s bring everything we talked about above together:</p> <p>The differential equation</p> <p>\begin{equation} a_ny^{(n)}+\ldots+a_1y’+a_0y=g(t) \end{equation}</p> <p>is just the equation \(p(D)y=g(t)\), where \(p(x)=a_nx^n+\ldots+a_1x+a_0\). That is, we are looking for preimages of \(g(t)\) under \(p(D)\), and every solution will be of the form \(y=y_p+y_h\) where \(y_p\) is some preimage of \(g(t)\), and \(y_h\) is any vector in the kernel of \(p(D)\). Note that because the kernel is always a subspace, this gives us the superposition property of homogeneous solutions for free.</p> <p>i.e. If \(y_1,\ldots,y_n\) are solutions to \(p(D)y=0\), then so will \(c_1y_1+\ldots+c_ny_n\).</p> <p>As before, to find the <strong>general solution</strong>, we need a basis \(\{y_1,\ldots,y_k\}\) of the kernel of \(p(D)\). Then, any solution \(y\) can be written as</p> \[y=y_p+c_1y_1+\ldots+c_ky_k\] <p>So how can we start to find a basis for the kernel? And how many vectors will be in it? Well, we know that any eigenvector of \(D\) with eigenvalue \(\lambda\) will be an eigenvector of \(p(D)\) with eigenvalue \(p(\lambda)\). So we can try to find some solutions by solving \(p(\lambda)=0\). And there we go: the characteristic polynomial is now something obvious to try, and we have yet to even <em>mention</em> exponentials.</p> <p>Let \(\lambda\) be any solution to \(p(\lambda)=0\). Now we are looking for eigenvectors \(y\) such that \(Dy=\lambda y\).</p> <h1 id="the-most-important-ode">The Most Important ODE</h1> <p>\begin{equation} y’=\lambda y \end{equation}</p> <p>Yeah, I said it.</p> <p>Now, let’s try to solve this. Well, this differential equation says that the rate of change is proportional to the value of the function. We can see \(y=0\) trivially satisfies the equation, so we can accept that \(y=0\) is a possible solution and just assume that \(y\neq0\) to consider the proportionality constant</p> \[\frac{y'}{y}=\lambda\] <p>But, wait. \(\frac{y'}{y}=\frac{d}{dt}\ln\left\lvert y \right\rvert\). So \(\frac{d}{dt}\ln\left\lvert y \right\rvert=\lambda\). That is,</p> \[\ln\left\lvert y \right\rvert=\lambda t+c\implies \left\lvert y \right\rvert=e^ce^{\lambda t}\] \[\implies y=Ce^{\lambda t}\implies y\in\operatorname{span}(e^{\lambda t})\] <p>Remember, \(y\in\operatorname{span}(e^{\lambda t})\) just means that \(y\) is some linear combination of \(e^{\lambda t}\), which just means \(y=c_1e^{\lambda t}\).</p> <p>We have demonstrated that \(y'=\lambda y\implies y\in\operatorname{span}(e^{\lambda t})\), and I encourage you to verify that \(y'=\lambda y\impliedby y\in\operatorname{span}(e^{\lambda t})\). However, how do we <em>know</em> this provides us with <em>every</em> solution? No, get that Picard–Lindelöf s#!% out of here (it’s beautiful, yes, but not particularly intuitive). Suppose we have two solutions \(y_1,y_2\) with \(y_1\neq0\). Then</p> \[\frac{d}{dt}\left(\frac{y_2}{y_1}\right)=\frac{y_1y_2'-y_2y_1'}{y_1^2}\] \[=\frac{y_1(\lambda y_2)-y_2(\lambda y_1)}{y_1^2}=0\] <p>So \(\frac{y_2}{y_1}\) is just some constant \(C\), which implies that \(y_2=Cy_1\), which implies \(y_2\) is necessarily linearly dependent on \(y_1\). Therefore, we’ve shown that the solution space of \(y'=\lambda y\) (which is just \((D-\lambda)y=0\): the kernel of \((D-\lambda)\)) has dimension exactly 1, and it has a basis \(e^{\lambda t}\). In other words,</p> <p>\begin{equation}\label{kerda} \ker(D-\lambda)=\operatorname{span}(e^{\lambda t}) \end{equation}</p> <p>This demonstrates that <strong>every eigenvector of the differential operator is an exponential function \(Ce^{\lambda t}\)</strong>.</p> <p>Now, we have answered our first question:</p> <p><strong>Why is our guess for the solution \(e^{\lambda t}\)?</strong></p> <p><em>Because exponentials are the unique functions that can be eigenvectors of the differential operator</em>. Therefore, they will also be eigenvectors of the polynomial differential operator.</p> \[p(D)e^{\lambda t}=p(\lambda)e^{\lambda t}\] <p>So if \(p(\lambda)=0\), then \(e^{\lambda t}\) is a solution. That is why we consider</p> \[p(\lambda)=a_n\lambda^n+\ldots+a_1\lambda+a_0=0\] <p>Thus, because the kernel is a subspace closed under linear combinations, we can take a linear combination of exponentials \(e^{\lambda_1t},\ldots,e^{\lambda_nt}\), where \(\lambda_1,\ldots,\lambda_n\) are the roots of \(p(x)\), to say</p> \[y=c_1e^{\lambda_1t}+\ldots+c_ke^{\lambda_nt}\] <p>will be a solution to \(p(D)y=0\). Note: we do not say the <em>general</em> solution quite yet, as any repeated roots will make the set of these exponentials trivially linearly dependent.</p> <p>We remark that \eqref{kerda} equivalently tells us that if \(q(x)\) is a degree one polynomial, then \(\ker(q(D))\) has a basis \(\{e^{\lambda t}\}\) for \(\lambda\) such that \(q(\lambda)=0\), implying that the general solution to \(q(D)y=0\) is \(y=Ce^{\lambda t}\).</p> <p>We actually have enough now to prove that an \(n\)th order equation has homogeneous solution space of dimension exactly \(n\), but I want to take a detour to repeated roots first.</p> <h1 id="repeated-roots">Repeated Roots</h1> <p>i.e. why in the sweet heavenly cheese husk do we multiply by \(t\) of all things to get our other solutions?</p> <p>I brushed over this topic with a meme about reduction of order last time, but I actually have a good explanation this time.</p> <p>Let’s start with the simplest case. What’s the solution to the differential equation</p> \[D^ky=0\implies y^{(k)}=0?\] <p>This is one of the easiest differential equations, because we can just integrate \(n\) times to get</p> \[y=c_1+c_2t+\ldots+c_kt^{k-1}\] <p>Okay, easy enough. Now you may be thinking, “okay, when you put it like that, yeah repeated roots means repeated differentiation so that’s where the \(t\)’s come from <em>in this case</em>. But what about \((D-\lambda)^k\)?”</p> <h2 id="exponential-shift">Exponential Shift</h2> <p>Consider the derivative of \(e^{\lambda t}f(t)\).</p> \[D\left(e^{\lambda t}f(t)\right)=e^{\lambda t}(f'(t)+\lambda f(t))=e^{\lambda t}(D+\lambda)f(t)\] <p>If you’ve taken enough derivatives, you may have caught on to this little shortcut. Since exponentials just get scaled by derivatives, we can just sum up the derivatives of the function multiplying it and scale by the constant appropriately. But let me tell you, this generalizes incredibly. We call this property the <strong>exponential shift</strong>:</p> <p>\begin{equation} De^{\lambda t}=e^{\lambda t}(D+\lambda),\quad e^{\lambda t}D=(D-\lambda)e^{\lambda t} \end{equation}</p> <p>I encourage you to prove that linearity actually guarantees that</p> \[\begin{gather} p(D)e^{\lambda t}=e^{\lambda t}p(D+\lambda)\\ e^{\lambda t}p(D)=p(D-\lambda)e^{\lambda t} \end{gather}\] <p>And this is going to make our lives <em>so</em> much easier. Let’s go back to</p> \[(D-\lambda)y=0\] <p>If we multiply by \(e^{-\lambda t}\), then we can change that pesky \((D-\lambda)\) into just \(D\). That is,</p> \[e^{-\lambda t}(D-\lambda)y=(D-(-\lambda)-\lambda)e^{-\lambda t}y\] \[=D(e^{-\lambda t}y)=0\] <p>And if \(e^{-\lambda t}y\) is in the kernel of \(D\), then it’s just a constant. So \(e^{-\lambda t}y=C\implies y=Ce^{\lambda t}\). If you look at what we did closely, you’ll notice we basically just did an <a href="../integratingfactor/" target="_blank">integrating factor</a> without doing an integrating factor.</p> <p>This is the takeaway: <strong>we can leverage the exponential shift to look at the kernel of just \(D\), which is equivalent to integration</strong>.</p> <h2 id="exponential-shift-into-overdrive">Exponential Shift Into Overdrive</h2> <p>Alright, so let’s answer the question. What happens when we have a repeated root in \(p(x)\) in general? Say that \((x-\lambda)^k\) divides \(p(x)\). That is, \(p(x)=q(x)(x-\lambda)^k\). Then if we multiply \(e^{-\lambda t}\) to \(p(D)y=0\), we get</p> \[e^{-\lambda t}q(D)(D-\lambda)^ky=q(D+\lambda)D^k(e^{-\lambda t}y)\] <p>If we let \(u=e^{-\lambda t}y\), then we get \(q(D+\lambda)D^ku=0\). Thus, any solution to \(D^ku=0\) will also be a solution to \(q(D+\lambda)D^ku=0\). And we know the solution to that is just \(c_1+c_2t+\ldots+c_kt^{k-1}\). Therefore,</p> \[u=e^{-\lambda t}y=c_1+c_2t+\ldots+c_kt^{k-1}\] \[y=e^{\lambda t}\left(c_1+c_2t+\ldots+c_kt^{k-1}\right)\] <p>is a solution to \(p(D)y=0\).</p> <p>And we have now answered our second question, <strong>“Why do we multiply by \(t\) when we have a repeated root of the characteristic polynomial?”</strong></p> <p><em>Because exponentials shift derivatives, and the kernel of repeated differentiation is a polynomial.</em></p> <p>What we have shown also essentially implies that every \(n\)th order differential equation of the form \(p(D)y=g(t)\) can be solved using \(n\) integrating factors. The first of which can be \(e^{-\lambda t}\), where \(\lambda\) is any root of \(p(x)\). After doing \(n\) integrations, we will be left with \(n\) arbitrary constants. This is consistent with the dimension of the kernel of an \(n\)th order linear equation’s solution space being dimension \(n\), but it does not answer why the functions on each arbitrary constant will necessarily be linearly independent. That is what we intend to prove in <a href="#second-order-two-independent-solutions">a later section</a>.</p> <p>But, this actually <em>does</em> imply something else very important: A particular solution to \(p(D)y=Be^{\lambda t}\) is guaranteed to exist, and can be obtained by factoring \(p(D)\), applying integrating factors, and integrating (taking all integration constants to be zero) until it is solved. However, we can actually find this particular solution directly using the exponential response formula, which is what we are going to do next.</p> <p>As a final remark for this section: for constant coefficients, reduction of order <em>is</em> just the exponential shift, but more roundabout. This is because you basically insert an exponential into the equation with \(y=ve^{\lambda t}\), which will cause things to shift. For example, if you’ve done reduction of order on \(y''-2ky'+k^2y=0\), \(y=e^{kt}v\), then you do end up with \(v''=0\). Which is exactly what we would expect:</p> \[(D-k)^2e^{kt}v=e^{kt}D^2v=0\implies v''=0\] <p>Except we didn’t have to do the product rule a bunch of times, plug in, and simplify.</p> <h2 id="exponential-response-formula">Exponential Response Formula</h2> <p>Alright, I’m going to prove and motivate the exponential response formula (ERF) in one line. Please try not to laugh too hard at how absurdly simple it is. By \eqref{divide} (yeah, remember <em>that</em> one?), if \(p(\lambda)\neq0\), then</p> \[p(D)e^{\lambda t}=p(\lambda)e^{\lambda t}\implies p(D)\left(\frac{Be^{\lambda t}}{p(\lambda)}\right)=Be^{\lambda t}\] <p>so \(\frac{Be^{\lambda t}}{p(\lambda)}\) is a particular solution. Okay, that’s all folks. Seeya next time!</p> <hr/> <p>No, but really, it is that simple. We get \(\frac{Be^{\lambda t}}{p(\lambda)}\) as a preimage of \(Be^{\lambda t}\) because it’s an eigenvector, and we can just divide by the eigenvalue when it’s nonzero.</p> <p>We can use some of our techniques here to get the generalized exponential response formula as well. However, it requires a result I don’t feel like proving:</p> <p><strong>\(p^{(k)}(\lambda)=0\) for \(0\leq k&lt;m\) and \(p^{(m)}(\lambda)\neq0\) if and only if \(\lambda\) is a root of \(p(x)\) with multiplicity exactly \(m\). And \(p(x)=q(x)(x-\lambda)^m\) where \(q(\lambda)=\frac{p^{(m)}(\lambda)}{m!}\neq0\)</strong></p> <p>Then for \(p(D)y=q(D)(D-\lambda)^my=Be^{\lambda t}\), we can use our good ol’ exponential shift with \(e^{-\lambda t}\).</p> \[q(D+\lambda)D^m(e^{-\lambda t}y)=B\] <p>Letting \(u=D^m(e^{\lambda t}y)\), we get \(q(D+\lambda)u=B\), where we know that plugging in \(x=0\) into \(q(x+\lambda)\) will give us something nonzero. Hence, we can use our ERF to say a particular solution is \(u_p=\frac{B}{q(0+\lambda)}=\frac{Bm!}{p^{(m)}(\lambda)}\). That is,</p> \[D^m(e^{-\lambda t}y_p)=u_p=\frac{Bm!}{p^{(m)}(\lambda)}\] <p>Now, since we’re just looking for a single particular solution, we can just integrate \(m\) times and take all the constants to be \(0\). And since the \(m\)th integral of \(1\) is \(\frac{t^m}{m!}\), we basically just multiply by \(t^m\) and divide by \(m!\). Thus,</p> <p>\begin{equation}\label{gerf} y_p=\frac{Bt^me^{\lambda t}}{p^{(m)}(\lambda)} \end{equation}</p> <p>and there we go. If \(p^{(k)}(\lambda)=0\) for \(0\leq k&lt;m\) and \(p^{(m)}(\lambda)\neq0\), then \eqref{gerf} is a particular solution to \(p(D)y=Be^{\lambda t}\).</p> <h1 id="nth-order-n-independent-solutions">nth Order. n Independent Solutions</h1> <p>We can prove that \eqref{kerda}’s implication that a first order equation has kernel of dimension exactly one actually generalizes to \(n\)th order equations by induction. \eqref{kerda} is our base case, so let us assume that an \(n\)th order equation has a kernel of exactly degree \(n\). That is, if \(q(x)\) is degree \(n\), then \(\ker(q(D))\) has a basis \(y_1,\ldots,y_n\).</p> <p>Suppose \(p(x)\) is an \(n+1\)th degree polynomial. Then \(p(x)=(x-\lambda)q(x)\) for some \(q(x)\) that is degree \(n\) (this is guaranteed by the fundamental theorem of algebra, meaning \(\lambda\) might be complex). Hence,</p> \[p(D)y=0\implies (D-\lambda)q(D)y=0\] \[\implies q(D)y\in\ker(D-\lambda)\] <p>\eqref{kerda} then tells us that \(q(D)y=Ce^{\lambda t}\) for some \(C\). Let us first consider \(q(D)y=e^{\lambda t}\) (as we can multiply the particular solution by \(C\) to get a solution to the preceding equation). We know we can get some particular solution \(y_p\) to \(q(D)y=e^{\lambda t}\) using the ERF.</p> <p>That is, \(y_p\) is a preimage of \(e^{\lambda t}\) under \(q(D)\), so the general solution to \(q(D)y=e^{\lambda t}\) is \(y=c_1y_1+\ldots+c_ny_n+y_p\), where \(y_1,\ldots,y_n\) is a basis for \(\ker(q(D))\) (guaranteed to be size \(n\) by the inductive hypothesis) by the preimage theorem.</p> <p>But, if \(y_p\) is a preimage of \(e^{\lambda t}\) under \(q(D)\), then \(Cy_p\) is a preimage of \(Ce^{\lambda t}\) under \(q(D)\). Hence, the general solution to \(q(D)y=Ce^{\lambda t}\) is</p> \[y=c_1y_1+\ldots+c_ny_n+Cy_p\] <p>We claim that \(y\) is a solution to \(p(D)y=0\). If we take the image of \(y\) under \(q(D)\), we get \(Ce^{\lambda t}\) as we already defined \(y_1,\ldots,y_n\) to be a basis for its kernel and \(y_p\) to be a preimage of \(e^{\lambda t}\). That is,</p> \[q(D)\left(c_1y_1+\ldots+c_ny_n+Cy_p\right)=Ce^{\lambda t}\] <p>But if we then take the image of the result under \((D-\lambda)\), we will get zero by \eqref{kerda}. Hence, \(y\) is in the kernel of \((D-\lambda)\circ q(D)=(D-\lambda)q(D)=p(D)\).</p> <p>Now, \(y_p\) cannot be linearly dependent \(y_1,\ldots,y_n\), because then it would be in the kernel of \(q(D)\) and thus could not be a preimage of \(e^{\lambda t}\neq0\) under \(q(D)\). Hence, \(\left\{y_1,\ldots,y_n,y_p\right\}\) form a set of \(n+1\) linearly independent vectors in \(\ker(p(D))\), making its dimension at least \(n+1\). We now need to show that there cannot be another linearly independent solution.</p> <p>Suppose that \(y_{n+2}\) is also a solution to \(p(D)y=0\) which is linearly independent with \(\{y_1,\ldots,y_n,y_p\}\). \(y_{n+2}\) can’t be in the kernel of \(q(D)\), because then it would be linearly dependent with \(y_1,\ldots,y_n\). So</p> \[q(D)y_{n+2}\neq0\implies q(D)y_{n+2}\in\ker(D-\lambda)\] <p>That is, \(q(D)y_{n+2}=c_{n+2}e^{\lambda t}\). But, like before, we know that \(c_{n+2}y_p\) will be a preimage of \(c_{n+2}e^{\lambda t}\) under \(q(D)\), so by the preimage theorem \(y_{n+2}=c_{n+2}y_p+y_h\) where \(y_h\in\ker(q(D))\). But, that means \(y_h=d_1y_1+\ldots+d_ny_n\), implying \(y_{n+2}=c_{n+2}y_p+d_1y_1+\ldots+d_ny_n\), contradicting that \(y_{n+2}\) is linearly independent from \(\{y_1,\ldots,y_n,y_p\}\). Therefore, the dimension of \(\ker(p(D))\) is exactly \(n+1\)!</p> <p>This answers our third question, <strong>“Why does an \(n\)th order equation have exactly \(n\) linearly independent homogeneous solutions?”</strong></p> <p>Because a homogeneous \(n\)th order equation is equivalent to a nonhomogeneous \((n-1)\)th order equation, for which the particular solution is necessarily linearly independent from the homogeneous solution in the dimension \(n-1\) kernel. And the particular solution for the reduced equation will then be a homogeneous solution for the \(n\)th order equation. And by the preimage theorem, any other solution will be linearly dependent on the general solution.</p> <h2 id="complex-stuff">Complex Stuff</h2> <p>You may have noticed that I didn’t really specify what all the constants where in any of what I did. That’s because this general theory works for <em>all</em> polynomials, even complex ones. It’s only the very specific real polynomial case where you <em>can</em> always convert complex roots to sines and cosines. In most textbooks, they present this stuff in operator notation, but I’ll just speedrun through it for completion’s sake.</p> <p>If \(p(x)\) is a real polynomial (it has real coefficients, not complex ones), then \(p(D)\) is an operator that maps real functions to real functions. That is, if \(f(x)\) is a real valued function, then \(p(D)f(x)\) is also always real valued.</p> <p>Thus, if \(F(x)=u(x)+iv(x)\), then by linearity of linear transformations</p> \[p(D)F(x)=p(D)u(x)+ip(D)v(x)\] <p>So the real part of \(p(D)F(x)\) is \(p(D)\) applied to the real part of \(F(x)\), and similarly with the imaginary part.</p> <p>Hence, if \(p(D)F(x)=0\), then both the real and imaginary parts must be zero, so \(p(D)u(x)=p(D)v(x)=0\). That is, the real and imaginary parts of any complex element of the kernel are also individually in the kernel.</p> <p>It follows from <a href="../eulersformula/" target="_blank">Euler’s Formula</a> that if \(\alpha=a+bi\) is a root of \(p(x)\), then</p> \[e^{(a+bi)t}=e^{at}\left(\cos(bt)+i\sin(bt)\right)\] <p>will be a solution, so we can take the real and imaginary parts blah blah blah</p> \[y=e^{at}\left(c_1\cos(bt)+c_2\sin(bt)\right)\] <p>will be a solution.</p> <p>So, similar with repeated roots, the case of</p> \[y=(c_1+\ldots+c_kt^{k-1})e^{\alpha t}\] <p>just means two individual sets of solutions</p> \[\begin{align*} y_1=(c_1+\ldots+c_kt^{k-1})e^{at}\cos(bt)\\ y_2=(d_1+\ldots+d_kt^{k-1})e^{at}\sin(bt)\\ \end{align*}\] <p>It may seem like we’re doubling the number of solutions, but nah. This is because if \(p(x)\) is real valued, then any complex root will also have a corresponding complex conjugate root. And, since Euler’s formula basically just makes the conjugate flip the sign of the imaginary part, the two solutions taken from the conjugate solution will be linearly dependent on the two solutions obtained from the original complex root. That is, normally we have</p> \[y=(c_1+\ldots+c_kt^{k-1})e^{\alpha t}+(d_1+\ldots+d_kt^{k-1})e^{\overline\alpha t}\] <p>so we’re still left with \(2k\) solutions in the end.</p> <hr/> <h2 id="final-thoughts">Final Thoughts</h2> <p>I love this perspective. The ideas of “exponentials would be an expected guess”, “to get the other solutions, multiply one solution by an arbitrary function \(v(t)\)”, or “just reduce the \(n\)th order equation down to a system of first order equations” would be mildly intuitive, but only in retrospect. As someone who questions a lot, and wants to know <em>why</em> something is true, the answers felt very contrived.</p> <p>But once we change from a differential equation to a linear operator, suddenly everything is a lot simpler (at least to me). The progression from first order to \(n\)th order is smooth and things just work out exactly the way you would expect them to.</p> <ul> <li>Exponentials are an expected guess because they are eigenvectors of the differential operator.</li> <li>To get the other solutions, we do an exponential shift, which effectively has us multiply by our first given solution.</li> <li>The dimension is \(n\) because each linear differential factor increases the dimension of the kernel by one.</li> </ul> <p>Granted, these justifications <em>only</em> work for constant coefficients. And, in general, reduction of order and reduction to a system of first order equations is necessary to generalize these ideas. But I greatly relish in the idea that we can make a much more elementary argument for this most special of cases that doesn’t end up relying on Picard–Lindelöf.</p> <p>But this is coming from someone who thinks a lot about these subjects, so perhaps it isn’t much easier for you. Especially if you don’t know any linear algebra (and if you got here without knowing linear algebra, uh… why? thank you for reading, but why?) I hope this was helpful, though.</p> <p>Also, this is a heavily condensed version of a chapter from a Linear Algebra + Differential Equations textbook I’m working on in my spare time (for which I have almost none now that I am in a PhD program). I hope you liked it, and maybe you’ll want to check out the textbook when it’s done (ETA: probably never). Alright, thanks for reading.</p> <p><a href="https://youtu.be/Tptx8boeGhE?si=1G60x2ZMJgUPC8Gr" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor Grant</name></author><category term="differential-equations"/><category term="best"/><summary type="html"><![CDATA[Eigenvectors and operators reveal a hidden intuition to the most important type of differential equation]]></summary></entry><entry><title type="html">In Defense of Cramer’s Rule</title><link href="https://smashmath.github.io/blog/cramers/" rel="alternate" type="text/html" title="In Defense of Cramer’s Rule"/><published>2023-06-27T00:00:00+00:00</published><updated>2023-06-27T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/cramers</id><content type="html" xml:base="https://smashmath.github.io/blog/cramers/"><![CDATA[<p>In this post, I am going to defend my dear determinants. I am a fan of them, and I think they are useful. I am <em>not</em> saying that they are always the best tool for the job.</p> <h2 id="the-problems-with-determinants">The Problems with Determinants</h2> <p>Now, to discuss why determinants <em>can</em> be great, I must first discuss why people don’t like them. Sheldon Axler famously hates them. Why? Well, there are a few main reasons:</p> <ul> <li>While very manageable when small, they get out of hand very quickly and become very computationally expensive</li> <li>They are not very sensitive tools. When the determinant is zero, it tells you nothing about the rank besides that it is not full. For example, if the determinant of a \(100\times100\) matrix is zero, you know basically nothing about its rank. And the computation power required to calculate that will really not have been worth it, when you could have just row reduced it to learn the same thing and more with much less work.</li> <li>It is difficult to define them without sounding like a raving lunatic</li> <li>They cannot be easily used for non-square systems</li> </ul> <p>All of these are absolutely valid criticisms. However, in the vast majority of cases where you would be expected to do them by hand, these issues simply do not matter!</p> <h2 id="2x2-cramers-rule">2x2 Cramer’s Rule</h2> <p>\(2\times2\) determinants are <em>easy</em>. The formula is easy enough to do in your head.</p> \[\begin{equation} \det\begin{pmatrix}a&amp;b\\c&amp;d\end{pmatrix}=ad-bc \end{equation}\] <p>And using Cramer’s rule, we can solve any \(2\times2\) system \(\mathbf{A}\mathbf{x}=\mathbf{b}\) by computing exactly three \(2\times2\) determinants!</p> <p>First, we define the notation: For the system \(\mathbf{A}\mathbf{x}=\mathbf{b}\), where \(\mathbf{A}\) is a square matrix, we define \(\mathbf{A}_i\) to be the matrix obtained by replacing the \(i\)th column with \(\mathbf{b}\). For example, if we have</p> \[\begin{equation} \begin{pmatrix}a&amp;b\\c&amp;d\end{pmatrix}\begin{pmatrix}x_1\\x_2\end{pmatrix}=\begin{pmatrix}B_1\\B_2\end{pmatrix} \end{equation}\] <p>then</p> \[\mathbf{A}_1=\begin{pmatrix}B_ 1&amp;b\\B_ 2&amp;d\end{pmatrix},\quad \mathbf{A}_2=\begin{pmatrix}a&amp;B_1\\c&amp;B_2\end{pmatrix}\] <p>Then, Cramer’s rule tells us that if \(\det(A)\neq0\), then the solution to the above system is</p> \[\begin{equation} x_1=\frac{\begin{vmatrix}B_1&amp;b\\B_2&amp;d\end{vmatrix}}{ad-bc},\quad x_2=\frac{\begin{vmatrix}a&amp;B_1\\c&amp;B_2\end{vmatrix}}{ad-bc} \end{equation}\] <p>The more general result being</p> \[\begin{equation} x_i=\frac{\det(\mathbf{A}_i)}{\det(\mathbf{A})} \end{equation}\] <p>And this is actually very quick! First you do \(ad-bc\), and see if it is zero. If it is, then you can check by inspection if the \(\mathbf{b}\) vector is a scalar multiple of the columns of \(\mathbf{A}\). If not, then you can replace the columns with the \(\mathbf{b}\) vector and take the determinant. Personally, what I do, is cover up the columns of an augmented matrix, and negate the first determinant I take.</p> \[\left( \begin{array}{cc|c} a&amp;b&amp;B_1\\ c&amp;d&amp;B_2 \end{array} \right)\] <h3 id="a-2x2-example">A 2x2 example</h3> <p>Consider the system of equations</p> \[\left( \begin{array}{cc|c} 6&amp;-5&amp;26\\ 14&amp;3&amp;2 \end{array} \right)\] <p>Now, this does not look fun to row reduce. Especially if you are like me and avoid fractions like the plague. So, let us use Cramer’s rule instead. I will show the work the way I would do it (avoiding large multiplications)</p> \[\begin{vmatrix}6&amp;-5\\14&amp;3\end{vmatrix}=2\begin{vmatrix}3&amp;-5\\7&amp;3\end{vmatrix}=2(9+35)=88\] <p>This tells us that</p> \[x_1=-\frac{}{88},\quad x_2=\frac{}{88}\] <p>Now, we cover up the first column:</p> \[\begin{vmatrix}-5&amp;26\\3&amp;2\end{vmatrix}=2\begin{vmatrix}-5&amp;13\\3&amp;1\end{vmatrix}=2(-5-39)=-88\] \[\implies x_1=-\frac{-88}{88}=1\] <p>Next, we cover up the second column:</p> \[\begin{vmatrix}6&amp;26\\14&amp;2\end{vmatrix}=2^2\begin{vmatrix}3&amp;13\\7&amp;1\end{vmatrix}=4(3-91)=-4(88)\] \[\implies x_2=\frac{-4(88)}{88}=-4\] <p>Done.</p> <h3 id="a-side-note-about-3x3s">A side note about 3x3s</h3> <p>Certainly there are cases where Cramer’s rule can be optimal for \(3\times3\) systems and larger, if the matrix and \(\mathbf{b}\) vector are particularly simple. But this is so rare, I am not going to bother creating a magical example.</p> <h2 id="a-defense-of-determinants">A Defense of Determinants</h2> <p>Finally, I am going to stay up on my soap box and tell y’all why determinants <em>can</em> be very useful.</p> <p>Mostly, this is focused on the \(2\times2\) and \(3\times3\) cases, which are the vast majority of problems you are expected to do by hand.</p> <ul> <li>They are very easy to compute when small, or at least not difficult to enter into a calculator</li> <li>There are methods to do them in your head</li> <li>They can tell you a lot for small matrices</li> <li>They provide the easiest method for \(3\times3\) inverses</li> <li>They can give you an explicit formula for the solution to a system</li> </ul> <p>I will briefly detail my reasonings for these points.</p> <h3 id="computing-small-determinants">Computing small determinants</h3> <p>As we mentioned above, \(2\times2\) determinants are no problems at all. \(3\times3\)’s are similarly not too bad. If they cannot be done in one’s head, a few steps of row reduction can often bring it to that point. And plugging it into a calculator usually isn’t that bad.</p> <h3 id="information-for-small-matrices">Information for small matrices</h3> <p>Especially for \(3\times3\) matrices, the effort of computing the determinant can have big gains. Specifically, because it is often very difficult to see that a matrix has rank 2 by inspection. Rank 1 is obvious, because every column is a scalar multiple, but seeing that one of the rows is a linear combination of the other two isn’t so easy. Therefore, for the \(3\times3\) case, the determinant’s lack of sensitivity is not a problem. Usually, a zero determinant means that the rank is exactly two.</p> <h3 id="3x3-inverses">3x3 inverses</h3> <p>I <em>will</em> make a blog post about tricks for matrix inverses someday, I promise. And the adjugate matrix is generally the method of choice for \(3\times3\) matrices. You do <em>not</em> have to do that awful row reduction of a super augmented matrix.</p> <h3 id="giving-an-explicit-formula">Giving an explicit formula</h3> <p>Finally, I want to shout out Cramer’s rule for its applications. Variation of parameters for ordinary differential equations relies on Cramer’s rule to make the formula compact.</p> <p>If you only need a single variable from a system of equations, Cramer’s rule can also save you from having to row reduce the entire thing.</p> <p>Additionally, I have used it <a href="../functioninterp/" target="_blank">in my own personal research</a>.</p>]]></content><author><name>Taylor Grant</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[(and determinants)]]></summary></entry><entry><title type="html">Change of Basis</title><link href="https://smashmath.github.io/blog/changeofbasis/" rel="alternate" type="text/html" title="Change of Basis"/><published>2022-10-06T00:00:00+00:00</published><updated>2022-10-06T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/changeofbasis</id><content type="html" xml:base="https://smashmath.github.io/blog/changeofbasis/"><![CDATA[<h3 id="recommended-viewing">Recommended viewing</h3> <p>Before you subject yourself to this post, I recommend watching the following two linear algebra videos from 3blue1brown’s “Essence of linear algebra” series. Grant Sanderson is the superior Grant when it comes to math explanations, and I think it’s entirely possible that watching these two videos will answer pretty much every question you may have.</p> <ul> <li><a href="https://youtu.be/kYB8IZa5AuE" target="_blank">Linear transformations and matrices</a></li> <li><a href="https://youtu.be/P2LTAUO1TdA" target="_blank">Change of basis</a></li> <li>I’d also recommend reading my post on <a href="../columnperspective/" target="_blank"><em>column perspective</em></a>. Understanding column perspective is going to make a lot of this much easier, but it’s not required.</li> </ul> <h3 id="preface">Preface</h3> <p>This topic is so infrequent that when it comes up in my tutoring practice, I have to rederive it for myself as I’m explaining it every single time. I recommend trying to focus on <em>understanding</em> where these formulas comes from, rather than memorizing what they are. In my opinion, if you intuitively understand what a change of basis matrix and a coordinate vector are, and how they interact, it all sort of comes together very naturally.</p> <p>Not to say change of basis is natural, absolutely not. This is possibly the most confounding topic I have had to consistently tutor. I just mean that once you speak the language of change of basis, then forming sentences is relatively intuitive.</p> <h2 id="notation">Notation</h2> <p>This is important to get out of the way because there are <em>so</em> many different notations for all this stuff. So let’s get it all straight right up front so the amount of confusion experienced later is (hopefully) minimized.</p> <p>To be more general, we’re going to talk about the coordinates being in \(F^n\) rather than \(\mathbb{R}^n\), like you will probably see in a lower division class. Just think \(\mathbb{R}^n\) if you have no idea what a field is.</p> <p>We will be denoting vectors as simply lowercase letters \(v\). Vectors are also commonly denoted \(\textbf{v}\) or \(\vec{v}\), but this is linear algebra and there are a ton of vectors and it’s a lot easier to type “v” than it is “\textbf{v}” or “\vec{v}” (and I am a lazy person). For a similar reason, we won’t be distinguishing between column vectors and row vectors. As far as we are concerned here,</p> \[(x,y)=\begin{pmatrix}x\\y\end{pmatrix}\] <p>The vectors in this post are lazy and generally prefer to lay down on their sides. They will stand up when they need to be multiplied by a matrix.</p> <h3 id="the-standard-basis">The standard basis</h3> <p>We denote the standard basis “\(\varepsilon\)”. In the case of \(F^n\), we say \(\varepsilon=\{e_1,\ldots,e_n\}\), where \(e_i\) is the \(i\)th column of the \(n\times n\) identity matrix (that is, zeros everywhere except for a \(1\) in the \(i\)th entry). For polynomial rings \(\varepsilon=\{1,x,\ldots,x^n\}\), and so on.</p> <h3 id="transformation-matrices-and-coordinate-vectors">Transformation matrices and coordinate vectors</h3> <ul> <li>The coordinate vector for a vector \(v\) with respect to a basis \(\beta\) is denoted \([v]_\beta\).</li> <li>The matrix for a linear transformation \(T:V\to V\) with respect to a basis \(\beta\) is denoted \([T]_\beta\)</li> <li>The matrix for a linear transformation \(T:V\to W\) with respect to bases \(\alpha\) of \(V\) and \(\beta\) of \(W\) is denoted \([T]_\alpha^\beta\)</li> <li>The matrix for a linear transformation \(T:V\to W\) with respect to <strong>standard</strong> bases \(\varepsilon_v\) of \(V\) and \(\varepsilon_w\) of \(W\) is denoted \([T]_\varepsilon^\varepsilon\)</li> </ul> <h3 id="change-of-basis-matrices">Change of basis matrices</h3> <p>For “the matrix which changes coordinates from a basis \(\alpha\) to a basis \(\beta\)”, there are a bunch of different notations. Here are a few:</p> \[[I]_\alpha^\beta,\quad P_{\alpha\to\beta},\quad P_{\beta\gets\alpha}\] <p>We will use \(P_{\beta\gets\alpha}\) for this post, because I like it best. To explain why, I am going to show a formula which will be explained later with all three different versions</p> \[\begin{array}{ccc} [I]_\alpha^\beta&amp;=&amp;[I]_\varepsilon^\beta[I]_\alpha^\varepsilon\\ P_{\alpha\to\beta}&amp;=&amp;P_{\varepsilon\to\beta} P_{\alpha\to\varepsilon}\\ P_{\beta\gets\alpha}&amp;=&amp;P_{\beta\gets\varepsilon} P_{\varepsilon\gets\alpha}\\ \end{array}\] <p>Personally, I like the third one because you can easily read it right to left (which is important because transformations are always applied right to left). The second one is jarring for me because it mixes right to left with left to right. And while the first one isn’t that bad, it’s a little jumpy and dense, in my opinion. Use whatever you like, but I will use the third.</p> <h2 id="how-coordinates-work">How coordinates work</h2> <h3 id="transformation-matrix-definition">Transformation matrix definition</h3> <p>A change of basis is just another linear transformation. So it is worth at least writing out the general formula for constructing a transformation matrix. <em>Rarely</em> it is easier to directly compute a change of basis matrix using the following formulation. However, it is slightly more common that computing \([T]_\alpha^\beta\) directly is faster, because the systems of equations that come up when finding it may have very obvious solutions (for example, if a transformation just scales or permutes vectors. This happens when using an eigenbasis). For this reason, <strong>it is often worth applying the transformation to the basis vectors of the domain and seeing if there is an obvious relationship between the outputs and the vectors in the basis of the codomain.</strong></p> <p>If \(T:V\to W\) is a linear transformation, and \(\alpha=(v_1,\ldots,v_n)\) is a basis for \(V\), then the matrix for the transformation \(T\) with respect to the bases \(\alpha\) of \(V\) and \(\beta\) of \(W\) is given by the \(\dim(W)\times n\) matrix</p> \[\begin{equation}\label{transformation} [T]_\alpha^\beta=\Big([T(v_1)]_\beta\quad\cdots\quad [T(v_n)]_\beta\Big) \end{equation}\] <h3 id="linear-transformation-example">Linear transformation example</h3> <p>Here is an example where you don’t actually need to do change of basis, really.</p> <p>Consider the transformation \(T:P_2(F)\to F^2\) defined by</p> \[T(a_0+a_1x+a_2x^2)=\begin{pmatrix}2a_1-a_2\\a_0+3a_1-3a_2\end{pmatrix}\] <p>Find the matrix for the transformation with respect to the bases \(\alpha=\{1+x+x^2,1+x^2,2+x+x^2\}\) and \(\beta=\left\{\begin{pmatrix}1\\1\end{pmatrix},\begin{pmatrix}1\\2\end{pmatrix}\right\}\).</p> <p>First, we can apply \(T(\alpha)\) just to see what happens. You can either directly plug in each basis vector into the transformation, or you could find the matrix for the transformation with respect to the standard bases and apply that to the coordinate vectors. We will do the latter as practice:</p> \[T(1)=\begin{pmatrix}0\\1\end{pmatrix},\quad T(x)=\begin{pmatrix}2\\3\end{pmatrix},\quad T(x^2)=\begin{pmatrix}-1\\-3\end{pmatrix}\] <p>Thus, \([T]_\varepsilon^\varepsilon=\begin{pmatrix}0&amp;2&amp;-1\\1&amp;3&amp;-3\end{pmatrix}\)</p> <p>Applying that matrix to the coordinate vectors of \(\alpha\) with respect to the standard basis (\(P_{\varepsilon\gets\alpha}\))</p> \[\begin{pmatrix}0&amp;2&amp;-1\\1&amp;3&amp;-3\end{pmatrix}\begin{pmatrix}1&amp;1&amp;2\\1&amp;0&amp;1\\1&amp;1&amp;1\end{pmatrix}=\begin{pmatrix}1&amp;-1&amp;1\\1&amp;-2&amp;2\end{pmatrix}\] <p>And as it so happens, the columns are just scalar multiples of our \(\beta\) basis vectors (wow it’s almost like this example was designed for that to happen). We can see that</p> \[T(\alpha_1)=1\beta_1,\quad T(\alpha_2)=-1\beta_2,\quad T(\alpha_3)=1\beta_2\] <p>Therefore, the matrix \([T]_\alpha^\beta\) is just</p> \[[T]_\alpha^\beta=\begin{pmatrix}1&amp;0&amp;0\\0&amp;-1&amp;1\end{pmatrix}\] <p>I encourage you to try this example with the tools later outlined in this post. It isn’t <em>that</em> bad, but the purpose of this example is to let you know that there are often simpler ways to go about these problems. You shouldn’t always bust out the formula just because you <em>can</em>.</p> <h3 id="change-of-basis-matrix-properties">Change of basis matrix properties</h3> <p>Just to get this out of the way because it’s quick, change of basis matrices are basically designed to have the following property</p> \[\begin{equation}\label{changebasis} P_{\beta\gets\alpha}[v]_\alpha=[v]_\beta \end{equation}\] <p>This is another reason I like the right to left notation. The \(\alpha\)s are on the same side and seem to cancel out.</p> <p>We also have a property of when you do successive changes of basis, it’s all just one big change of basis. That is, going from \(\alpha\) to \(\beta\) and then from \(\beta\) to \(\gamma\) is the same as just going from \(\alpha\) to \(\gamma\).</p> \[\begin{equation}\label{basiscombine} P_{\gamma\gets\beta}P_{\beta\gets\alpha}=P_{\gamma\gets\alpha} \end{equation}\] <p>One more important property that follows directly from \eqref{changebasis} (by multiplying the inverse matrix to both sides) is</p> \[\begin{equation}\label{basisinverse} P_{\alpha\gets\beta}=(P_{\beta\gets\alpha})^{-1} \end{equation}\] <p>That is, to reverse the change of basis, you just invert the matrix.</p> <h3 id="wait-what-is-a-coordinate-vector">Wait what IS a coordinate vector?</h3> <p>Given a vector \(v=(1,1)\) and the basis \(\beta=\{(1,-1),(-1,2)\}\), the coordinate vector of \(v\) with respect to the basis \(\beta\) is</p> \[[v]_\beta=(3,2)\] <p>But what does that <em>mean</em> and how could we find that?</p> <p>The idea is that if we have a basis \(\alpha=\{v_1,\ldots,v_n\}\), and \([x]_\alpha=(c_1,\ldots,c_n)\), then that is defined to mean</p> \[\begin{equation}\label{coordinate} x=c_1v_1+\ldots+c_nv_n \end{equation}\] <p>In words, \([x]_\alpha=(c_1,\ldots,c_n)\) tells us that “to get the vector \(x\), we need \(c_1\) of \(v_1\), \(c_2\) of \(v_2\), …, and \(c_n\) of \(v_n\)”.</p> <p>For the example above, we can observe that it’s true that</p> \[\begin{pmatrix}1\\1\end{pmatrix}= 3\begin{pmatrix}1\\-1\end{pmatrix} +2\begin{pmatrix}-1\\2\end{pmatrix}\] <p>The question becomes, then, “how could we find the numbers \(3\) and \(2\)? Don’t worry, we will answer that <a href="#computing-coordinate-vectors" target="_blank">in time</a>.</p> <p>One observation to make is that for all \(v\in F^n\), \(v=[v]_\varepsilon\). To show an example in \(F^2\), where \(\varepsilon=\left\{\begin{pmatrix}1\\0\end{pmatrix},\begin{pmatrix}0\\1\end{pmatrix}\right\}\).</p> \[\begin{pmatrix}x\\y\end{pmatrix}= x\begin{pmatrix}1\\0\end{pmatrix} +y\begin{pmatrix}0\\1\end{pmatrix}\] <p>Which is to say</p> \[\left[\begin{pmatrix}x\\y\end{pmatrix}\right]_\varepsilon= \begin{pmatrix}x\\y\end{pmatrix}\] <p>In words: “every vector in \(F^n\) is its own coordinate vector with respect to the standard basis”. In a sense, that is <em>why</em> it’s the standard basis.</p> <p>This gives us a method to compute coordinate vectors</p> \[[x]_\beta=(P_{\varepsilon\gets\beta})^{-1}x\] <p>We will see in the next section that finding the matrix \(P_{\varepsilon\gets\beta}\) is actually very easy.</p> <h3 id="coordinate-vectors-with-matrices">Coordinate vectors with matrices</h3> <p>Now, if you’re comfortable with <a href="../columnperspective/" target="_blank"><em>column perspective</em></a>, you may have seen \eqref{coordinate} and thought, “that looks like matrix multiplication!” If so, yes, it is! If you have no idea what I’m talking about, basically</p> \[c_1v_1+\ldots+c_nv_n= \Bigg(v_1\quad \cdots\quad v_n\Bigg) \begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}\] <p>For a more concrete example, take the previous example of \(v=(1,1)\) and the basis \(\beta=\{(1,-1),(-1,2)\}\), for which \([v]_\beta=(3,2)\).</p> \[3\begin{pmatrix}1\\-1\end{pmatrix} +2\begin{pmatrix}-1\\2\end{pmatrix}= \begin{pmatrix}1&amp;-1\\-1&amp;2\end{pmatrix}\begin{pmatrix}3\\2\end{pmatrix}= \begin{pmatrix}1\\1\end{pmatrix}\] <p>It appears that \(\begin{pmatrix}1&amp;-1\\-1&amp;2\end{pmatrix}\) is the matrix for which</p> \[\begin{pmatrix}1&amp;-1\\-1&amp;2\end{pmatrix}[v]_\beta=[v]_\varepsilon\] <p>And it is! \(\begin{pmatrix}1&amp;-1\\-1&amp;2\end{pmatrix}\) <em>is</em> the change of basis matrix \(P_{\varepsilon\gets\beta}\).</p> <p><strong>For a given basis \(\beta=\{v_1,\ldots,v_n\}\), the change of basis matrix \(P_{\varepsilon\gets\beta}\) is constructed by just sticking the vectors into the columns.</strong></p> <p>So, for example, if the basis is \(B=\{(1,1,1),(1,-1,0),(1,1,-2)\}\), then the change of basis matrix \(P_{\varepsilon\gets B}\) is just</p> \[\begin{pmatrix}1&amp;1&amp;1\\1&amp;-1&amp;1\\1&amp;0&amp;-2\end{pmatrix}\] <h3 id="computing-coordinate-vectors">Computing coordinate vectors</h3> <p>A few sections ago, we stated the following formula for a coordinate vector</p> \[\begin{equation}\label{coordinatecompute} [x]_\beta=(P_{\varepsilon\gets\beta})^{-1}x \end{equation}\] <p>Now that we know how to get \(P_{\varepsilon\gets\beta}\) (put the vectors as the columns), we can say more directly that the coordinate vector for a basis \(\beta=\{v_1,\ldots,v_n\}\) can be computed by</p> \[\begin{equation}\label{coordinatecomputetwo} [x]_\beta=\bigg(v_1\quad \cdots\quad v_n\bigg)^{-1}x \end{equation}\] <p>Column perspective makes this formula very intuitive, as the definition of a coordinate vector is essentially to be the solution to the equation</p> \[c_1v_1+\ldots+c_nv_n=x\] <p>which can be expressed as the matrix equation</p> \[\Bigg(v_1\quad \cdots\quad v_n\Bigg)\begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}=x\] <h3 id="coordinate-vector-example">Coordinate vector example</h3> <p>Let’s calculate the coordinate vector from before: \(v=(1,1)\) with the basis \(\beta=\left\{\begin{pmatrix}1\\-1\end{pmatrix},\begin{pmatrix}-1\\2\end{pmatrix}\right\}\). We have that</p> \[P_{\varepsilon\gets\beta}=\begin{pmatrix}1&amp;-1\\-1&amp;2\end{pmatrix} \implies P_{\beta\gets\varepsilon}=\begin{pmatrix}2&amp;1\\1&amp;1\end{pmatrix}\] <p>using the formula of \(\begin{pmatrix}a&amp;b\\c&amp;d\end{pmatrix}^{-1}=\frac{1}{ad-bc}\begin{pmatrix}d&amp;-b\\-c&amp;a\end{pmatrix}\). Thus,</p> \[[v]_\beta=P_{\beta\gets\varepsilon}v=\begin{pmatrix}2&amp;1\\1&amp;1\end{pmatrix}\begin{pmatrix}1\\1\end{pmatrix}=\begin{pmatrix}3\\2\end{pmatrix}\] <h2 id="how-to-change-bases">How to change bases</h2> <p>We will be going into how to directly compute a general change of basis matrix <a href="#Finding-change-of-basis-matrices">in a later section</a>. This is a more practical formulation which works better for two and three dimensional vector spaces.</p> <p>First, as an analogy, let’s say you want to translate something from French into German, but you don’t speak either all that well. It would be a lot easier if you could translate the sentence into English first, and then translate that English sentence into German (assuming you are more comfortable with \(\varepsilon\)nglish). That is the fundamental idea behind the following corollary to \eqref{basiscombine} (which is <em>incredibly</em> useful).</p> \[\begin{equation}\label{ezchange} P_{\beta\gets\alpha}=P_{\beta\gets\varepsilon}P_{\varepsilon\gets\alpha}=(P_{\varepsilon\gets\beta})^{-1}P_{\varepsilon\gets\alpha} \end{equation}\] <p>Why are these formulas great, you may ask? Well, it’s because the matrix \(P_{\varepsilon\gets\alpha}\) is <em>really</em> easy to find (it’s just the vectors as the columns!), while \(P_{\beta\gets\alpha}\) is <em>really</em> computationally intensive to find directly if neither \(\alpha\) or \(\beta\) are the standard basis.</p> <h3 id="change-of-basis-example">Change of basis example</h3> <p>Let’s find the matrix, for example, which changes from the basis \(\alpha=\left\{\begin{pmatrix}2\\1\end{pmatrix},\begin{pmatrix}3\\2\end{pmatrix}\right\}\) to the basis \(\beta=\left\{\begin{pmatrix}1\\1\end{pmatrix},\begin{pmatrix}1\\2\end{pmatrix}\right\}\).</p> <p>The matrix with the vectors as the columns will give us the matrix which changes from the basis to the standard basis.</p> \[P_{\varepsilon\gets\alpha}=\begin{pmatrix}2&amp;3\\1&amp;2\end{pmatrix}\] \[P_{\varepsilon\gets\beta}=\begin{pmatrix}1&amp;1\\1&amp;2\end{pmatrix}\implies P_{\beta\gets\varepsilon}=\begin{pmatrix}2&amp;-1\\-1&amp;1\end{pmatrix}\] <p>Using our formula of \(P_{\beta\gets\alpha}=P_{\beta\gets\varepsilon}P_{\varepsilon\gets\alpha}\)</p> \[P_{\beta\gets\alpha}= \begin{pmatrix}2&amp;-1\\-1&amp;1\end{pmatrix} \begin{pmatrix}2&amp;3\\1&amp;2\end{pmatrix}= \begin{pmatrix}3&amp;4\\-1&amp;-1\end{pmatrix}\] <p>How do we know we’re right, though? Basically, you just need to ensure that the columns of your resulting matrix \(\begin{pmatrix}3\\-1\end{pmatrix},\begin{pmatrix}4\\-1\end{pmatrix}\) are the correct coordinates of the old basis (\(\alpha\)) vectors in the new basis (\(\beta\)):</p> \[\begin{array}{cccccc} 3\begin{pmatrix}1\\1\end{pmatrix}&amp;-&amp;\begin{pmatrix}1\\2\end{pmatrix}&amp;=&amp;\begin{pmatrix}2\\1\end{pmatrix}\\ 4\begin{pmatrix}1\\1\end{pmatrix}&amp;-&amp;\begin{pmatrix}1\\2\end{pmatrix}&amp;=&amp;\begin{pmatrix}3\\2\end{pmatrix} \end{array}\] <p>And those are indeed the vectors of \(\alpha\)!</p> <p>Note that (by column perspective) this computation is equivalent to multiplying \(P_{\varepsilon\gets\beta}P_{\beta\gets\alpha}\) and making sure you get \(P_{\varepsilon\gets\alpha}\).</p> <h2 id="transformations-on-other-bases">Transformations on other bases</h2> <p>One of the most common applications of change of basis is to find the matrix of a transformation with respect to other bases.</p> <h3 id="linear-operators">Linear operators</h3> <p>One of the most common cases is that you have a matrix for a linear operator \(T:V\to V\) with respect to some known basis \(\alpha\) of \(V\) (usually the standard basis \(\alpha=\varepsilon\)), and want to find the matrix with respect to another basis of \(V\), \(\beta\). The formula is as follows:</p> \[\begin{equation} [T]_\beta=P_{\beta\gets\alpha}[T]_\alpha P_{\alpha\gets\beta} \end{equation}\] <p>The basic idea is this: suppose you cannot speak French, so you are using a translator to speak to your eight-year-old nephew who only speaks French. In the conversation, your nephew says something in French, which the translator has to convey to you in English (\(P{\text{Eng}\gets\text{Fr}}\)). Then, you can respond in English \([T]{\text{Eng}}\). But, for your nephew to understand what you said, the translator must translate it back into French (\(P{\text{Fr}\gets\text{Eng}}\)). Giving the full “conversation” as</p> \[[T]_{\text{Fr}}=P_{\text{Fr}\gets\text{Eng}}[T]_{\text{Eng}}P_{\text{Eng}\gets\text{Fr}}\] <h3 id="linear-operator-example">Linear operator example</h3> <p>Take the transformation which is a reflection about the line \(y=x\). That is, \(T:F^2\to F^2,\quad T(x,y)=(y,x)\). From the definition of \(T(x,y)=(y,x)\), you can verify very easily that the matrix for this transformation on the standard basis is</p> \[[T]_\varepsilon=\begin{pmatrix}0&amp;1\\1&amp;0\end{pmatrix}\] <p>Let’s say that I want the matrix for the transformation on the basis \(\beta=\left\{\begin{pmatrix}1\\1\end{pmatrix},\begin{pmatrix}1\\2\end{pmatrix}\right\}\) (from before). We previously computed the change of basis matrices between \(\beta\) and the standard basis to be</p> \[P_{\varepsilon\gets\beta}=\begin{pmatrix}1&amp;1\\1&amp;2\end{pmatrix},\quad P_{\beta\gets\varepsilon}=\begin{pmatrix}2&amp;-1\\-1&amp;1\end{pmatrix}\] <p>This tells us that the transformation matrix with respect to \(\beta\), \([T]_\beta\), should be</p> \[[T]_\beta=P_{\beta\gets\varepsilon}[T]_\varepsilon P_{\varepsilon\gets\beta}\] \[[T]_\beta=\begin{pmatrix}2&amp;-1\\-1&amp;1\end{pmatrix}\begin{pmatrix}0&amp;1\\1&amp;0\end{pmatrix}\begin{pmatrix}1&amp;1\\1&amp;2\end{pmatrix}=\begin{pmatrix}1&amp;3\\0&amp;-1\end{pmatrix}\] <p>We can check that this is indeed the correct answer by transforming the basis vectors of \(\beta\).</p> \[\begin{array}{cccccc} T\begin{pmatrix}1\\1\end{pmatrix}&amp;=&amp;\begin{pmatrix}1\\1\end{pmatrix}&amp;=&amp;1\begin{pmatrix}1\\1\end{pmatrix}&amp;+&amp;0\begin{pmatrix}1\\2\end{pmatrix}\\ T\begin{pmatrix}1\\2\end{pmatrix}&amp;=&amp;\begin{pmatrix}2\\1\end{pmatrix}&amp;=&amp;3\begin{pmatrix}1\\1\end{pmatrix}&amp;-&amp;1\begin{pmatrix}1\\2\end{pmatrix} \end{array}\] <p>We can confirm that the coordinate vector for \(T(v_1)=\begin{pmatrix}1\\1\end{pmatrix}\) with respect to the basis \(\beta\) is \(\begin{pmatrix}1\\0\end{pmatrix}\), the first column of \([T]_\beta\), and similarly \([T(v_2)]_\beta=\begin{pmatrix}3\\-1\end{pmatrix}\), the second column of \([T]_\beta\).</p> <h3 id="v-to-w">V to W</h3> <p>If \(T:V\to W\) is a linear transformation to a different vector space \(W\), then you have to deal with two different bases for each space: let’s say \(\alpha,\alpha'\) for \(V\), and \(\beta,\beta'\) for \(W\).</p> <p>Here’s the thing… A formula for \([T]_{\alpha'}^{\beta'}\) given \([T]_\alpha^\beta\) is simply <em>not</em> worth memorizing, in my opinion. So, instead of just giving it to you, I’m going to explain the intuition behind what the formula <em>does</em>. Hopefully, that can give you the capability to come up with the formula on your own.</p> <p>We can transform \(\alpha\) vectors in terms of \(\beta\) vectors using \([T]_\alpha^\beta\), and we want to transform \(\alpha'\) vectors in terms of \(\beta'\) vectors. Thus, we could translate our input of \(\alpha'\) vectors to \(\alpha\) vectors first (\(P_{\alpha\gets\alpha'}\)), and apply the transformation which gives us the results in terms of \(\beta\) vectors (\([T]_\alpha^\beta\)). Finally, to get things in terms of \(\beta'\) vectors, we translate one last time from \(\beta\) to \(\beta'\) (\(P_{\beta'\gets\beta}\)). Putting it all together:</p> \[\begin{equation} [T]_{\alpha'}^{\beta'}=P_{\beta'\gets\beta}[T]_\alpha^\beta P_{\alpha\gets\alpha'} \end{equation}\] <p>Imagine you have a magical French to German translation machine, and you want to translate English to Spanish. Then you could put the magical machine between intermediary machines that take English to French and German to Spanish. That is, translate English to what your magical machine takes in (French), so that it can take the input. Then, the output of the magical machine will be German. So, we take the magical machine output of German, and translate it to Spanish, which is what we want. The resulting combination of the three machines is something that takes in English, and outputs something in Spanish.</p> \[[T]_\text{Eng}^\text{Spa}=P_{\text{Spa}\gets\text{Ger}}[T]_\text{Fr}^\text{Ger} P_{\text{Fr}\gets\text{Eng}}\] <h2 id="finding-change-of-basis-matrices">Finding change of basis matrices</h2> <p>While \eqref{ezchange} is great for two and three dimensional vector spaces (because \(2\times2\) and \(3\times3\) matrix inverses are relatively easy), if you have a larger vector space, it can be quite difficult to invert the matrix and multiply it all out. Instead, there is a direct process that uses row reduction of a super-augmented matrix instead.</p> <p>To find \(P_{\beta\gets\alpha}\),</p> \[\begin{eqnarray} \big[\begin{array}{c|c} P_{\varepsilon\gets\beta}&amp;P_{\varepsilon\gets\alpha} \end{array}\big] &amp;\quad\underrightarrow{\text{row reduce}}\quad&amp; \big[\begin{array}{c|c} I&amp;P_{\beta\gets\alpha} \end{array}\big]\\ \big[\begin{array}{c|c} \text{new basis}&amp;\text{old basis} \end{array}\big] &amp;\quad\underrightarrow{\text{row reduce}}\quad&amp; \big[\begin{array}{c|c} I&amp;P_{\text{new}\gets\text{old}} \end{array}\big] \end{eqnarray}\] <p>It’s similar to finding the inverse of a matrix, but instead we have a non-identity matrix on both sides.</p> <p>If you want to know why this works, there are two explanations. If you don’t, then idk the post is over you can leave now.</p> <ol> <li>You can get to this by solving \(n\) systems of equations at once: trying to find the coordinate vectors of the vectors in \(\beta\) with respect to the vectors in \(\alpha\)</li> <li>Using <a href="../columnperspective/" target="_blank"><em>column perspective</em></a>, you can see that if you multiply the matrix by \((P_{\varepsilon\gets\alpha})^{-1}\), you get \((P_{\varepsilon\gets\beta})^{-1}\big[\begin{array}{c|c} P_{\varepsilon\gets\beta}&amp;P_{\varepsilon\gets\alpha} \end{array}\big]= \big[\begin{array}{c|c} (P_{\varepsilon\gets\beta})^{-1}P_{\varepsilon\gets\beta}&amp;P_{\beta\gets\varepsilon}P_{\varepsilon\gets\alpha} \end{array}\big]= \big[\begin{array}{c|c} I&amp;P_{\alpha\gets\beta} \end{array}\big]\) and, if you didn’t know, row reducing an invertible matrix is exactly equivalent to multiplying on the left by the inverse.</li> </ol> \[A^{-1}\big[\begin{array}{c|c} A&amp;I \end{array}\big]= \big[\begin{array}{c|c} A^{-1}A&amp;A^{-1}I \end{array}\big]= \big[\begin{array}{c|c} I&amp;A^{-1} \end{array}\big]\] <p>(so that’s why that works if you were wondering)</p> <p><a href="https://youtu.be/GxPSApAHakg" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor Grant</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[the most confusing thing in linear algebra (don't @ me)]]></summary></entry><entry><title type="html">Linear Constant Coefficient ODEs</title><link href="https://smashmath.github.io/blog/linconstcoef/" rel="alternate" type="text/html" title="Linear Constant Coefficient ODEs"/><published>2022-07-21T00:00:00+00:00</published><updated>2022-07-21T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/linconstcoef</id><content type="html" xml:base="https://smashmath.github.io/blog/linconstcoef/"><![CDATA[<p>Ah, “second order constant coefficient linear homogeneous ordinary differential equations”. What a wonderfully concise name for one of the most common types of problems in differential equations.</p> <p>This is, in my opinion, the most important topic in an intro to ODEs class. Probably the best thing about these kind of equations is that they are easily solvable, and can be thoroughly understood. The same cannot be said for most ODEs, even linear ones!</p> <p>We will start with second order homogeneous equations, and then briefly discuss how these topics generalize to higher order equations. Finally, I will leave some resources to discuss nonhomogeneous equations (or make another post on that later).</p> <h2 id="intro">Intro</h2> <p>First, we need to talk about exponentials, because they are <em>crucial</em>. Specifically, what I am going to call a “simple (complex) exponential”. A simple exponential is an exponential function like</p> \[y=Ce^{\lambda t}\] <p>Where \(\lambda\) is some complex number. That’s right! We are going to definitely need complex numbers for this topic. But let that be for later. For now, you can imagine \(\lambda\) being some real number like \(2\).</p> <p>For now, just observe that \(y'=\lambda Ce^{\lambda t}=\lambda y\). We can rearrange that to say that</p> \[y'-\lambda y=0\] <p>One way to articulate this (in linear algebra terms), is to say that simple exponentials are linearly dependent on their derivatives. That just means that we can scale them by numbers and add them up in a way that gives zero. For example,</p> \[\left(\frac{d^2}{dt^2}e^{2t}\right)-5\left(\frac{d}{dt}e^{2t}\right)+6e^{2t}=0\] <p>Because when we evaluate those derivatives we end up with</p> \[2^2e^{2t}-5(2e^{2t})+6e^{2t}=0\] <p>So, this means that \(e^{2t}\) is a solution to</p> \[y''-5y'+6y=0\] <p>How could we have found that? Well, I’ll tell you.</p> <h2 id="the-characteristic-polynomial">The characteristic polynomial</h2> <p>Take the example equation we had before</p> \[y''-5y'+6y=0\] <p>How might we solve this? Well, we already observed that exponentials are linearly dependent on their derivatives, so we might expect that our solution is a simple exponential. Then, we will use the idea behind most techniques in differential equations: “<em>the solution probably looks like this, so let’s plug it in and solve for the unknowns</em>”.</p> <p>That is, plug in \(y=e^{\lambda t}\) and see what happens! When we do so, we get</p> \[\lambda^2e^{\lambda t}-5\lambda e^{\lambda t}+6e^{\lambda t}=0\] <p>We can factor out the exponential to get</p> \[(\lambda^2-5\lambda+6)e^{\lambda t}=0\] <p>Now, one of the cool things about exponentials is how they are never zero. So, clearly, we need \(\lambda^2-5\lambda+6=0\). And this is just a quadratic equation! We call it the “characteristic polynomial”.</p> <p>In general, we get the characteristic polynomial from the differential equation like so</p> \[ay''+by'+cy=0 \implies a\lambda^2+b\lambda+c=0\] <p>Pretty easy, huh? Just take the coefficients directly and factor the quadratic. Eventually (assuming you can factor polynomials or complete the square in your head…), you can solve these problems without writing down anything.</p> <p>Back to our example,</p> \[\lambda^2-5\lambda+6=(\lambda-2)(\lambda-3)=0\] <p>Then it appears that we need \(\lambda=2\) or \(\lambda=3\). Meaning that it seems that two solutions are \(y_1=e^{2t}\) (like we saw before!) and \(y_2=e^{3t}\).</p> <p>We can check to see if the same thing that happened with \(e^{2t}\) happens with \(e^{3t}\).</p> \[\left(\frac{d^2}{dt^2}e^{3t}\right)-5\left(\frac{d}{dt}e^{3t}\right)+6e^{3t}=3^2e^{3t}-5(3e^{3t})+6e^{3t}=0\] <p>And it does! So we have two solutions, now. But are there more? Yes!</p> <p>Take, for example, \(y_3=e^{2t}+e^{3t}\). Plugging this in gives</p> \[\begin{multline} (2^2e^{2t}+3^2e^{3t})-5(2e^{2t}+3e^{3t})+6(e^{2t}+e^{3t})\\ =(2^2e^{2t}-5(2e^{2t})+6e^{2t}) +(3^2e^{3t}-5(3e^{3t})+6e^{3t})=0 \end{multline}\] <p>Notice exactly what happened here. We added up the solutions to get \(y_3\), but when we plugged it all in, we were able to just separate them, and they both went to zero like they did individually. We call this the Principle of Superposition!</p> <p>Now, you may choose to read my incoherent explanations of linear independence and superposition. You could read my explanation on why we need two solutions for second order equations. <strong>Or, you could just <a href="#real-roots">skip all of it</a> and get into how we solve these equations.</strong></p> <h2 id="superposition">Superposition</h2> <p>“Principle of Superposition” might sound scary and intimidating, but it’s just a result of two important properties of the derivative</p> \[(cf(t))'=cf'(t),\quad (f(t)+g(t))'=f'(t)+g'(t)\] <p>Basically, you can pull a constant outside of a derivative, and you can split up the derivative of a sum as the sum of derivatives. We can combine these facts to say</p> \[\begin{equation} (c_1f(t)+c_2g(t))'=c_1f'(t)+c_2g'(t) \end{equation}\] <p>Note: When we start adding and scaling stuff we call it a “linear combination”. So \(c_1f(t)+c_2g(t)\) is a “linear combination of \(f(t)\) and \(g(t)\)”.</p> <p>We can apply this to linear differential equations. For example, if we define the “differential operator” for the generic second order linear differential equation</p> \[L[y]=y''+p(t)y'+q(t)y\] <p>Then watch what happens when we evaluate \(L[c_1y_1+c_2y_2]\):</p> \[L[c_1y_1+c_2y_2]=(c_1y_1+c_2y_2)''+p(t)(c_1y_1+c_2y_2)'+q(t)(c_1y_1+c_2y_2)\] \[L[c_1y_1+c_2y_2]=c_1y_1''+c_2y_2''+p(t)(c_1y_1'+c_2y_2')+q(t)(c_1y_1+c_2y_2)\] <p>Now, if we separate the \(c_1\) and \(c_2\) terms,</p> \[L[c_1y_1+c_2y_2]=c_1(y_1''+p(t)y_1'+q(t)y_1)+c_2(y_2''+p(t)y_2'+q(t)y_2)\] \[L[c_1y_1+c_2y_2]=c_1L[y_1]+c_2L[y_2]\] <p>Nifty. We can use this to say that if \(L[y_1]=L[y_2]=0\) (that is to say, \(y_1\) and \(y_2\) are solutions to the differential equation), then</p> \[L[c_1y_1+c_2y_2]=c_1L[y_1]+c_2L[y_2]=c_10+c_20=0\] <p>Meaning that any arbitrary linear combination of solutions is also a solution.</p> <p>However, this may all look like mumbo jumbo to you. What you should get from this is the following:</p> <blockquote> <p>“ya scale a solution, ya get a solution. ya add solutions together, ya get a solution. ya do both at once, ya get a solution.”</p> </blockquote> <p>So, to get <em>every</em> solution, we take an arbitrary linear combination of our individual solutions. As long as you got all the individual solutions you were supposed to get (see next section), we call this the “general solution”.</p> <p>For our example of \(y''-5y'+6y=0\), this would mean that the general solution is</p> \[y=c_1e^{2t}+c_2e^{3t}\] <h2 id="linear-independence">Linear independence</h2> <p>I will admit, for the sake of simplicity, I was a little vague about how we get our general solution. You may, for example, say that \(y_1=e^{2t}\) is a solution, and \(y_2=2e^{2t}\) is also a solution. So can we not say that our general solution is \(y=c_1e^{2t}+c_22e^{2t}\)?</p> <p>No, we can’t. The reason should hopefully make sense: that doesn’t give us every solution! There is no constants \(c_1,c_2\) such that \(c_1e^{2t}+c_22e^{2t}=e^{3t}\). We need a \(y_1\) and \(y_2\) such that \(c_1y_1+c_2y_2\) can give us every possible solution.</p> <p>So how do we check if our choice of solutions is gucci? Well, we have a test for that! It’s called the Wronskian (do <strong>not</strong> look Wronskian up on Urban Dictionary).</p> \[\begin{equation} W[y_1,y_2](t)=\det\left(\begin{bmatrix}y_1(t)&amp;y_2(t)\\y_1'(t)&amp;y_2'(t)\end{bmatrix}\right) =y_1(t)y_2'(t)-y_1'(t)y_2(t) \end{equation}\] <blockquote> <p>If the Wronskian of two functions is nonzero at a point \(t_0\), then the functions are linearly independent at that point, and they form a fundamental set of solutions which can solve any initial value problem centered at \(t_0\).</p> </blockquote> <p><strong>One important result that you can verify with this is that \(y_1=e^{at}\) and \(y_2=e^{bt}\) are linearly independent if and only if \(a\neq b\).</strong> Meaning that we don’t have to worry and check if \(e^{2t}\) and \(e^{3t}\) are independent or not using the wronskian. They are independent because \(2\neq3\).</p> <p>How does this tell us anything? Where does it come from? <strong>If you couldn’t care less, then skip <a href="#how-many-solutions-to-expect">here</a></strong>! If you are genuinely curious, though, we can see it like this:</p> <p>Imagine you are trying to find the solution to the initial value problem with initial conditions \(y(t_0)=y_0, y'(t_0)=y'_0\). Given your two solutions \(y_1,y_2\), you would find the constants \(c_1,c_2\) such that \(y=c_1y_1+c_2y_2\) by solving the system</p> \[\begin{array}{ccccc} c_1y_1(t_0)&amp;+&amp;c_2y_2(t_0)&amp;=&amp;y_0\\ c_1y_1'(t_0)&amp;+&amp;c_2y_2'(t_0)&amp;=&amp;y'_0\\ \end{array}\] <p><a href="http://mathb.in/72311" target="_blank">If you would like to see a derivation that does not use linear algebra, please see here.</a> Otherwise, we’re just going to shift the burden of proof to linear algebra.</p> <h3 id="thank-you-linear-algebra">Thank you, linear algebra</h3> <p>The system of equations can be written as a matrix system</p> \[\begin{bmatrix}y_1(t_0)&amp;y_2(t_0)\\y_1'(t_0)&amp;y_2'(t_0)\end{bmatrix} \begin{bmatrix}c_1\\c_2\end{bmatrix}= \begin{bmatrix}y_0\\y'_0\end{bmatrix}\] <p>Let’s denote this matrix \(\mathbf{W}(t_0)=\begin{bmatrix}y_1(t_0)&amp;y_2(t_0)\\y_1'(t_0)&amp;y_2'(t_0)\end{bmatrix}\) and rewrite this system as</p> \[\mathbf{W}(t_0)\mathbf{c}=\mathbf{y}_0\] <p>The invertible matrix theorem tells us this system of equations has exactly one solution if and only if \(\det(\mathbf{W}(t_0))\neq0\). Therefore, we are <em>guaranteed</em> to have a unique solution to the initial value problem if \(\det(\mathbf{W}(t_0))=y_1(t_0)y_2'(t_0)-y_1'(t_0)y_2(t_0)=W[y_1,y_2](t_0)\neq0\).</p> <p>Emphasis on <em>guaranteed</em>. It’s possible to solve an initial value problem if you don’t have every solution. It’s just not <em>always</em> possible. For example, using the “not general solution” of \(y=c_1e^{2t}+c_22e^{2t}\), you <em>can</em> solve the initial value problem</p> \[y''-5y'+6y=0,\quad y(0)=2, y'(0)=4\] <p>The solution is just \(y=2e^{2t}\). But you can no longer solve it if we change it to</p> \[y''-5y'+6y=0,\quad y(0)=2, y'(0)=5\] <p>This is why \(y=c_1e^{2t}+c_22e^{2t}\) doesn’t work as a <em>general</em> solution. We need \(e^{3t}\) to solve this one.</p> <p>As a final note, when a determinant is nonzero, it means that the columns are linearly independent. This is why we say that the Wronskian tells us if a set of given functions are linearly independent or not.</p> <h2 id="how-many-solutions-to-expect">How many solutions to expect</h2> <p>I have not yet explained why only two solutions are sufficient for a second order equation. The short answer is that an \(n\)th order linear equation will have \(n\) linearly independent solutions. So, for second order, that means two. One way to think about it is “two derivatives means two arbitrary constants”. <strong>If that’s good enough for you, you can <a href="#repeated-roots">skip</a> this section</strong>. But if you’re like me and you want to know <em>why</em>, then continue.</p> <p>The answer can get pretty technical, so I will leave a relatively simple explanation that cites the all important <strong>Picard–Lindelöf existence and uniqueness theorem</strong>. We don’t have to worry at all about when solutions will exist or if the theorem will apply, because constant coefficients guarantees existence everywhere (since it trivially satisfies the requirements of the theorem).</p> <ol> <li>The existence and uniqueness theorem guarantees that a second order initial value problem with two conditions <strong>must</strong> have a <strong>unique</strong> solution.</li> <li>Since this must be true for any two arbitrary initial conditions, one function is not enough. However, two functions which are linearly independent will be able to satisfy any two arbitrary initial conditions (thanks linear algebra).</li> <li>If there was a third linearly independent solution \(y_3\) (i.e. it is not a linear combination of the first two), then that would contradict the uniqueness of the solution \(y=c_1y_1+c_2y_2\) which satisfies \(y(0)=y_3(0), y'(0)=y_3'(0)\).</li> </ol> <h1 id="real-roots">Real Roots</h1> <p>Okay! So, either you read through all or some of my convoluted attempts at explaining superposition, linear independence, and the fundamental assumptions underlying how we solve these problems, or you didn’t. If you didn’t, here is a tl;dr:</p> <blockquote> <p>second order means two different solutions. gotta have different power constants or it ain’t gucci.</p> </blockquote> <p>Either way, let’s get onto the different cases we encounter when solving the characteristic polynomial!</p> <h2 id="distinct-roots">Distinct roots</h2> <p>This is the case we were dealing with before. It’s the simplest and is the most straightforward. As mentioned back in <a href="#linear-independence">Linear independence</a>, if the power constants are different, then they are trivially linearly independent.</p> <p>In summary, if the roots to the characteristic polynomial are \(a\) and \(b\), then the general solution is</p> \[y=c_1e^{at}+c_2e^{bt}\] <h2 id="repeated-roots">Repeated roots</h2> <p>Consider the differential equation</p> \[y''+4y'+4y=0\] <p>Characteristic polynomial is \(\lambda^2+4\lambda+4=(\lambda+2)^2\). To get it to be zero we need \(\lambda=-2\), and there is no other solution for \(\lambda\). So we get the solution \(y_1=e^{-2t}\). But now we have run into a problem.</p> <p>Where is the second solution?</p> <p>In general, for a second order equation, to get a second solution when you only have one, you can use <strong>reduction of order</strong> to get another. But we’re not going to worry about that at all! Instead, I will impart the wisdom that reduction order bestows upon you when you apply to it to constant coefficient equations:</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/redoford.jpg" data-zoomable="true"/> </div> </div> \[\] <p>yeah so the second solution is just \(y_2=te^{-2t}\).</p> <p>In general, if you get a repeated root of \(a\), then the general solution is just</p> \[y=c_1e^{at}+c_2te^{at}\] <h2 id="complex-roots">Complex roots</h2> <p>Take, for example,</p> \[y''+4y'+13y=0\] <p>Characteristic polynomial is \(\lambda^2+4\lambda+13=0\). We can complete the square to get</p> \[(\lambda+2)^2+9=0\] <p>So, it appears we have no real solutions. If we solve for \(\lambda\) though, we get that \(\lambda=-2\pm3i\). Then, is the solution going to be this?</p> \[y=c_1e^{(-2+3i)t}+c_2e^{(-2-3i)t}\] <p>Well, yes and no. See, the mathematics community is a bit discriminatory against complex solutions. They’re not seen as being <em>real</em> solutions. This is especially true when the original problem involves exclusively real terms. So we’re going to have to make our solutions real before our professor gives us any credit.</p> <p>So what do we do? Well, as it turns out, we can just take the real and imaginary parts of either solution, and that will give us two linearly independent solutions!</p> <h3 id="real-and-imaginary-parts-of-solutions">Real and imaginary parts of solutions</h3> <p>Why is this true? Well, if the differential equation is all real, and you put in a complex solution, we can use linearity. Suppose that the solution is \(y(t)=u(t)+iv(t)\), where \(u(t)\) and \(v(t)\) are real functions. Then,</p> \[L[y]=L[u+iv]=L[u]+iL[v]\] <p>Under the assumption that \(L[y]=0\), we get that</p> \[L[u]+iL[v]=0\] <p>Since the differential equation is real, \(L[u]\) and \(L[v]\) are both real as well. And the only way that \(a+bi=0\) is if \(a=b=0\). So,</p> \[L[u]=0,\quad L[v]=0\] <p>Meaning they are two solutions individually. Note that this <em>also</em> means that \(u-iv\) is a solution as well.</p> <p>One thing that isn’t too difficult to prove with the Wronskian is that</p> \[\begin{equation} W[u+iv,u-iv](t)\neq0\iff W[u,v](t)\neq0 \end{equation}\] <p>What about our example? What are the real and imaginary parts of \(e^{(-2+3i)t}\)? For that, we need Euler’s formula!</p> <h3 id="eulers-formula">Eulers formula</h3> <p>If you are unfamiliar with what it means to raise \(e\) to a complex number, you can read <a href="../eulersformula/" target="_blank">this</a> (and ignore the circular logic that comes from that post mentioning linear constant coefficient differential equations). But the basic summary which you can take as fact is that</p> \[\begin{equation} e^{ix}=\cos(x)+i\sin(x) \end{equation}\] <p>Based on properties of exponentials, we can then say that</p> \[\begin{equation} e^{(a+bi)x}=e^{ax}\cos(bx)+ie^{ax}\sin(bx) \end{equation}\] <h3 id="the-final-solution">The final solution</h3> <p>Onto the final solution (to second order linear constant coefficient homogeneous ordinary differential equations)! If we take the real and imaginary parts of \(e^{(-2+3i)t}=e^{-2t}\cos(3t)+ie^{-2t}\sin(3t)\), we get</p> \[y=c_1e^{-2t}\cos(3t)+c_2e^{-2t}\sin(3t)\] <p>And, in general, if the roots of the characteristic polynomial are \(a\pm bi\), then the general solution is</p> \[y=c_1e^{at}\cos(bt)+c_2e^{at}\sin(bt)\] <h2 id="summary">Summary</h2> <p>In summary, given the differential equation</p> \[ay''+by'+cy=0\] <p>Where \(a,b,c\) are real (and \(a\neq0\)). If the roots of \(a\lambda^2+b\lambda+c=0\) are \(\lambda=\lambda_1,\lambda_2\) which are</p> <ol> <li>Real and distinct (\(\lambda_1\neq\lambda_2\)), then the solution is \(\begin{equation} y=c_1e^{\lambda_1t}+c_2e^{\lambda_2t} \end{equation}\)</li> <li>Real and identical (\(\lambda_1=\lambda_2\)), then the solution is \(\begin{equation} y=c_1e^{\lambda_1t}+c_2te^{\lambda_1t} \end{equation}\)</li> <li>Complex conjugates (\(\lambda=\alpha\pm i\beta\)), then the solution is \(\begin{equation} y=c_1e^{\alpha t}\cos(\beta t)+c_2e^{\alpha t}\sin(\beta t) \end{equation}\)</li> </ol> <h2 id="higher-order-equations">Higher order equations</h2> <p>What about third, fourth, or twentieth order equations? Actually, nothing changes. You just have more solutions. But the rules are exactly the same.</p> <p>You still look at the characteristic polynomial, and the roots tell you the solutions.</p> <h3 id="more-complex-roots">More complex roots</h3> <p>If you get complex roots, you still just get a \(\cos\) and a \(\sin\). For example, in</p> \[y^{(4)}-2y'''+2y''-10y'+25y=0\] <p>the roots are \(\lambda=-1\pm2i,2\pm i\). So our solutions will be</p> \[\begin{array}{cccc} \lambda=-1\pm2i&amp;\implies&amp; y_1=e^{-t}\cos(2t),&amp;y_2=e^{-t}\sin(2t)&amp;\\ \lambda=2\pm i&amp;\implies&amp; y_3=e^{2t}\cos(t),&amp;y_4=e^{2t}\sin(t)&amp;\\ \end{array}\] <p>The general solution is then</p> \[y=c_1e^{-t}\cos(2t)+c_2e^{-t}\sin(2t)+c_3e^{2t}\cos(t)+c_4e^{2t}\sin(t)\] <h3 id="more-repeated-roots">More repeated roots</h3> <p>If a root is repeated, just keep slapping \(t\) on there. For example, in</p> \[y'''+3y''+3y'+y=0\] <p>The only root of the characteristic polynomial is \(\lambda=-1\) (repeated three times). Thus, the general solution is simply</p> \[y=c_1e^{-t}+c_2te^{-t}+c_3t^2e^{-t}\] <p>It’s also possible to have repeated complex roots. In the equation</p> \[y^{(4)}+4y'''+14y''+20y'+25y=0\] <p>The roots are \(\lambda=-1\pm2i\), both repeated. So we still get solutions \(y_1=e^{-t}\cos(2t)\) and \(y_2=e^{-t}\sin(2t)\), but we also slap a \(t\) on there.</p> \[y=c_1e^{-t}\cos(2t)+c_2e^{-t}\sin(2t)+c_3te^{-t}\cos(2t)+c_4te^{-t}\sin(2t)\] <p>And, that’s it! Really, this topic is more about factoring polynomials and doing algebra more than it is differential equations (once you get past the derivation). After that, it’s just converting the roots into solutions from the formulas.</p>]]></content><author><name>Taylor Grant</name></author><category term="differential-equations"/><summary type="html"><![CDATA[the most important topic in an ODE class]]></summary></entry><entry><title type="html">Integrating Factors Explained</title><link href="https://smashmath.github.io/blog/integratingfactor/" rel="alternate" type="text/html" title="Integrating Factors Explained"/><published>2022-07-18T00:00:00+00:00</published><updated>2022-07-18T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/integratingfactor</id><content type="html" xml:base="https://smashmath.github.io/blog/integratingfactor/"><![CDATA[<p>This is one of the first things taught in an intro to differential equations class, and I think it is very accessible to students who have taken basic calculus. The only snag might be that you may not know how to do the integral you inevitably obtain, but the overall idea for the process is straightforward enough, I think.</p> <p>As usual I kind of go overboard on the additional facts which you don’t really <em>need</em> to know. So just ignore them if you don’t care. I’ll put <strong>Aside</strong> in front of those kind of things.</p> <h2 id="what-is-a-first-order-linear-ode">What is a first order linear ODE?</h2> <p>A first order linear differential equation is one of the form</p> \[\begin{equation} y'+p(t)y=g(t) \end{equation}\] <p>Maybe you have a function on the \(y'\) term, but dividing it out gives the standard form above. An initial condition such as \(y(0)=3\) is also common, but that only needs to be worried about near the end of the solving process.</p> <p>We also won’t be concerning ourselves about where solutions to the equation (initial value problems) exist. That’s a bit too technical for the purposes of this post.</p> <p>We will also be using some compact prime notation. Basically</p> \[\begin{equation} \frac{d}{dt}(f(t)h(t))=(fh)' \end{equation}\] <h2 id="how-do-we-solve-them">How do we solve them?</h2> <p>Well, first, let’s review some basic calculus. Specifically, the product rule!</p> \[\begin{equation} (fh)'=fh'+f'h \end{equation}\] <p>The order doesn’t matter because it’s a sum. Now, observe what happens if we replace \(h\) with \(y\)</p> \[(fy)'=fy'+f'y\] <p>That looks sort of close to the left side of our original differential equation!</p> <p>So, the motivating idea is to find some function <a href="https://puu.sh/JbL0d/bbacfdb2ca.mp4" target="_blank">\(\mu(t)\)</a> which will make the left side of our equation a simple product rule. Because then we can integrate both sides and solve for \(y\).</p> <h2 id="finding-the-integrating-factor">Finding the integrating factor</h2> <p>Now, this is the idea behind almost every single technique you learn in a differential equations class: “<em>the solution probably looks like this, so let’s plug it in and solve for the unknowns</em>”.</p> <p>When we multiply our original equation by \(\mu\), we get</p> \[\mu y'+\mu p(t)y=\mu g(t)\] <p>And our goal, remember, is to get that left side to look like</p> \[\mu y'+p(t)\mu y=(fy)'\] <p>Well, if we expand it out, we get</p> \[\mu y'+p(t)\mu y=fy'+f'y\] <p>So, matching up the terms, we get that \(\mu=f\) and \(p(t)\mu=f'=\mu'\).</p> <p>Then, to solve for \(\mu\), we need to solve</p> \[\begin{equation} \mu'=p(t)\mu \end{equation}\] <p>Now, you can solve this differential equation in a lot of different ways. Usually as a separable equation. But instead of teaching you that technique which you may not know, I’m just going to show you the differentiation rule for exponentials:</p> <h3 id="a-detour-with-exponentials">A detour with exponentials</h3> \[\begin{equation} \left(e^{f(t)}\right)'=f'(t)e^{f(t)} \end{equation}\] <p>In words, the observation to make is that differentiating an exponential function is the same as multiplying it by some function. That is,</p> \[y'=f'(t)y\iff y=Ce^{f(t)}\] <p>Note that we can multiply by a constant since it doesn’t change anything about the differentiation.</p> <h2 id="solving-for-μ">Solving for μ</h2> <p>All this to say, we just need some function \(P(t)\) such that \(P'(t)=p(t)\). Basically, \(P(t)=\int p(t)dt\). Any one will do! <strong>Aside</strong>: This is because \(e^{P(t)+K}=Ce^{P(t)}\), and any scalar multiple of an integrating factor is also an integrating factor. So just pick one antiderivative and go with it.</p> <p>So, if we plug that into our differential equation,</p> \[e^{P(t)}y'+p(t)e^{P(t)}y=e^{P(t)}g(t)\] <p>We can verify that, under the assumption that \(P'(t)=p(t)\), the left side is indeed \(e^{P(t)}y'+p(t)e^{P(t)}y=\left(e^{P(t)}y\right)'\).</p> \[\left(e^{P(t)}y\right)'=e^{P(t)}g(t)\] <p>In summary, the basic equation for the integrating factor is</p> \[\begin{equation} \mu(t)=e^{\int p(t)dt} \end{equation}\] <h2 id="solving-the-equation">Solving the equation</h2> <p>Now, we can just integrate both sides (not forgetting our constant of integration).</p> \[e^{P(t)}y=C+\int e^{P(t)}g(t)dt\] <p>Finally, we can solve for \(y\) by dividing both sides by \(e^{P(t)}\) (or multiplying by \(e^{-P(t)}\)).</p> \[y=Ce^{-P(t)}+e^{-P(t)}\int e^{P(t)}g(t)dt\] <p><strong>Aside</strong>: One interesting observation is that we can express this solution more compactly in terms of \(\mu\).</p> \[y=\frac{C}{\mu}+\frac{\int \mu g}{\mu}\] <p>And if you’re in differential equations or basic calculus now, your professor may not like this more compact way of writing the integral without the d(whatever). But the textbook I used for my intro to analysis class (Understanding Analysis by Stephen Abbott) used it, so just tell your professor that you’re using Abbott (i.e. big brain advanced baller) notation (and then don’t blame me if your professor isn’t having that).</p> <h2 id="unnecessary-further-simplifications">Unnecessary further simplifications</h2> <p>The rest of the post is an <strong>aside</strong>. This is just gravy if you’re still interested in knowing more and going further with these ideas. The post is over; you can leave and go home now if you want.</p> <p>There are some other simplifications you can make which I personally like. They aren’t really super practical in terms of actually solving these equations, usually, but I think they’re cool!</p> <p>Personally, I like to use dummy variables for integration.</p> \[y=Ce^{-P(t)}+e^{-P(t)}\int_{t_0}^t e^{P(s)}g(s)ds\] <p>We can also use another dummy variable \(P(t)\):</p> \[P(t)=\int_{t_0}^tp(r)dr\] <p>Also using that \(P(s)=\int_{t_0}^sp(r)dr\). Changing our solution to</p> \[y=Ce^{-\int_{t_0}^tp(r)dr}+e^{-\int_{t_0}^tp(r)dr}\int_{t_0}^t e^{\int_{t_0}^sp(r)dr}g(s)ds\] <p>This has the added benefit of making it so that we can replace \(C\) with \(y(t_0)\). This is because plugging in \(t=t_0\) makes all the integrals zero, leaving us with just \(C\). This eliminates the need to solve for \(C\) in an initial value problem. Just be careful that the integrals do not diverge when integrating from \(t_0\). If they do, choose any other arbitrary starting bound that gives a convergent (gucci) integral.</p> \[y=y(t_0)e^{-\int_{t_0}^tp(r)dr}+e^{-\int_{t_0}^tp(r)dr}\int_{t_0}^t e^{\int_{t_0}^sp(r)dr}g(s)ds\] <p>We may also distribute the exponential into the integral now, because we are using a dummy variable for the integration. Note that in the exponent we will get</p> \[\int_{t_0}^sp(r)dr-\int_{t_0}^tp(r)dr\] <p>Which we can simplify to \(\int_t^{t_0}p(r)dr+\int_{t_0}^sp(r)dr=\int_{t}^sp(r)dr\) using our rules for integration.</p> \[y=y(t_0)e^{-\int_{t_0}^tp(r)dr}+\int_{t_0}^t e^{\int_{t}^sp(r)dr}g(s)ds\] <p>Another personal preference is to use \(\exp(x)\) instead of \(e^x\) when the exponent is big and messy (such as when we stick a whole integral up in there).</p> \[y=y(t_0)\exp\left(-\int_{t_0}^tp(r)dr\right)+\int_{t_0}^t\exp\left(\int_{t}^sp(r)dr\right)g(s)ds\] <h2 id="further-notes">Further notes</h2> <p>Here’s some more information on how this concept generalizes to second order equations, if you’re interested.</p> <p>Did you know every linear second order differential equation also has an integrating factor? The reason nobody ever uses this, though, is that, usually, finding it is just as hard as solving the equation normally.</p> <p>For example, trying to use an integrating factor to solve</p> \[y''+y=0\] <p>ends up giving you the following differential equation for the integrating factor \(\mu\):</p> \[\mu''+\mu=0\] <p>Which means that the integrating factor for the differential equation must be a solution to the differential equation. Making this method absolutely useless for this problem.</p> <p>If you want to know how the method works, though, basically the idea is to find a function \(\mu\) and \(f\) such that when you multiply \(P(t)y''+Q(t)y'+R(t)y\) by \(\mu\), you get</p> \[P(t)\mu y''+Q(t)\mu y'+R(t)\mu y=(P(t)\mu y'+fy)'\] <p>Setting equal the coefficients gives you</p> \[f'=R\mu,\quad P\mu'+P'\mu+f=Q\mu\] <p>If you differentiate the right equation and substitute in \(R\mu\) for \(f'\), you get</p> \[P\mu''+(2P'-Q)\mu'+(P''-Q'+R)\mu=0\] <p>This is also called the Adjoint Equation, and it’s usually not much nicer than the original. If \(P''-Q'+R=0\), though, then the adjoint equation is first order (and linear) in \(\mu'\), allowing you to solve it either with another integrating factor or as a separable equation. When this happens, the original equation is called “exact” (yes, this is the second order version of exact equations!).</p>]]></content><author><name>Taylor Grant</name></author><category term="differential-equations"/><summary type="html"><![CDATA[the most common way to solve first order linear ODE's]]></summary></entry><entry><title type="html">Bases for the fundamental spaces of a matrix</title><link href="https://smashmath.github.io/blog/rowcolspace/" rel="alternate" type="text/html" title="Bases for the fundamental spaces of a matrix"/><published>2022-06-11T00:00:00+00:00</published><updated>2022-06-11T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/rowcolspace</id><content type="html" xml:base="https://smashmath.github.io/blog/rowcolspace/"><![CDATA[<p>In case you need a refresher on what things like pivots and the column space are. Otherwise, you can just skip to <a href="#row-reduction">Row Reduction</a>.</p> <h2 id="preliminary-vocab-and-notation">Preliminary vocab and notation</h2> <p>As a note on notation, we typically say \(A\) is an \(m\times n\) matrix which we are looking at. And we will call its reduced row echelon form (or rref for short) \(R\).</p> <p>The matrix we will use as an example will be the following</p> \[A=\begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix}\] <p>And its rref</p> \[R=\begin{pmatrix} 1&amp;2&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;1&amp;1&amp;0\\ 0&amp;0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0&amp;0 \end{pmatrix}\] <p>In the discussion of this topic, it often helps to assign variables to each column, as if we were solving some arbitrary system \(A\textbf{x}=\textbf{b}\). To explicitly define our variables,</p> \[\begin{equation} \label{aeq} \begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix}\begin{pmatrix} x_1\\x_2\\x_3\\x_4\\x_5 \end{pmatrix}=\textbf{b} \end{equation}\] <p>Which is row reduced to</p> \[\begin{equation} \label{req} \begin{pmatrix} 1&amp;2&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;1&amp;1&amp;0\\ 0&amp;0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0&amp;0 \end{pmatrix}\begin{pmatrix} x_1\\x_2\\x_3\\x_4\\x_5 \end{pmatrix}=\textbf{b}' \end{equation}\] <p>When we refer to “variables” in “the system”, we will be referring to \(x_1,\ldots,x_5\) in the above.</p> <p>Other important vocab includes</p> <ul> <li><strong>Pivot</strong>: A pivot is a leading one (the first nonzero entry of each nonzero row) in the rref. The pivots of \(R\) are in the first, third, and fifth column.</li> <li><strong>Pivot column</strong>: A column with a pivot. In rref, pivot columns are just columns of the identity matrix. The pivot columns of \(R\) are then the first, third, and fifth columns. Pivot columns are also linearly independent.</li> <li><strong>Nonpivot column</strong>: A column that doesn’t have a pivot.</li> <li><strong>Leading variable</strong>: The variables corresponding to pivot columns. They can be solved for in terms of free variables. So for the system \eqref{req}, the leading variables are \(x_1,x_3,x_5\).</li> <li><strong>Free variable</strong>: A variable corresponding to nonpivot columns. For the system mentioned above, they would be \(x_2,x_4\).</li> <li><strong>Rank</strong>: The dimension of the column and row space. Also the number of pivot columns.</li> <li><strong>Nullity</strong>: The dimension of the nullspace. Also the number of nonpivot columns.</li> </ul> <p>Next, the four fundamental spaces of a matrix \(A\):</p> <ul> <li><strong>Column Space</strong>: The range of the columns. Also every vector \(\textbf{b}\) such that \(A\textbf{x}=\textbf{b}\) is consistent.</li> <li><strong>Nullspace</strong>: Every vector \(\textbf{n}\) such that \(A\textbf{n}=\textbf{0}\). Also called the kernel of \(A\).</li> <li><strong>Row Space</strong>: The range of the rows. Also the column space of \(A^T\).</li> <li><strong>Left Nullspace</strong>: Every vector \(\textbf{n}\) such that \(\textbf{n}^TA=\textbf{0}^T\). Also the kernel of \(A^T\).</li> </ul> <h3 id="dimensions-of-the-spaces">Dimensions of the spaces</h3> <p>Finally, a quick summary on the dimensions of the four spaces of an \(m\times n\) matrix \(A\) with \(\operatorname{rank}(A)=r\) using the rank-nullity theorem:</p> <ul> <li>\(\dim(\operatorname{col}(A))=\dim(\operatorname{row}(A))=\operatorname{rank}(A)=r=\) the number of pivot columns.</li> <li>\(\dim(\operatorname{null}(A))=\operatorname{nullity}(A)=n-r=\) the number of nonpivot columns.</li> <li>\(\dim\left(\operatorname{null}\left(A^T\right)\right)=\operatorname{nullity}(A^T)=m-r\).</li> </ul> <p>As a brief aside, I personally tend to go for a different kind of row echelon form other than rref. Because I hate fractions in my matrix, I don’t force “leading ones”. I do, however, try to keep pivot columns having the pivot be the only nonzero entry. That way, leading variables are still easy to solve for, and you avoid fractions.</p> <p>To illustrate this point, basically I prefer \(\begin{pmatrix} 2&amp;0&amp;1\\ 0&amp;3&amp;-1\\ \end{pmatrix}\) to \(\begin{pmatrix} 1&amp;0&amp;\frac{1}{2}\\ 0&amp;1&amp;-\frac{1}{3}\\ \end{pmatrix}\)</p> <h2 id="row-reduction">Row reduction</h2> <p>First, It’s important to make clear what row reduction <em>does</em> and <em>does not</em> change about a matrix.</p> <ul> <li>Row reduction does <em>not</em> change the row space or the null space.</li> <li>Row reduction <em>usually</em> changes the column space</li> </ul> <p>Let’s explain why the row and null spaces are unaffected real quick.</p> <p>The row space is not changed because the rows of \(R\) are just linear combinations of the rows of \(A\). Thus, the rows of \(R\) are in the span of the rows of \(A\), leaving them contained in the row space. Since row operations are reversible (invertible), we don’t lose any information about the row space either.</p> <p>The reason the null space is not affected is a bit more complicated. The short answer is that column dependencies remain the same in both \(A\) and \(R\) (we will discuss this at length further on). The slightly longer and possibly more intuitive answer is that if \(R\) is the rref of \(A\), then there is an invertible matrix \(P\) such that \(PA=R\). It follows that if \(A\textbf{n}=\textbf{0}\), then \(R\textbf{n}=PA\textbf{n}=P\textbf{0}=\textbf{0}\).</p> <p>Hopefully, it is obvious that the column space is usually completely different. Most of the columns of \(A\) have a nonzero fourth entry, but all of the columns of \(R\) have a zero fourth entry. So \((1,1,1,1)\), the first column of \(A\) which is by definition in the column space, cannot be in the column space of \(R\). Thus, the column spaces are not always the same.</p> <h2 id="row-space">Row Space</h2> <p>So, this is actually pretty easy. A basis for the row space of \(A\) is just the nonzero rows of \(R\). As mentioned before, they are in the span of the rows of \(A\). And they are also obviously linearly independent.</p> <p>From our \(R=\begin{pmatrix} 1&amp;2&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;1&amp;1&amp;0\\ 0&amp;0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0&amp;0 \end{pmatrix}\) matrix, the nonzero rows are</p> \[(1,2,0,-1,0),\quad (0,0,1,1,0),\quad (0,0,0,0,1)\] <p>The first vector is the only one with a nonzero first entry, so it’s definitely linearly independent from the others. The second is the only one with a nonzero third entry, so it’s definitely linearly independent from the others. And the third is the only one with a nonzero fifth entry, so (etc.)</p> <p>Our Row Space basis is then</p> \[\operatorname{row}(A)=\operatorname{span}\left\{(1,2,0,-1,0),(0,0,1,1,0),(0,0,0,0,1)\right\}\] <p>Those who remember that you have to look at the columns of the original matrix for the column space may be tempted to say that the first three rows of \(A\) must be a basis for the row space as well. But that would be wrong here! Most of the time you are okay doing that, but in this case, the first two rows of \(A\) are identical, and thus linearly dependent. They cannot be a basis, then.</p> <h2 id="column-space">Column Space</h2> <p>Before, I mentioned that “column dependencies remain the same in both \(A\) and \(R\)”. Let’s go over exactly what I mean by that.</p> <p>Look at \(R\):</p> \[R=\begin{pmatrix} 1&amp;2&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;1&amp;1&amp;0\\ 0&amp;0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0&amp;0 \end{pmatrix}\] <p>The columns are very simple, so it’s very easy to see the column dependencies of the nonpivot columns in terms of the pivot columns.</p> <p>The second column, \((2,0,0,0)\) is twice the first column (col2=2col1)</p> \[(2,0,0,0)=2(1,0,0,0)\] <p>And the fourth column \((-1,1,0,0)\) is the difference of the first and third columns (col4=col3-col1)</p> \[(-1,1,0,0)=(0,1,0,0)-(1,0,0,0)\] <p>Now, this blew my mind when I first saw it. But the same relationships are true for \(A\) as well!</p> \[\begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix}\] <p>(col2=2col1)</p> \[(2,2,2,2)=2(1,1,1,1)\] <p>(col4=col3-col1)</p> \[(0,0,2,0)=(1,1,3,1)-(1,1,1,1)\] <p>This isn’t just coincidence either! This is always true of a matrix and its rref. In other words, <em>the rref tells us the relationships between the columns</em>.</p> <p>So, we know that column two is linearly dependent on column one, and column four is linearly dependent on columns one and three. Thus, we exclude them from our basis.</p> <p><strong>This is why we take the pivot columns of the original matrix!</strong> It’s because the rref tells us they are <em>linearly independent</em>.</p> <p>Then our pivot columns are columns one, three, and five. So a basis for the column space is</p> \[\operatorname{col}(A)=\operatorname{span}\left\{ \begin{pmatrix} 1\\1\\1\\1 \end{pmatrix}, \begin{pmatrix} 1\\1\\3\\1 \end{pmatrix}, \begin{pmatrix} 1\\1\\3\\2 \end{pmatrix} \right\}\] <p>Note: the nonpivot columns of \(R\) are the coordinate vectors of the nonpivot columns of \(A\) with respect to this pivot column basis!</p> <h2 id="nullspace">Nullspace</h2> <p>So, we actually already did all the work we needed to do in the previous section!</p> <p>We found the following column dependencies:</p> <p>(col2=2col1 \(\implies\) 2col1-col2=0) and (col4=col3-col1 \(\implies\) col1-col3+col4=0)</p> <p>This actually gives us a basis for the null space, using <a href="../columnperspective/" target="_blank">column perspective</a>.</p> <p>If we take \(2\) of column one, and \(-1\) of column two, we get zero. So \((2,-1,0,0,0)\) is in the null space.</p> \[\begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix} \begin{pmatrix} 2\\-1\\0\\0\\0 \end{pmatrix}= 2\begin{pmatrix} 1\\1\\1\\1 \end{pmatrix}- \begin{pmatrix} 2\\2\\2\\2 \end{pmatrix}= \begin{pmatrix} 0\\0\\0\\0 \end{pmatrix}\] <p>The other relationship tell us that \(1\) of column one, \(-1\) of column three, and \(1\) of column four will also give us zero. Putting \((1,0,-1,1,0)\) in the null space as well.</p> \[\begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix} \begin{pmatrix} 1\\0\\-1\\1\\0 \end{pmatrix}= \begin{pmatrix} 1\\1\\1\\1 \end{pmatrix}- \begin{pmatrix} 1\\1\\3\\1 \end{pmatrix}+ \begin{pmatrix} 0\\0\\2\\0 \end{pmatrix}= \begin{pmatrix} 0\\0\\0\\0 \end{pmatrix}\] <p>\((2,-1,0,0,0)\) and \((1,0,-1,1,0)\) are pretty clearly independent, and since we have two nonpivot columns, we know the dimension of the null space is two. Thus, we have a proper basis for the null space!</p> \[\operatorname{null}(A)=\operatorname{span}\left\{ \begin{pmatrix} 2\\-1\\0\\0\\0 \end{pmatrix}, \begin{pmatrix} 1\\0\\-1\\1\\0 \end{pmatrix} \right\}\] <h2 id="left-nullspace">Left Nullspace</h2> <p><strong>PRO TIP</strong>: You don’t have to start at square one and row reduce the entirety of \(A^T\). You can just take the pivot columns, tranpose them, and row reduce that!</p> <p>For our example,</p> \[\operatorname{rref}\left( \begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix}^T \right)= \begin{pmatrix} 1&amp;1&amp;0&amp;0\\ 0&amp;0&amp;1&amp;0\\ 0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0 \end{pmatrix}\] <p>Whilst</p> \[\operatorname{rref}\left( \begin{pmatrix} 1&amp;1&amp;1\\ 1&amp;1&amp;1\\ 1&amp;3&amp;3\\ 1&amp;1&amp;2 \end{pmatrix}^T \right)= \begin{pmatrix} 1&amp;1&amp;0&amp;0\\ 0&amp;0&amp;1&amp;0\\ 0&amp;0&amp;0&amp;1\\ \end{pmatrix}\] <p>Neat, huh? It’s just the nonzero rows! This is because we already know the nonpivot columns are linearly dependent on the pivot columns. So, when row reducing the transpose, those columns (now rows) will end up zeroing out as we row reduce.</p> <p>From the summary at the beginning, we know that since we have three pivot columns and four rows in \(A\), the dimension of the left nullspace will be \(4-3=1\). Thus, we will only get one vector in the left nullspace. The only nonpivot column is column two, and we can see that if we take col1-col2 we will get zero. So our null vector is \((1,-1,0,0)\).</p> <p>Thus,</p> \[\operatorname{null}(A^T)=\operatorname{span}\left\{ (1,-1,0,0) \right\}\]]]></content><author><name>Taylor Grant</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[a lot of students struggle with this so here. row space, column space, null space, and left null space.]]></summary></entry><entry><title type="html">Column and Row Perspective</title><link href="https://smashmath.github.io/blog/columnperspective/" rel="alternate" type="text/html" title="Column and Row Perspective"/><published>2022-06-06T00:00:00+00:00</published><updated>2022-06-06T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/columnperspective</id><content type="html" xml:base="https://smashmath.github.io/blog/columnperspective/"><![CDATA[<h1 id="column-perspective">Column Perspective</h1> <p>First, let’s start with general matrix column vector multiplication. We’ll focus on \(2\times2\) matrices first, as the principles extend very naturally for larger matrices.</p> <p>If we define the matrices</p> \[\textbf{A}=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix},\quad \textbf{x}=\begin{pmatrix} x_1\\x_2 \end{pmatrix}\] <p>Then,</p> \[\begin{equation} \textbf{A}\textbf{x}=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} x_1\\x_2 \end{pmatrix} =\begin{pmatrix} a_{11}x_1+a_{12}x_2\\ a_{21}x_1+a_{22}x_2\\ \end{pmatrix} \end{equation}\] <p>But look what happens when we separate this result by the \(x\) values.</p> \[\begin{pmatrix} a_{11}x_1+a_{12}x_2\\ a_{21}x_1+a_{22}x_2\\ \end{pmatrix}= \begin{pmatrix} a_{11}x_1\\ a_{21}x_1\\ \end{pmatrix}+\begin{pmatrix} a_{12}x_2\\ a_{22}x_2\\ \end{pmatrix}\] \[\begin{pmatrix} a_{11}x_1+a_{12}x_2\\ a_{21}x_1+a_{22}x_2\\ \end{pmatrix}= x_1\begin{pmatrix} a_{11}\\ a_{21}\\ \end{pmatrix}+x_2\begin{pmatrix} a_{12}\\ a_{22}\\ \end{pmatrix}\] \[\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} x_1\\x_2 \end{pmatrix}=x_1\begin{pmatrix} a_{11}\\ a_{21}\\ \end{pmatrix}+x_2\begin{pmatrix} a_{12}\\ a_{22} \end{pmatrix}\] <p>Which are just the original columns of \(\textbf{A}\). So, if we denote the columns of \(\textbf{A}\) by \(\textbf{A}_1,\textbf{A}_2\), we can say that</p> \[\begin{equation} \textbf{A} \begin{pmatrix} x_1\\x_2 \end{pmatrix}=x_1\textbf{A}_1+x_2\textbf{A}_2 \end{equation}\] <p>Which we can interpret as saying: “we want \(x_1\) of the first column of \(\textbf{A}\), and \(x_2\) of the second column of \(\textbf{A}\).”</p> <p>And, in general, if we denote the columns of an \(m\times n\) matrix \(\textbf{A}\) by \(\textbf{A}_1,\ldots,\textbf{A}_n\), we can say that</p> \[\textbf{A}\begin{pmatrix} x_1\\x_2\\\vdots\\x_n \end{pmatrix}=x_1\textbf{A}_1+x_2\textbf{A}_2+\ldots+x_n\textbf{A}_n\] <p>Thus, when multiplying a matrix on the right by a column vector, the column vector tells us how many of each column we are taking.</p> <h2 id="applying-this-to-general-matrix-multiplication">Applying this to general matrix multiplication</h2> <p>Suppose we have that</p> \[\textbf{A}=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix},\quad \textbf{B}=\begin{pmatrix} b_{11}&amp;b_{12}\\ b_{21}&amp;b_{22} \end{pmatrix}\] <p>Consider the product \(\textbf{A}\textbf{B}\),</p> \[\textbf{A}\textbf{B}=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} b_{11}&amp;b_{12}\\ b_{21}&amp;b_{22} \end{pmatrix}\] \[\textbf{A}\textbf{B} =\begin{pmatrix} a_{11}b_{11}+a_{12}b_{21}&amp;a_{11}b_{12}+a_{12}b_{22}\\ a_{21}b_{11}+a_{22}b_{21}&amp;a_{21}b_{12}+a_{22}b_{22} \end{pmatrix}\] <p>Now, look at each column of the product. Notice that the second column of \(\textbf{B}\) has no bearing on the first column of \(\textbf{A}\textbf{B}\). Similarly, the first column of \(\textbf{B}\) has no effect on the second column of \(\textbf{A}\textbf{B}\).</p> <p>If you don’t quite see what I mean, look at what happens if we separate the multiplication by the columns of \(\textbf{B}\).</p> \[\textbf{A}\textbf{B}_1=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} b_{11}\\b_{21} \end{pmatrix}\] \[\textbf{A}\textbf{B}_1 =\begin{pmatrix} a_{11}b_{11}+a_{12}b_{21}\\ a_{21}b_{11}+a_{22}b_{21} \end{pmatrix}= b_{11}\textbf{A}_1+b_{21}\textbf{A}_2\] <p>Which is the first column of \(\textbf{A}\textbf{B}\).</p> \[\textbf{A}\textbf{B}_2=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} b_{12}\\b_{22} \end{pmatrix}\] \[\textbf{A}\textbf{B}_2 =\begin{pmatrix} a_{11}b_{12}+a_{12}b_{22}\\ a_{21}b_{12}+a_{22}b_{22} \end{pmatrix}= b_{12}\textbf{A}_1+b_{22}\textbf{A}_2\] <p>Which is the second column of \(\textbf{A}\textbf{B}\).</p> <p>Thus, we see that</p> \[\textbf{A}\textbf{B}= \bigg( \textbf{A}\textbf{B}_1\quad\textbf{A}\textbf{B}_2 \bigg)\] <p>And more generally,</p> \[\begin{equation} \textbf{A} \bigg( \textbf{B}_1\quad\cdots\quad\textbf{B}_n \bigg)= \bigg( \textbf{A}\textbf{B}_1\quad\cdots\quad\textbf{A}\textbf{B}_n \bigg) \end{equation}\] <p>That is to say, each column of the product, is just the left matrix times the individual column. Hence, we can use the column perspective for each column individually.</p> <h2 id="column-perspective-examples">Column Perspective Examples</h2> <p>This can GREATLY speed up certain computations.</p> <p>For example,</p> \[\textbf{A}\begin{pmatrix} 1\\0\\0 \end{pmatrix}=\textbf{A}_1\] <p>will just give us the first column of \(\textbf{A}\). And</p> \[\textbf{A}\begin{pmatrix} 1\\1\\0 \end{pmatrix}=\textbf{A}_1+\textbf{A}_2\] <p>is just the first column and the second column added together.</p> <p>This can also help us in the reverse direction! Take the example of solving</p> \[\left( \begin{array}{ccc|c} 1&amp;0&amp;3&amp;0\\ 0&amp;1&amp;-1&amp;0\\ 0&amp;0&amp;0&amp;0 \end{array} \right)\] <p>We can see that if we take the third column as it is (that is to say, taking exactly \(1\) of it), we can cancel out the nonzero entries by taking \(-3\) of column one, and \(1\) of column two. Thus, a solution will be \((-3,1,1)\). Since any scalar multiple of this vector will also yield the zero vector, the general solution will be</p> \[\textbf{x}=c\begin{pmatrix} -3\\1\\1 \end{pmatrix}\] <p>since taking \(-3\) of the first column, \(1\) of the second column, and \(1\) of the third column, will cancel everything out. No need to turn it back into equations and solve or whatever.</p> <p>Let’s use an example of matrix multiplication</p> \[\begin{pmatrix}-2&amp;1&amp;1\\1&amp;-2&amp;1\\1&amp;1&amp;-2\end{pmatrix} \begin{pmatrix}0&amp;1&amp;1\\1&amp;0&amp;1\\1&amp;1&amp;0\end{pmatrix}\] <p>The first column of the right matrix tells us to add up the second and third columns: \((1,-2,1)+(1,1,-2)=(2,-1,-1)\). The second column tells us to add the first and third columns: \((-2,1,1)+(1,1,-2)=(-1,2,-1)\). And the third column tells us to add up the first and second columns: \((-2,1,1)+(1,-2,1)=(-1,-1,2)\). Therefore, the product will be</p> \[\begin{pmatrix}-2&amp;1&amp;1\\1&amp;-2&amp;1\\1&amp;1&amp;-2\end{pmatrix} \begin{pmatrix}0&amp;1&amp;1\\1&amp;0&amp;1\\1&amp;1&amp;0\end{pmatrix}= \begin{pmatrix}2&amp;-1&amp;-1\\-1&amp;2&amp;-1\\-1&amp;-1&amp;2\end{pmatrix}\] <p>Personally, I can say that I prefer adding up the columns as opposed to doing nine three-dimensional dot products.</p> <hr/> <h1 id="row-perspective">Row Perspective</h1> <p>Row perspective, while not <em>quite</em> as useful as column perspective, still has its share of uses and applications. It is essentially the transpose of column perspective. Then, we will define our notation for the rows of an \(m\times n\) matrix \(A\) as \(\textbf{a}_1^T,\ldots,\textbf{a}_m^T\).</p> <p>Row perspective, is as follows.</p> \[\begin{equation} \begin{pmatrix} \textbf{a}_1^T\\\textbf{a}_2^T\\\vdots\\\textbf{a}_m^T \end{pmatrix}\textbf{B}= \begin{pmatrix} \textbf{a}_1^T\textbf{B}\\\textbf{a}_2^T\textbf{B}\\\vdots\\\textbf{a}_m^T\textbf{B} \end{pmatrix} \end{equation}\] <p>Where the entries of \(\textbf{a}_i^T\) tell you how many of each <strong>row</strong> of \(B\) to take.</p> <p>To see this for a \(2\times2\),</p> \[\begin{pmatrix} \textbf{a}_1^T\\ \textbf{a}_2^T\\ \end{pmatrix}\textbf{B} =\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} b_{11}&amp;b_{12}\\ b_{21}&amp;b_{22} \end{pmatrix}\] \[\begin{pmatrix} \textbf{a}_1^T\\ \textbf{a}_2^T\\ \end{pmatrix}\textbf{B} =\begin{pmatrix} a_{11}b_{11}+a_{12}b_{21}&amp;a_{11}b_{12}+a_{12}b_{22}\\ a_{21}b_{11}+a_{22}b_{21}&amp;a_{21}b_{12}+a_{22}b_{22} \end{pmatrix}\] \[\begin{pmatrix} \textbf{a}_1^T\\ \textbf{a}_2^T\\ \end{pmatrix}\textbf{B} =\begin{pmatrix} a_{11}(b_{11}\quad b_{12})+a_{12}(b_{21}\quad b_{22})\\ a_{21}(b_{11}\quad b_{12})+a_{22}(b_{21}\quad b_{22})\\ \end{pmatrix}\] \[\begin{pmatrix} \textbf{a}_1^T\\ \textbf{a}_2^T\\ \end{pmatrix}\textbf{B} =\begin{pmatrix} (a_{11}\quad a_{12})\begin{pmatrix} b_{11}&amp;b_{12}\\ b_{21}&amp;b_{22} \end{pmatrix}\\ (a_{21}\quad a_{22})\begin{pmatrix} b_{11}&amp;b_{12}\\ b_{21}&amp;b_{22} \end{pmatrix}\\ \end{pmatrix}\] \[\begin{pmatrix} \textbf{a}_1^T\\ \textbf{a}_2^T\\ \end{pmatrix}\textbf{B} =\begin{pmatrix} \textbf{a}_1^T\textbf{B}\\ \textbf{a}_2^T\textbf{B}\\ \end{pmatrix}\] <p>So an example of this would be</p> \[\begin{pmatrix}0&amp;1&amp;1\\1&amp;1&amp;0\\1&amp;0&amp;1\end{pmatrix} \begin{pmatrix}-2&amp;1&amp;1\\1&amp;-2&amp;1\\1&amp;1&amp;-2\end{pmatrix}\] <p>The first row of the left matrix tells us we want \(1\) of the second row and \(1\) of the third row: \((1,-2,1)+(1,1,-2)=(2,-1,-1)\). The second row tells us to add up the first and second rows: \((-2,1,1)+(1,-2,1)=(-1,-1,2)\). And the third rows tells us to add the first and third rows: \((-2,1,1)+(1,1,-2)=(-1,2,-1)\). Therefore, the product will be</p> \[\begin{pmatrix}0&amp;1&amp;1\\1&amp;1&amp;0\\1&amp;0&amp;1\end{pmatrix} \begin{pmatrix}-2&amp;1&amp;1\\1&amp;-2&amp;1\\1&amp;1&amp;-2\end{pmatrix}= \begin{pmatrix}2&amp;-1&amp;-1\\-1&amp;-1&amp;2\\-1&amp;2&amp;-1\end{pmatrix}\] <h2 id="application-to-row-reduction">Application to row reduction</h2> <p>I use row perspective often when row reducing matrices. It helps me do multiple steps at the same time. So, let’s say I am trying to row reduce</p> \[\textbf{A}=\begin{pmatrix} 1&amp;1&amp;3&amp;2\\-1&amp;0&amp;-2&amp;-3\\2&amp;2&amp;6&amp;7 \end{pmatrix}\] <p>My thought process is as follows:</p> <p>First, the second row would be better if its negative was the first row, because then we would have a pivot in the first column and a zero in the second entry. Thus, the first row of my row reduction matrix should be \((0,-1,0)\), since we want \(-1\) of the second row.</p> <p>Next, if we add up the first two rows, then we get a pivot in the second column and a zero in the first entry. So, the second row of our row reduction matrix will be \((1,1,0)\), since we want to add up the first and second row.</p> <p>Finally, we can cancel out the first three entries of the third row by adding \(-2\) of row one. The third column should then be \((-2,0,1)\), since we want to take the third row and subtract \(2\) times the first row.</p> <p>Putting it all together, our row reduction matrix is</p> \[\textbf{R}=\begin{pmatrix} 0&amp;-1&amp;0\\1&amp;1&amp;0\\-2&amp;0&amp;1 \end{pmatrix}\] <p>And if we multiply \(\textbf{A}\) by our row reduction matrix, we get</p> \[\textbf{R}\textbf{A}= \begin{pmatrix} 0&amp;-1&amp;0\\1&amp;1&amp;0\\-2&amp;0&amp;1 \end{pmatrix}\begin{pmatrix} 1&amp;1&amp;3&amp;2\\-1&amp;0&amp;-2&amp;-3\\2&amp;2&amp;6&amp;7 \end{pmatrix}\] \[\textbf{R}\textbf{A}= \begin{pmatrix} 1&amp;0&amp;2&amp;3\\ 0&amp;1&amp;1&amp;-1\\ 0&amp;0&amp;0&amp;3 \end{pmatrix}\] <p>The final steps of row reduction are then very simple. Divide row three by \(3\), and then add the correct multiples of it to the first two rows to get the final pivot column.</p> <hr/> <h1 id="finding-inverses-by-inspection">Finding inverses by inspection</h1> <p>We can also use these perspectives to find inverses relatively easily (depending on the matrix). Of course, this is relatively pointless for a \(2\times2\) since there is the very easy formula. But I try to do this when possible for larger matrices.</p> <p>Let’s say we want to invert our row reduction matrix from before</p> \[\textbf{R}=\begin{pmatrix} 0&amp;-1&amp;0\\1&amp;1&amp;0\\-2&amp;0&amp;1 \end{pmatrix}\] <p>This is a <em>good</em> candidate for the perspectives, because there are lots of zeros! The more zeros, the easier it is to use this. If a matrix has no nonzero entries, unless there is some amazingly obvious pattern, I would just use either the adjugate matrix if it’s \(3\times3\) or row reduction.</p> <p>So, to do this, we want to find combinations of the rows and columns to get the identity matrix.</p> <p>The most obvious one to me is that if we take just \(1\) of the third column, we get the third column of the identity matrix. So that means the third column of \(\textbf{R}^{-1}\) will be \((0,0,1)\)</p> \[\textbf{R}^{-1}=\begin{pmatrix} *&amp;*&amp;0\\**&amp;*&amp;0\\**&amp;*&amp;1 \end{pmatrix}\] <p>Also, if we take \(-1\) of the first row, we will get the second row of the identity matrix. So the second row should be \((-1,0,0)\)</p> \[\textbf{R}^{-1}=\begin{pmatrix} *&amp;*&amp;0\\-1&amp;0&amp;0\\**&amp;*&amp;1 \end{pmatrix}\] <p>I can also see that to get the second column of the identity matrix we can take \(1\) of column one and \(2\) of column three: \((0,1,-2)+2(0,0,1)=(0,1,0)\). So the second column of our inverse will be \((1,0,2)\)</p> \[\textbf{R}^{-1}=\begin{pmatrix} *&amp;1&amp;0\\-1&amp;0&amp;0\\**&amp;2&amp;1 \end{pmatrix}\] <p>Finally, to get the first column, we need a combination of columns one and three to get \((1,0,0)\), given that we have \(-1\) of column two (from the 2,1 entry), \(-1(-1,1,0)\). To cancel out the second entry, it looks like we need \(1\) of column one:</p> \[1(0,1,-2)-1(-1,1,0)=(1,0,-2)\] <p>So, to cancel out that \(-2\), we can take \(2\) of column three. Thus, the first column of the inverse is \((1,-1,2)\).</p> \[\textbf{R}^{-1}=\begin{pmatrix} 1&amp;1&amp;0\\-1&amp;0&amp;0\\2&amp;2&amp;1 \end{pmatrix}\] <p>We could have also looked at rows one and three individually! I chose to do the whole column at once because that was my personal preference. You can do it in whatever order you like. I just do whatever is most obvious to me first.</p>]]></content><author><name>Taylor Grant</name></author><category term="linear-algebra"/><category term="best"/><summary type="html"><![CDATA[How to simplify matrix multiplication with the best perspectives (and also find certain inverse matrices fast!)]]></summary></entry><entry><title type="html">Series Solutions Done Quick</title><link href="https://smashmath.github.io/blog/seriessolutions/" rel="alternate" type="text/html" title="Series Solutions Done Quick"/><published>2022-06-06T00:00:00+00:00</published><updated>2022-06-06T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/seriessolutions</id><content type="html" xml:base="https://smashmath.github.io/blog/seriessolutions/"><![CDATA[<h1 id="important-series">Important Series</h1> \[\begin{align} y&amp;=\sum_{n=0}^\infty a_nx^n\\ y'&amp;=\sum_{n=0}^\infty (n+1)a_{n+1}x^{n}&amp;=&amp;\sum_{n=0}^\infty na_{n}x^{n-1}\\ y''&amp;=\sum_{n=0}^\infty (n+1)(n+2)a_{n+2}x^{n}&amp;=&amp;\sum_{n=0}^\infty n(n-1)a_{n}x^{n-2}\\ \end{align}\] <p>Note: For the series with \(x^{n-1}\) or \(x^{n-2}\), you may have to have the starting index be \(n=1\) or \(n=2\) respectively. However, we can technically start at zero generally, because when \(n=0\), we get that \(na_{n}x^{n-1}=0\) too. Try not to worry too much about it, as it really speeds things along when you start them all on \(n=0\).</p> <hr/> <p>Here I will present the fast way to do series solutions. Essentially, we rearrange the differential equation such that reindexing is unnecessary. To explain how that works, let’s start by looking at Euler Equations.</p> <h1 id="euler-equations-intro">Euler Equations (Intro)</h1> <p>An Euler equation is of the form</p> \[\begin{equation} p_2x^2y''+p_1xy'+p_0y=0 \end{equation}\] <p>Where \(p_0,p_1,p_2\) are constants.</p> <p>The solutions to these equations are generally of polynomial form (similar to how solutions to constant coefficient linear ODEs are of exponential form), as we multiply by the independent variable with every differentiation, canceling the decreasing power which occurs when differentiating polynomials. We can use this fact to skip reindexing the series!</p> <p>Suppose we plug in our series solution to this differential equation. Because we have \(xy'\), we want to use \(y'=\sum_{n=0}^\infty na_{n}x^{n-1}\), and because we have \(x^2y''\), we want to use \(y''=\sum_{n=0}^\infty n(n-1)a_{n}x^{n-2}\), since that will give us \(x^n\) on all the terms. Notice how all three terms will also have \(a_n\)! Thus, when we combine it all, we get</p> \[\begin{equation} \sum_{n=0}^\infty(p_2n(n-1)+p_1n+p_0)a_nx^n=0 \end{equation}\] <p>Notice how the term multiplying \(a_nx^n\) is <em>almost</em> the characteristic polynomial if it was constant coefficients. This is actually called the “indicial equation”, which is slightly different. For the \(p_mt^my^{(m)}\) term in the ODE, the corresponding term in the indicial equation is \(p_mn(n-1)\ldots(n-m+1)=p_m\frac{n!}{(n-m)!}\).</p> <p>So, what happens when it isn’t exactly an Euler equation? Then you just group them up by shifted powers. For example,</p> \[(x^2+x+1)y''+(x^2+1)y'+(x^2+x+1)y=0\] <p>can be grouped up as follows:</p> \[(x^2y''+y'+y)+(xy''+y')+(y'')+(x^2y)+(xy)\] <p>Basically, you want groups of descending powers and orders. We’ll call these “EE groups”, for Euler Equation groups.</p> \[(x^2y''+y'+y)+\frac{1}{x}(x^2y''+xy')+\frac{1}{x^2}(x^2y'')+x^2(y)+x(y)\] <p>Then you will have to shift the index by some amount. To see this, compare \(y''\mapsto(n+1)(n+2)a_{n+2}\) and \(x^2y''\mapsto n(n-1)a_n\). The first is just the second, but \(n\) has been shifted <strong>up</strong> by 2.</p> <p>So if you have a positive power of \(x\) multiplying an Euler Equation, you shift the index of the EE group down by that power.</p> <p>If you have a positive power of \(x\) dividing an Euler Equation, you shift the index of the EE group up by that power.</p> <p>Let’s look at an example.</p> <h1 id="example-legendre-equation">Example (Legendre equation)</h1> \[(1-x^2)y''-2xy'+\alpha(\alpha+1)y=0\] <p>Notice how this is <em>almost</em> an Euler equation. We have an \(x^2y''\) and an \(xy'\) term, but there’s a \(y''\) all by itself bringing down the party. So let’s rearrange the equation as such:</p> \[y''-(x^2y''+2xy'-\alpha(\alpha+1)y)=0\] <p>And, well, we actually know the series expansion for both terms. We know that \(y''=\sum_{n=0}^\infty (n+1)(n+2)a_{n+2}x^{n}\) and</p> \[x^2y''+2xy'-\alpha(\alpha+1)y=\sum_{n=0}^\infty(n(n-1)+2n-\alpha(\alpha+1))a_nx^n\] <p>We can factor \(n(n-1)+2n-\alpha(\alpha+1)=(n-\alpha)(n+\alpha+1)\).</p> <p>So, plugging in our series will yield us</p> \[\sum_{n=0}^\infty \bigg( (n+1)(n+2)a_{n+2}-(n-\alpha)(n+\alpha+1)a_n \bigg)x^n=0\] <p>Observe that there was no point to even writing the sum. We could have jumped straight to</p> \[(n+1)(n+2)a_{n+2}-(n-\alpha)(n+\alpha+1)a_n=0\implies a_{n+2}=\frac{(n-\alpha)(n+\alpha+1)a_n}{(n+1)(n+2)}\] <p>And there’s our recurrence relation! That was quick. Now we can start plugging in \(n=0,1,2,\ldots\) to find our power series coefficients. Such as,</p> \[a_2=-\frac{\alpha(\alpha+1)}{2}a_0\] \[a_3=-\frac{(\alpha-1)(\alpha+2)}{3!}a_0\] <h1 id="the-straggler-coefficients">The Straggler Coefficients</h1> <p>When doing series solutions the normal way, often you get stragglers that end up outside of the sum. Rather than worry about them at all, we can skip that entirely by calculating the relationship between the first few coefficients directly from the original ODE itself.</p> <h2 id="the-direct-way-to-calculate-coefficients">The direct way to calculate coefficients</h2> <p>Once you see it, you may think “wait, we can just do that?” And the answer will be a resounding “YES”. Also, <strong>this is almost always the fastest way to do problems which ask you to find the first few terms of the series solution to an initial value problem, but not a closed form</strong>!</p> <p>Basically, for the initial value problem,</p> \[\begin{equation} y''=p(x)y'+q(x)y,\quad y(0)=y_0,\;y'(0)=y'_0 \end{equation}\] <p>First, we know that \(a_0=y_0\) and \(a_1=y'_0\). That directly follows from Taylor’s theorem.</p> <p>Now, check out what happens if we just plug in \(x=0\) to the equation. Note that if \(y=a_0+a_1x+a_2x^2+\ldots\), then using Taylor’s theorem we know that \(a_n=\frac{y^{(n)}(0)}{n!}\). Which implies that \(y^{(n)}(0)=n!\,a_n\).</p> \[y''(0)=p(0)y'(0)+q(0)y(0)\implies 2!\,a_2=p(0)y'_0+q(0)y_0\] <p>Thus, we can solve for \(a_2\) directly:</p> \[a_2=\frac{1}{2}(p(0)y'_0+q(0)y_0)\] <p>There is really no need to memorize this formula, but just remember that you can use this method to directly calculate the coefficients which are left out of the series (or cannot be directly computed from the recurrence relation).</p> <p>But wait, there’s more! You can find more than just \(a_2\), if you differentiate the whole ODE.</p> \[y'''=p(x)y''+(p'(x)+q(x))y'+q'(x)y\] <p>Then you can just plug in \(x=0\) again!</p> <p><strong>Important note</strong>: If there is a coefficient on the \(y''\) term, then it is often advantageous NOT to divide it out. This usually simplifies the differentiation greatly, since you’re doing the product rule and not the quotient rule. And there are tricks for multiple applications of the product rule (like using Pascal’s triangle), but nothing like that for the quotient rule.</p> <h1 id="when-the-ivp-is-not-centered-at-0">When the IVP is not centered at 0</h1> <p>This is actually really great. You can do a quick change of variables and then carry on as if \(x_0=0\). Just do \(t=x-x_0\implies x=t+x_0\). So,</p> \[P(x)y''+Q(x)y'+R(x)y=0,\quad y(x_0)=y_0,\;y'(x_0)=y'_0\] <p>becomes</p> \[P(t+x_0)y''+Q(t+x_0)y'+R(t+x_0)y=0,\quad y(0)=y_0,\;y'(0)=y'_0\] <p>And then you just do as you do normally. No need to write \(y=\sum_{n=0}^\infty a_n(x-x_0)^n\), or to find the Taylor series of \(P,Q,R\) centered around \(x_0\). Do you <em>want</em> to do that? Because I don’t.</p> <p>Instead, just do the normal solution method with \(y=\sum_{n=0}^\infty a_nt^n\).</p> <h1 id="more-examples">More Examples</h1> <p>Find closed forms for the general solution of</p> \[(x^2-4x-1)y''-2(x-2)y'+2y=0\] <p>We rearrange it into EE groups:</p> \[(x^2y''-2xy'+2y)-4(xy''-y')-y''=0\] <p>The first term will be \(x^2y''-2xy'+2y\mapsto (n(n-1)-2n+2)a_n=(n-1)(n-2)a_n\).</p> <p>The second term will be \(4(n(n-1)-n)a_n=4n(n-2)a_n\) but shifted up by one, since it’s a power below an Euler Equation. Thus, \(4(xy''-y')\mapsto 4(n+1)(n-1)a_{n+1}\).</p> <p>And finally, we are very familiar with \(y''\mapsto(n+1)(n+2)a_{n+2}\).</p> <p>In total, giving us</p> \[(n-1)(n-2)a_n-4(n+1)(n-1)a_{n+1}-(n+1)(n+2)a_{n+2}=0\] <p>Solving for the highest index term,</p> \[a_{n+2}=(n-1)\frac{(n-2)a_n-4(n+1)a_{n+1}}{(n+1)(n+2)}\] <p>The most interesting thing to observe is the \((n-1)\) multiplying the whole thing. This means that plugging in \(n=1\) gives us</p> \[a_3=0\] <p>And if we plug in anything higher than \(n=1\), such as \(n=2\), we get</p> \[a_4=1\frac{0a_2-8a_3}{12}=0\] <p>So it seems our solutions don’t have Taylor series which go higher than a quadratic. Plugging in \(n=0\) yields</p> \[a_2=a_0+2a_1\] <p>So we can say that our solution is</p> \[y=a_0+a_1x+(a_0+2a_1)x^2=a_0(1+x^2)+a_1(x+2x^2)\] <p>We can let \(a_0\) and \(a_1\) be our arbitrary constants, and it gives us the <a href="../normalized/" target="_blank">normalized solutions</a>!</p> \[y=c_1(1+x^2)+c_2(x+2x^2)\] <p>Setting \(c_1=-2,\;c_2=1\) gives us that \(y=x-2\) is a solution. As a pro-tip, you could have actually seen that solution by inspection!</p> <p>In general, if you have a linear function on \(y'\) and the slope constant on \(y\),</p> \[P(x)y''+(mx+b)y'-my=0\] <p>then \(y=mx+b\) will be a solution. This is especially useful if you have something like</p> \[P(x)y''-xy'+y=0\] <p>Then \(y=x\) is a solution to this, for example.</p> <hr/> <p>Find the solution to</p> \[y''+(t-1)y'+2y=0,\quad y(1)=0,\;y'(1)=1\] <p>We can start by simplifying the problem with a change of variables which will center the initial value problem around zero: \(x=t-1\). Plugging that in gives us</p> \[y''+xy'+2y=0,\quad y(0)=0,\;y'(0)=1\] <p>We can rearrange it into two EE groups</p> \[y''+(xy'+2y)=0\] <p>\(xy'+2y\) is fairly straightforward as \((n+2)a_n\). And, once again, \(y''\mapsto(n+1)(n+2)a_{n+2}\). So our recurrence relation is</p> \[((n+1)(n+2)a_{n+2})+((n+2)a_n)=0\] <p>Since \(n\geq0\implies n+2\neq0\), we can divide out the \(n+2\) from the equation to get</p> \[a_{n+2}=-\frac{a_n}{n+1}\] <p>Now, we can find our coefficients.</p> \[a_2=-a_0=0\] <p>We can see that every even \(n\) will result in zero. So we can just focus on the odd terms. In cases where every other coefficient is zero, it is sometimes the case that the solution (or part of it) is a function of \(x^2\). Let’s keep that in mind.</p> \[a_3=-\frac{a_1}{2}=-\frac{1}{2}\] \[a_5=\frac{1}{2\cdot 4}=\frac{1}{2^2(2!)}\] <p>Often, it can make it easier to see the pattern when you force a factorial in there.</p> \[a_7=-\frac{1}{2^2(2!)\cdot 6}=-\frac{1}{2^3(3!)}\] <p>And we can see that this pattern will repeat. So it appears that</p> \[a_{2n+1}=\frac{\left(-\frac{1}{2}\right)^n}{n!}\] <p>Making our solution \(y=\sum_{n=0}^\infty \frac{\left(-\frac{1}{2}\right)^nx^{2n+1}}{n!}\), which we can manipulate into</p> \[y=x\sum_{n=0}^\infty \frac{\left(-\frac{x^2}{2}\right)^n}{n!}=xe^{-\frac{x^2}{2}}\] <p>Finally, we undo the substitution to get our actual solution</p> \[y=(t-1)e^{-\frac{(t-1)^2}{2}}\] <hr/> <p>Note, we could have sorta cheesed it by using the direct method. By solving for \(y''\), we get</p> \[y''=-(t-1)y'-2y\] <p>Then, plugging in \(t=1\) and then differentiating, we get the following:</p> \[2a_2=0a_1-2(0)=0\] \[y'''=-(t-1)y''-3y'\] \[3!a_3=-3a_1\implies a_3=-\frac{1}{2}\] \[y^{(4)}=-(t-1)y'''-4y''\] <p>We can observe once again, that the even terms produce zero.</p> \[y^{(5)}=-(t-1)y^{(4)}-5y'''\] \[5!a_5=-5(3!)a_3\implies a_5=\frac{1}{8}\] <p>And so on…</p>]]></content><author><name>Taylor Grant</name></author><category term="differential-equations"/><category term="best"/><summary type="html"><![CDATA[the fast way to do series solutions. no reindexing required.]]></summary></entry><entry><title type="html">New Ways to Calculate Normalized Solutions to Linear Constant-Coefficient Differential Equations</title><link href="https://smashmath.github.io/blog/newnormalized/" rel="alternate" type="text/html" title="New Ways to Calculate Normalized Solutions to Linear Constant-Coefficient Differential Equations"/><published>2022-01-12T00:00:00+00:00</published><updated>2022-01-12T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/newnormalized</id><content type="html" xml:base="https://smashmath.github.io/blog/newnormalized/"><![CDATA[<h2 id="the-method">The Method</h2> <p>This is actually ridiculously simple. All you have to do is solve <em>one</em> system of first-order equations with a really simple matrix.</p> <p>To get the normalized solutions \(Y_1,\ldots,Y_n\), to</p> <p>\begin{equation}\label{prob} y^{(n)}=p_0y+p_1y’+\ldots+p_{n-1}y^{(n-1)} \end{equation}</p> <p>One only needs to solve the \(n\times n\) system,</p> \[\begin{equation}\label{method} \textbf{Y}'= \begin{pmatrix} 0&amp;0&amp;\dots&amp;0&amp;p_0\\ 1&amp;0&amp;\dots&amp;0&amp;p_1\\ 0&amp;1&amp;\dots&amp;0&amp;p_2\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\ 0&amp;0&amp;\dots&amp;1&amp;p_n \end{pmatrix} \textbf{Y},\quad \textbf{Y}(0)= \begin{pmatrix} 1\\0\\\vdots\\0\\0 \end{pmatrix} \end{equation}\] <p>This is very easy to put into a computer and quick to solve.</p> <p>There are also some interesting consequences and relationships between the normalized solutions that this reveals.</p> <h2 id="wait-what-are-normalized-solutions">Wait, what are normalized solutions?</h2> <p>I made a <a href="/math/normalized" target="_blank">post</a> about them before, but here is a short explanation.</p> <p>Normalized solutions make solving initial value problems easy. They are the solutions \(Y_1,\ldots,Y_n\) such that the solution to the initial value problem with initial conditions \(y(t_0)=y_0,\ldots,y^{(n-1)}(t_0)=y^{(n-1)}_0\) is simply</p> <p>\begin{equation} \label{normsol} y=y_0Y_1+\ldots+y^{(n-1)}_0Y_n \end{equation}</p> <p>Basically, the constants are just the initial conditions. This way, you don’t have to solve for the constants.</p> <p>Consequently, each of the solutions satisfy the \(n\) initial value problems</p> \[\begin{equation} \begin{array}{cccc} Y_1(t_0)=1&amp;Y_1'(t_0)=0&amp;\ldots&amp;Y_1^{(n-1)}(t_0)=0\\ Y_2(t_0)=0&amp;Y_2'(t_0)=1&amp;\ldots&amp;Y_2^{(n-1)}(t_0)=0\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\\ Y_n(t_0)=0&amp;Y_n'(t_0)=0&amp;\ldots&amp;Y_n^{(n-1)}(t_0)=1\\ \end{array} \end{equation}\] <p>Note that these will be the entries of their Wronskian, and at \(t_0\) the Wronskian is the identity matrix!</p> <p>Now you may be asking yourself, “why would I want to solve \(n\) initial value problems to get these solutions?” Apart from the fact that I have just detailed a method to make it solving just one initial value problem, in theory their use is solving multiple initial value problems.</p> <p>Say you have a differential equation which models some physical model (like a circuit or something idk I’m not an engineer), and you want to know what the response to many different inputs is. Say you have 100 different initial conditions you want to test. Rather than solve for the general solution and then solve 100 different systems of equations for the constants, just solve for the \(n\) normalized solutions and then the solution to the 100 different initial value problems will require no solving.</p> <h2 id="proof-and-derivation">Proof and Derivation</h2> <h3 id="proof">Proof</h3> <p>Consider the initial value problem</p> <p>\begin{equation} \label{ivp} y^{(n)}=p_0y+p_1y’+\ldots+p_{n-1}y^{(n-1)},\quad y(0)=y_0,\ldots,y^{(n-1)}(0)=y^{(n-1)}_0 \end{equation}</p> <p>If the normalized solutions are \(Y_1,\ldots,Y_n\), then the solution is \eqref{normsol}. This can be rewritten as</p> \[y_0Y_1+\ldots+y^{(n-1)}_0Y_n=\textbf{Y}\cdot\textbf{y}_0\] <p>Letting \(\textbf{Y}=(Y_1,\ldots,Y_n)\) be the vector of our normalized solutions, and \(\textbf{y}_0=(y_0,\ldots,y^{(n-1)}_0)\) the vector of our initial conditions.</p> <p>Or, alternatively,</p> \[y=\textbf{Y}^T\textbf{y}_0\] <p>It is a fact that the \(n\)-th order \eqref{ivp} can be rewritten as a system of first order equations with the change of variables \(y_1=y,\ldots,y_n=y^{(n-1)}\). Let \(B\) be the matrix for which the matrix form of the reduced system is</p> \[\begin{equation} \textbf{y}'=B\textbf{y},\quad \textbf{y}(0)=\textbf{y}_0 \end{equation}\] \[B=\begin{pmatrix} 0&amp;1&amp;0&amp;\dots&amp;0\\ 0&amp;0&amp;1&amp;\dots&amp;0\\ \vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\\ 0&amp;0&amp;0&amp;\dots&amp;1\\ p_0&amp;p_1&amp;p_2&amp;\dots&amp;p_{n-1} \end{pmatrix} =\left(\begin{array}{ccccccc} \begin{array}{cc} 0&amp;I_{n-1} \end{array}\\\vec{p} \end{array}\right)\] <p>The solution to this system is \(\textbf{y}(t)=e^{Bt}\textbf{y}_0\).</p> <p>From our definition of our variables, \(y=y_1=(1,\ldots,0)\cdot\textbf{y}\). Denoting the first standard basis vector of \(\mathbb{R}^n\), \(\textbf{e}_1=(1,\ldots,0)\), we can rewrite this neatly as</p> \[y=\textbf{e}_1^T\textbf{y}\] \[y=\textbf{e}_1^Te^{Bt}\textbf{y}_0\] \[y=\left(\left(e^{Bt}\right)^T \textbf{e}_1\right)^T\textbf{y}_0\] \[y=\left(e^{B^Tt}\textbf{e}_1\right)^T\textbf{y}_0\] <p>Now we have two solutions to this initial value problem: \(y=\textbf{Y}^T\textbf{y}_0=\left(e^{B^Tt}\textbf{e}_1\right)^T\textbf{y}_0\). Therefore, by the existence and uniqueness theorem,</p> <p>\begin{equation} \textbf{Y}=e^{B^Tt}\textbf{e}_1 \end{equation}</p> <p>However, \(\textbf{Y}=e^{B^Tt}\textbf{e}_1\) is the unique solution to the initial value problem</p> \[\begin{equation} \textbf{Y}'= B^T \textbf{Y},\quad \textbf{Y}(0)= \textbf{e}_1 \end{equation}\] <p>Q.E.D.</p> <h3 id="derivation">Derivation</h3> <p>Now, the proof may seem like a fine derivation. But I think there is a more intuitive approach to reveal this.</p> <p>We start by letting \(s\) be any root/solution of the characteristic polynomial</p> \[s^n=p_0+p_1s+\ldots+p_{n-1}s^{n-1}\] <p>Then \(y=e^{st}\) must be a solution to \eqref{prob}, with the initial conditions \(y(0)=1,y'(0)=s,\ldots,y^{n-1}(0)=s^{n-1}\). By the uniqueness theorem, it follows that</p> \[e^{st}=Y_1+sY_2+\ldots+s^{n-1}Y_n\] <p>Now, I was certainly surprised by this formula at first. But it checks out! For example, \(e^{it}\) is a solution to \(y''=-y\), and the normalized solutions are \(Y_1=\cos(t),Y_2=\sin(t)\). And indeed</p> \[e^{it}=\cos(t)+i\sin(t)\] <p>is Euler’s formula! I enjoyed verifying this for some other problems like \(e^{kt}\) and \(y''=-k^2y+2ky\).</p> <p>Differentiate this expression,</p> \[se^{st}=Y_1'+sY_2'+\ldots+s^{n-1}Y_n'\] \[s(Y_1+sY_2+\ldots+s^{n-1}Y_n)=Y_1'+sY_2'+\ldots+s^{n-1}Y_n'\] \[sY_1+s^2Y_2+\ldots+s^{n}Y_n=Y_1'+sY_2'+\ldots+s^{n-1}Y_n'\] \[sY_1+s^2Y_2+\ldots+(p_0+p_1s+\ldots+p_{n-1}s^{n-1})Y_n=Y_1'+sY_2'+\ldots+s^{n-1}Y_n'\] <p>Combine terms by the power of \(s\).</p> \[p_0Y_n+s(Y_1+p_1Y_n)+\ldots+s^{n-1}(Y_{n-1}+p_{n-1}Y_n)=Y_1'+sY_2'+\ldots+s^{n-1}Y_n'\] <p><em>something something</em> linear independence.</p> \[\begin{gather*} Y_1'=p_0Y_n\\ Y_2'=Y_1+p_1Y_n\\ \vdots\\ Y_n'=Y_{n-1}+p_{n-1}Y_n \end{gather*}\] <p>We also know by the definition of our normalized solutions that</p> \[Y_1(0)=1,Y_2(0)=0,\ldots,Y_n(0)=0\] <p>In matrix form,</p> \[\begin{pmatrix} Y_1'\\Y_2'\\\vdots\\Y_{n-1}'\\Y_n' \end{pmatrix}= \begin{pmatrix} 0&amp;0&amp;\dots&amp;0&amp;p_0\\ 1&amp;0&amp;\dots&amp;0&amp;p_1\\ 0&amp;1&amp;\dots&amp;0&amp;p_2\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\ 0&amp;0&amp;\dots&amp;1&amp;p_n \end{pmatrix} \begin{pmatrix} Y_1\\Y_2\\\vdots\\Y_{n-1}\\Y_n \end{pmatrix},\quad \begin{pmatrix} Y_1(0)\\Y_2(0)\\\vdots\\Y_{n-1}(0)\\Y_n(0) \end{pmatrix}= \begin{pmatrix} 1\\0\\\vdots\\0\\0 \end{pmatrix}\] <p>And hey, that’s the transpose of our \(B\) matrix!</p> \[\textbf{Y}'= B^T \textbf{Y},\quad \textbf{Y}(0)= \textbf{e}_1\] <h2 id="some-interesting-consequences">Some Interesting Consequences</h2> <p>Now the most interesting equation that comes out of this is</p> \[\begin{equation} \label{proportional} Y_1'=p_0Y_n \end{equation}\] <p>Implying that the integral of \(Y_n\) is proportional to \(Y_1\). Not going to lie, I don’t have an intuitive explanation for why this is true, but it is verifiable. :man_shrugging: Below I discuss the Laplace Transforms of the normalized solutions, and the properties are more obvious when looking at those.</p> <p>One observation to make is that if \(p_0=0\), then the expression for \(Y_1\) becomes</p> \[Y_1'=0,\quad Y_1(0)=1\] <p>And the solution to that is just \(Y_1(t)=1\). Since that function obviously satisfies the conditions, it makes sense that we can just jump right ahead to that being the first normalized solution. Similarly, depending on the multiplicity of the root \(0\), the first couple of normalized solutions should be \(Y_k=\frac{t^k}{k!}\).</p> <p>Also, we see that for \(1\leq k\leq n-1\)</p> \[\begin{equation} \label{recursive} Y_k=Y_{k+1}'-p_kY_n \end{equation}\] <p>Giving a recursive formula for each normalized solution.</p> <p>Now, both of these equations involve \(Y_n\). So if you can solve for that, then you can get all of the other solutions. You can get \(Y_1\) by integrating it (or, alternatively, solve for \(Y_1\) and then differentiate it to get \(Y_n\), assuming \(p_0\neq0\)), and you can directly get \(Y_{n-1}\) with</p> \[Y_{n-1}=Y_n'-p_{n-1}Y_n\] <p>In the case with complex roots, especially with a nonzero real part, differentiation is going to be easier than integration.</p> <p>And then you could get \(Y_{n-2}\) with</p> \[Y_{n-2}=Y_{n-1}'-p_{n-2}Y_n\] <p>And so on.</p> <h2 id="some-notes-on-the-laplace-transforms">Some notes on the Laplace Transforms</h2> <p>One thing to note is that \(Y_n\) is particularly easy to find using the Laplace Transform. In fact, it turns out to actually be the weight function, giving it even more applications than one (including me) might originally expect. The weight function, briefly, is the inverse Laplace Transform of the transfer function (the reciprocal of the characteristic polynomial),</p> \[\mathscr{L}\{Y_n\}=\frac{1}{s^n-p_{n-1}s^{n-1}-\ldots-p_1s-p_0}\] <p>For convenience, we will from now on denote \(p(s)=s^n-p_{n-1}s^{n-1}-\ldots-p_1s-p_0\)</p> <p>The weight function’s primary application is in solving for particular solutions. As the unique solution to the initial value problem</p> \[y^{(n)}=p_0y+p_1y'+\ldots+p_{n-1}y^{(n-1)}+g(t),\quad y(0)=0,\ldots,y^{(n-1)}(0)=0\] <p>will be the convolution</p> \[y_p(t)=Y_n(t)*g(t)=\int_0^tY_n(t-\tau)g(\tau)\,d\tau\] <p>Using superposition, this gives a formula for the solution to the general nonhomogeneous initial value problem</p> \[y^{(n)}=p_0y+p_1y'+\ldots+p_{n-1}y^{(n-1)}+g(t),\quad y(0)=y_0,\ldots,y^{(n-1)}(0)=y^{(n-1)}_0\] \[y(t)=y_0Y_1(t)+\ldots+y^{(n-1)}_0Y_n(t)+Y_n(t)*g(t)\] <p>My conjecture is that the reason \(Y_n\) is so useful/shows up so much in the above formulas is exactly because it is the weight function.</p> <p>There are similar formulas for the Laplace Transforms of the other normalized solutions.</p> \[\begin{gather*} \mathscr{L}\{Y_n\}=\frac{1}{p(s)}\\ \mathscr{L}\{Y_{n-1}\}=\frac{s-p_{n-1}}{p(s)}\\ \mathscr{L}\{Y_{n-2}\}=\frac{s^2-p_{n-1}s-p_{n-2}}{p(s)}\\ \vdots\\ \mathscr{L}\{Y_1\}=\frac{s^{n-1}-p_{n-1}s^{n-2}+\ldots-p_2s-p_1}{p(s)} \end{gather*}\] <p>In general, the numerator of \(\mathscr{L}\{Y_k\}\) is just the characteristic polynomial after lobbing off the first \(k\) lowest degree terms, and then dividing by the lowest power of \(s\) (\(s^k\)).</p> \[s^kp(s)\mathscr{L}\{Y_k\}=s^{n}-\sum_{i=k}^{n-1}p_{i}s^{i}\] <p>Viewing normalized solutions this way makes the properties \eqref{proportional} and \eqref{recursive} much easier to see and verify.</p> <p>Now, when solving a system of initial value problems,</p> \[\textbf{x}'=A\textbf{x},\quad \textbf{x}(0)=\textbf{x}_0\] <p>In general, one can use the Laplace Transform to solve it as</p> \[(sI-A)\mathscr{L}\{\textbf{x}\}=\textbf{x}_0\] \[\mathscr{L}\{\textbf{x}\}=(sI-A)^{-1}\textbf{x}_0\] <p>For our system,</p> \[\mathscr{L}\{\textbf{Y}\}=(sI-B^T)^{-1}\textbf{e}_1\] <p>Thus, we can see that the Laplace Transforms of our normalized solutions will be the first column of \((sI-B^T)^{-1}\). Consequently, the numerators will be the first column of \((sI-B^T)^{adj}\), which will also be the cofactors of the first row of \(sI-B^T\), which will also be the cofactors of the first column of \(sI-B\). Thankfully, because \(B\) is relatively simple, the cofactors are not too difficult to calculate.</p> <p>Of course, why do any of that when I have most <em>graciously</em> provided the results for you, already?</p> <h2 id="normalized-solutions-recap">Normalized solutions recap</h2> <p>Now here are all of the ways I know of to calculate normalized solutions. I am going to compare and assess each of their uses.</p> <ol> <li>Solve all \(n\) initial value problems individually</li> <li>Invert the Wronskian</li> <li>Solve the one system of initial value problems detailed above \eqref{method}</li> <li>Get \(Y_n\) and obtain the others recursively</li> </ol> <p>You could also take the first row of \(e^{Bt}\) or take the inverse laplace transform of the first column of \((sI-B^T)^{-1}\), but these are just longer and more difficult/roundabout ways to do method 3.</p> <p>When discussing using a “computer” I’m generally imagining putting it into Wolfram Alpha or MATLAB. I usually prefer doing stuff by hand though, to be honest.</p> <p>Generally, the pattern is that all of these methods are “fine” for the second order case and require <em>approximately</em> the same amount of work by hand. Some are better than others for the third order case using certain shortcuts. But for fourth order and higher, doing things by hand gets to be too difficult.</p> <h3 id="solving-n-problems-individually">Solving n problems individually</h3> <p>You could solve the \(n\) initial value problems with the following initial conditions</p> \[\begin{array}{cccc} Y_1(t_0)=1&amp;Y_1'(t_0)=0&amp;\ldots&amp;Y_1^{(n-1)}(t_0)=0\\ Y_2(t_0)=0&amp;Y_2'(t_0)=1&amp;\ldots&amp;Y_2^{(n-1)}(t_0)=0\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\\ Y_n(t_0)=0&amp;Y_n'(t_0)=0&amp;\ldots&amp;Y_n^{(n-1)}(t_0)=1\\ \end{array}\] <p>Maybe do this once to see how tedious it can be and why you will never want to do it ever again.</p> <h3 id="inverting-the-wronskian">Inverting the Wronskian</h3> <p>This requires inverting an \(n\times n\) matrix. While this is very manageable for a \(2\times 2\) and tolerable for a \(3\times 3\) (using the adjugate) in the second and third order cases respectively, this becomes very tedious to do by hand for anything larger.</p> <p>The general idea is that to solve an initial value problem you get your general solution \(y=c_1y_1+\ldots+c_ny_n\). Then you solve the system of equations</p> \[\begin{array}{cccccc} c_1y_1(0)&amp;+&amp;\ldots&amp;+&amp;c_ny_n(0)&amp;=&amp;y_0\\ c_1y_1'(0)&amp;+&amp;\ldots&amp;+&amp;c_ny_n'(0)&amp;=&amp;y_0'\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\ c_1y_1^{(n-1)}(0)&amp;+&amp;\ldots&amp;+&amp;c_ny_n^{(n-1)}(0)&amp;=&amp;y_0^{(n-1)}\\ \end{array}\] <p>In augmented matrix form, this ends up being</p> \[\left( \begin{array}{ccc|c} y_1(0)&amp;\ldots&amp;y_n(0)&amp;y_0\\ y_1'(0)&amp;\ldots&amp;y_n'(0)&amp;y_0'\\ \vdots&amp;\ddots&amp;\vdots&amp;\vdots\\ y_1(0)^{(n-1)}&amp;\ldots&amp;y_n^{(n-1)}(0)&amp;y_0^{(n-1)}\\ \end{array} \right)\] <p>Since no matter what the initial conditions are, you do the same row operations to solve for the coefficients, we can put all \(n\) of our initial value problems in \(n\) different columns.</p> \[\left( \begin{array}{ccc|ccc} y_1(0)&amp;\ldots&amp;y_n(0)&amp;1&amp;\ldots&amp;0\\ y_1'(0)&amp;\ldots&amp;y_n'(0)&amp;0&amp;\ldots&amp;0\\ \vdots&amp;\ddots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\ y_1(0)^{(n-1)}&amp;\ldots&amp;y_n^{(n-1)}(0)&amp;0&amp;\ldots&amp;1\\ \end{array} \right)\] <p>The coefficient matrix is our Wronskian, so if we abbreviate \(W[y_1,\ldots,y_m](0)\) as \(W\), we can rewrite that system simply as</p> \[\left( \begin{array}{c|c} W&amp;I \end{array} \right)\] <p>And, well, putting that augmented matrix into reduced row echelon form is just a general method for finding \(W^{-1}\).</p> \[\left( \begin{array}{c|c} W&amp;I \end{array} \right) \to \left( \begin{array}{c|c} I&amp;W^{-1} \end{array} \right)\] <p>Thus, the columns of \(W^{-1}\) give you the coefficients for each of the normalized solutions.</p> <p>Now, this gets difficult quickly because you have to actually find \(y_j^{(i)}(0)\). Meaning you have to differentiate all \(n\) of your solutions \(n-1\) times. For the second order case, it’s just one derivative, meaning 2 total. For the third order case, however, it’s 6 differentiations. Then it’s 12, 20, etc. And differentiation can be kind of tedious, so it is not ideal.</p> <p>So even if you do the inversion using Wolfram Alpha, you have to calculate the entries manually.</p> <h3 id="solving-the-one-system-of-first-order-ivps">Solving the one system of first order IVPs</h3> <p>This method requires no differentiating, but it requires finding the eigenvectors of an \(n\times n\) matrix. This is trivial for a computer, and for this reason it is by far the fastest way to compute them that way. This system is very easy to put into Wolfram Alpha, for example.</p> <p>Doing this by hand, however, is a different story. Even just computing the characteristic polynomial can be tedious to do by hand. That said! The roots of the characteristic polynomial of \(B^T\) are the same as the roots of the characteristic polynomial of \eqref{prob}. So, assuming you already factored the characteristic polynomial in the process of solving the for the homogeneous solutions, you know the eigenvalues going in.</p> <p>Furthermore, the matrix is simple enough that finding eigenvectors is not too difficult. In fact, I found a formula for the eigenvectors:</p> \[v_\lambda=\begin{pmatrix} \lambda^{n-1}-p_{n-1}\lambda^{n-2}-\ldots-p_2\lambda-p_1\\ \lambda^{n-2}-p_{n-1}\lambda^{n-3}-\ldots-p_2\\ \vdots\\ \lambda-p_{n-1}\\ 1 \end{pmatrix}\] <p>In general, the entries are</p> \[v_i=\left(\lambda^{n-i}-\sum_{k=i}^{n-1}p_k\lambda^{k-i}\right)v_n\] <p>Hold on… What??? It’s the numerators of the Laplace Transforms evaluated at each of the roots!</p> <p>Now, generalized eigenvectors for the repeated case are not quite as easy to find. I have not found a simple general formula, and I don’t think it would be worth looking for.</p> <p>Finding eigenvectors when you already know the eigenvalues is basically equivalent to finding vectors in the null space of \(B^T-\lambda I\). Of course, generalized eigenvectors complicate things a bit, but overall it’s just row reduction.</p> <p>However, once you find all the eigenvectors, you have to solve the initial value problem. Luckily, there is a shortcut which is to calculate the cofactors of the first row of the matrix of eigenvectors (or modal matrix in the general case) and then divide by the determinant. This is equivalent to using Cramer’s rule, which is actually not a bad idea in this case since the \(\textbf{b}\) vector has only one nonzero entry, making it a good choice to expand the determinant along its column.</p> <p>In conclusion, this is not the best way to do it by hand, in my opinion. Choosing between this and inverting the Wronskian, I would probably choose this, though, because I find solving these systems a lot more fun than differentiating a bunch of different functions and then inverting a matrix.</p> <h3 id="using-the-recursive-formula">Using the recursive formula</h3> <p>For this method, you can solve just <em>one</em> \(n\)-th order equation to get \(Y_n\), and then use the recursive formula \eqref{recursive}</p> \[Y_k=Y_{k+1}'-p_kY_n\] <p>And if \(Y_n\) is easy to integrate, then you can also use \eqref{proportional}</p> \[Y_1'=p_0Y_n\] <p>Overall, though, this would probably be the second worst method for a computer. Because you would still have to do \(n\) separate commands. However, they wouldn’t be quite as bad as inputting \(n\) initial value problems.</p> <p>But by hand, this is not bad.</p> <h2 id="summary">Summary</h2> <ol> <li>Solving \(n\) problems individually: Requires solving \(n\) \(n\)-th order initial value problems. This includes factoring an \(n\)-th degree polynomial, evaluating \(n^2-n\) derivatives and solving \(n\) systems of \(n\) equations with \(n\) variables.</li> <li>Inverting the Wronskian requires no initial value problem solving, but instead turns it into a linear algebra problem where one must invert an \(n\times n\) matrix. You, of course first need to factor an \(n\)-th degree polynomial, but then you also need to differentiate and evaluate your \(n\) solutions (making that \(n^2-n\) derivatives total).</li> <li>Solving the one system of first order IVPs unsurprisingly requires solving one system of \(n\) first order initial value problems. This requires no derivatives but requires solving for a basis of \(n\) eigenvectors and solving one system of \(n\) equations with \(n\) variables.</li> <li>Using the recursive formula requires solving one \(n\)-th order initial value problem and then differentiating (\(n-1\) functions total) and linearly combining functions.</li> </ol> <p>I would say that, by hand, one \(n\)-th order IVP is better than \(n\) first order IVPs (as well as \(n\) \(n\)-th order IVPs of course).</p> <p>Finally ranking them by which method would be best to use in my opinion:</p> <table> <thead> <tr> <th> </th> <th>\(n\) \(n\)-th order IVPs</th> <th>Wronskian inverse</th> <th>\(n\) first order IVPs</th> <th>Recursive Formula</th> </tr> </thead> <tbody> <tr> <td>By hand</td> <td>4th</td> <td>3rd</td> <td>2nd</td> <td>1st</td> </tr> <tr> <td>Using a computer</td> <td>4th</td> <td>2nd</td> <td>1st</td> <td>3rd</td> </tr> </tbody> </table> <p>For non-constant coefficients, the Wronskian inverse is really your only option.</p>]]></content><author><name>Taylor Grant</name></author><category term="differential-equations"/><category term="normalized-solutions"/><summary type="html"><![CDATA[The fastest way for a computer, and a fast way by hand.]]></summary></entry><entry><title type="html">Systems of Linear Difference Equations</title><link href="https://smashmath.github.io/blog/discretesystems/" rel="alternate" type="text/html" title="Systems of Linear Difference Equations"/><published>2021-12-14T00:00:00+00:00</published><updated>2021-12-14T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/discretesystems</id><content type="html" xml:base="https://smashmath.github.io/blog/discretesystems/"><![CDATA[<h1 id="intro">Intro</h1> <p>In differential equations, students learn about the system</p> \[\textbf{x}'(t)=A\textbf{x}(t)\] <p>But here we will discuss the discrete system</p> \[\textbf{x}(n+1)=A\textbf{x}(n)\] <p>Where \(n\in\mathbb{Z}_0^+\), and instead of a continuous function, we get a discrete set of points</p> \[\textbf{x}(0),\textbf{x}(1),\ldots,\textbf{x}(n),\ldots\] <h2 id="solution-behavior">Solution Behavior</h2> <p>With systems of DEs, its the sign of the eigenvalues that tells you the behavior. Negative is asymptotically stable (going to zero), positive is unstable, and zero is statically stable.</p> <p>However, in these discrete systems, it’s the <em>magnitude</em>. To understand why, we need to know what our solutions even look like!</p> <p>With systems of DEs, the basic example is</p> \[x'(t)=ax(t)\] <p>Which has the solution \(x(t)=ce^{at}\). Therefore, its the sign of \(a\) which determines the behavior. However, with the discrete system</p> \[x(n+1)=ax(n)\] <p>the solutions are of the form \(x(t)=ca^n\). So we still get the behavior of asymptotically stable (going to zero), unstable, and types of static stability, but it’s not about the sign anymore, and there are some new types of behavior. I encourage you to try and think about what the differences could be before reading on! It’s an interesting thing to try to reason out, in my opinion.</p> <p>Repeatedly multiplying a number \(a\) to some nonzero initial number \(c\) gets big when \(| a | &gt; 1\), goes to zero when \(|a|&lt;1\), and has a consistent magnitude when \(|a|=1\).</p> <p>But the similarities don’t stop there. The solutions are still generally obtained by finding eigenvectors and eigenvalues, just like with DE systems. But instead of having solutions of the form \(\textbf{x}(t)=ce^{\lambda t}\textbf{v}\), they are of the form \(\textbf{x}(n)=c\lambda^n\textbf{v}\).</p> <h3 id="new-solution-behaviors">New Solution Behaviors</h3> <p>That said, there are type of behavior fpr discrete systems which does not occur for systems of ODEs. Specifically, if \(a\leq0\).</p> <p>When \(a&lt;0\), our solution is \((-a)^n=(-1)^na^n\), giving us an alternating pattern where each iteration flips its sign. This creates a sense of bouncing back and forth. With \(a&lt;-1\), it’s bouncing back and forth with ever increasing magnitude. With \(-1&lt;a&lt;0\), it’s bouncing back and forth towards the origin. And if \(a=-1\), it’s simply bouncing back and forth without getting closer or further away.</p> <p>When \(a=0\), solutions of this type start constant, and then immediately disappear after finitely many iterations. For this reason, it is convenient to explicitly define the function \(0^x\) in the following way</p> \[\begin{equation} 0^x=\begin{cases}1,&amp;x=0\\0,&amp;x\neq0\end{cases} \end{equation}\] <p>Now, I am not saying that \(0^0=1\) always. It is indeed an indeterminate form. However, for our purposes, everything is simply cleaner if we take that as fact. Notice that we make the same assumption when we choose to write \(e^x\) as \(e^x=\sum_{n=0}^\infty\frac{x^n}{n!}\) rather than \(e^x=1+\sum_{n=1}^\infty\frac{x^n}{n!}\).</p> <p>I know which one I prefer. :unamused:</p> <p>Either way, the most interesting similarity, in my opinion, is the analog for the matrix exponential.</p> <h1 id="the-best-solution">The Best Solution</h1> <p>In the system</p> \[\textbf{x}'(t)=A\textbf{x}(t)\] <p>the best general solution, though not always the most ideal to find, is</p> \[\textbf{x}(t)=e^{At}\textbf{x}(0)\] <p>Similarly, for the system</p> \[\textbf{x}(n+1)=A\textbf{x}(n)\] <p>the best general solution is</p> \[\textbf{x}(n)=A^n\textbf{x}(0)\] <p>Now, this is the coolest part, in my opinion.</p> <p>There are formulas for \(A^n\) which look nearly <em>identical</em> to the formulas for <a href="../2x2ezmatrixexp/" target="_blank">Matrix Exponentials</a> that I found. I will compare them here, but first! I would like to let you in on how I found them. I used the method described in <a href="../2x2ezmatrixexp/#another-approach">that matrix exponential post</a>. Basically, if the characteristic polynomial is \(s^2=p_0+p_1s\), solve</p> \[\begin{pmatrix} x(n+1)\\y(n+1) \end{pmatrix}= \begin{pmatrix} 0&amp;p_0\\1&amp;p_1 \end{pmatrix} \begin{pmatrix} x(n)\\y(n) \end{pmatrix},\quad \textbf{x}(0)= \begin{pmatrix} 1\\0 \end{pmatrix}\] <p>Then \(A^n=x(n)I+y(n)A\) (for 2x2s!). This does generalize, and I will at some point make a post about that.</p> <p>Theoretically, I’m fairly certain you can achieve the same results by solving for the <a href="/math/normalized" target="_blank">normalized solutions</a> of the linear difference equation</p> \[a_{n+2}-p_1a_{n+1}-p_0a_n=0\] <h1 id="solution-comparison-2x2">Solution Comparison (2x2)</h1> \[\begin{array}{cccccccc} \textbf{x}'(t)&amp;=&amp;A\textbf{x}(t)&amp;\implies&amp;\textbf{x}(t)&amp;=&amp;e^{At}\textbf{x}(0)\\ \textbf{x}(n+1)&amp;=&amp;A\textbf{x}(n)&amp;\implies&amp;\textbf{x}(n)&amp;=&amp;A^n\textbf{x}_0 \end{array}\] <p>The following two are quite simple, and we actually used them to find the matrix exponential formula.</p> <h2 id="nondefective-repeated">Nondefective Repeated</h2> <p>This occurs when \(A=kI\).</p> \[\begin{array}{ccc} e^{kIt}&amp;=&amp;e^{kt}I\\ (kI)^n&amp;=&amp;k^nI \end{array}\] <h2 id="rank-1">Rank 1</h2> <p>If \(A\) is any square matrix of rank one that also has a nonzero trace.</p> \[\begin{array}{ccccc} e^{At}&amp;=&amp;\frac{e^{\operatorname{tr}(A)t}A-(A-\operatorname{tr}(A)I)}{\operatorname{tr}(A)}&amp;=&amp;I+\frac{e^{\operatorname{tr}(A)t}-1}{\operatorname{tr}(A)}A\\ A^n&amp;=&amp;\frac{\operatorname{tr}(A)^nA-0^n(A-\operatorname{tr}(A)I)}{\operatorname{tr}(A)}&amp;=&amp;0^nI+\frac{\operatorname{tr}(A)^n-0^n}{\operatorname{tr}(A)}A \end{array}\] <p>As you can see, defining \(0^n\) as we did makes things very convenient, and allows us to take advantage of the underlying similarities between the two system types.</p> <h2 id="defective">Defective</h2> <p>If \(A\) is any \(2\times2\) matrix with a defective eigenvalue \(k\neq0\),</p> \[\begin{array}{ccc} e^{At}&amp;=&amp;e^{kt}\bigg(I+t\big(A-kI\big)\bigg)\\ A^n&amp;=&amp;k^n\bigg(I+\frac{n}{k}\big(A-kI\big)\bigg) \end{array}\] <p>If \(k=0\), then the solution is kind of… disturbing.</p> \[\begin{array}{ccc} e^{At}&amp;=&amp;I+tA\\ A^n&amp;=&amp;0^nI+0^{n-1}A \end{array}\] <p>But looking at it, it does <em>actually</em> work. At \(n=0\) it’s \(I\), and at \(n=1\) it’s \(A\), and then its zero ever after. Which is indeed what happens with this kind of matrix (nilpotent). I’m not writing \(0^n(I+0^{-1}A)\), though. This is clearly not a case where we can use regular exponent rules.</p> <h2 id="complex">Complex</h2> <p>If \(A\in\mathbb{R}^{2\times2}\) has complex eigenvalues \(a\pm bi=re^{\pm i\theta}\), then</p> \[\begin{align*} e^{At}=&amp;e^{at}\bigg(\cos(bt)I+\frac{\sin(bt)}{b}\big(A-aI\big)\bigg)\\ A^n=&amp;r^n\bigg(\cos(\theta n)I+\frac{\sin(\theta n)}{b}\big(A-aI\big)\bigg) \end{align*}\] <h2 id="distinct">Distinct</h2> <p>If \(A\) has two distinct eigenvalues \(\lambda_1,\lambda_2\), then</p> \[\begin{align*} e^{At}=&amp;\frac{e^{\lambda_2t}(A-\lambda_1I)-e^{\lambda_1t}(A-\lambda_2I)}{\lambda_2-\lambda_1}\\ A^n=&amp;\frac{\lambda_2^n(A-\lambda_1I)-\lambda_1^n(A-\lambda_2I)}{\lambda_2-\lambda_1} \end{align*}\]]]></content><author><name>Taylor Grant</name></author><category term="works-in-progress"/><summary type="html"><![CDATA[Like systems of ODEs, but discrete 👀]]></summary></entry></feed>