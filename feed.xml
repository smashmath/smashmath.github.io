<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://smashmath.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://smashmath.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-28T02:07:55+00:00</updated><id>https://smashmath.github.io/feed.xml</id><title type="html">smashmath</title><subtitle>i do math sometimes </subtitle><entry><title type="html">Why do we row reduce? What IS a matrix?</title><link href="https://smashmath.github.io/blog/rref/" rel="alternate" type="text/html" title="Why do we row reduce? What IS a matrix?"/><published>2024-07-27T00:00:00+00:00</published><updated>2024-07-27T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/rref</id><content type="html" xml:base="https://smashmath.github.io/blog/rref/"><![CDATA[<p>(This is a first draft, subject to change.)</p> <p>I’ve stood by and let this go on for too long. As someone who has been tutoring/teaching linear algebra for five years now, I’ve looked at the subject as a whole through many different perspectives. Linear algebra is very much a subject of abstraction, and it’s often confusing for students to understand <em>why</em> we need to say or prove things our brains find self-evident. The other difficulty in learning linear algebra is that it’s usually a student’s first experience with proofs.</p> <p>So we have this abstract difficult subject with a million terms and concepts (many terms actually being the SAME concept under a different name), problems that can look entirely different but end up asking the exact same question, AND most students have to figure it out while they are learning how to properly do a mathematical proof for the first time (which is NOT an easy skill to learn). No wonder people struggle with it! And, so, I can understand why many professors choose to offload the concepts and underlying “algebra” until later in the course, and start off with “Here’s a system of equations, row reduce it!” for three weeks. It’s certainly… easy? But also tedious, and generally unmotivated.</p> <p>The problem is that with so much to cover, my experience with most students is that the key concepts that tie EVERYTHING together are not being emphasized. So when the more important topics like basis, linear independence, span, coordinate vectors, kernel, image, etc. are introduced, the pathways between the new topic and what the student has already learned aren’t being drawn, and it all seems so disconnected. More like you’re learning something completely different solved with a similar method, rather than learning an extension of a previous concept.</p> <p>I personally see linear algebra as actually being relatively straightforward (once you understand the underlying ideas and motivations behind the definitions). So that’s what I want to do today. I’m going to start from square one, and we’re going to change how we view row reduction, matrix multiplication, and even a matrix <em>itself</em>.</p> <h3 id="image-terminology">Image Terminology</h3> <p>This post is pretty much entirely about functions, and to <em>really</em> talk about functions properly, we’re going to use the proper terminology. These are concepts you’re probably familiar with, but perhaps under different names.</p> <p>We have a function we denote \(f:X\to Y\) to mean \(f\) is a function that takes stuff in the <strong>domain</strong> \(X\) to the <strong>codomain</strong> \(Y\). i.e. <em>everything</em> in \(X\) <em>can</em> be put into the function, and every output is in \(Y\) (though we can’t necessarily reach everything in \(Y\)). For example, \(e^x\) is a function that takes in real numbers and outputs real numbers (so the \(\mathbb{R}\) is both the domain and codomain). We can put in any real number, and we’ll get out a real number. Even though every output is actually strictly positive (there’s no output for \(-1\), for example), it’s still correct to say it’s a function with codomain \(\mathbb{R}\).</p> <p>If \(f(x)=y\), then we say \(y\) is <em>the</em> <strong>image</strong> of \(x\). Images are <em>unique</em> (because a function must be “well-defined”). If you vaguely remember that a function can’t send one input to two outputs, or that it must pass the “vertical line test”, this is basically the more rigorous way to say it: the image is unique.</p> <p>It’s also <em>incredibly</em> useful to say that \(f(x)=y\) means that \(x\) is <em>a</em> <strong>preimage</strong> of \(y\). Preimages are not necessarily unique. For example, with the function \(f(x)=x^2\), \(f(2)=f(-2)=4\), so \(4\) has two preimages: \(2,-2\).</p> <p>We call the complete set of all possible outputs that \(f\) can produce the <strong>image/range of \(f\)</strong>. And for special types of functions (like the ones we study in linear algebra), we call the set of all preimages of \(0\) the <strong>kernel</strong>.</p> <p>This is important to talk about up front, because understanding why matrices are useful requires understanding how the images of a few special vectors under special functions allows us to simplify how we <em>write</em> that function.</p> <h2 id="what-is-a-matrix">What IS a matrix</h2> <p>Now, some smart-asses (many of which, I love) might object to what I’m about to say and say “a matrix is JUST an array of numbers which CAN induce a linear transformation!” And, yeah, but we’re talking in the context of linear algebra, here. In my opinion, the way to view a matrix that provides the most intuitive bang for your buck is as a <strong>Linear Transformation</strong> itself.</p> <h3 id="linear-transformations">Linear Transformations</h3> <p>First, though, I’m going to give you the completely oversimplified low-down of what the majority of abstract algebra is like, on the motivational level.</p> <blockquote> <p>“We have some things that interact in some nice way, let’s look at functions that preserve those interactions.”</p> </blockquote> <p>In terms of linear algebra, our “things” are vectors (over some field of scalars), and the interaction is vector addition and scalar multiplication.</p> <p><strong>NOTE: You can think of a “field” as just some collection of “numbers” that you can add/subtract/multiply and divide (if they’re nonzero). Just think \(\mathbb{R}\) or \(\mathbb{C}\) for the purposes of this post.</strong></p> <p>We can do these things and still get a vector, and combining these operations is called taking a “linear combination”.</p> \[v=c_1v_1+\ldots+c_nv_n\] <p>So, in a sense, we can view vector spaces as some collection of vectors, and we can take linear combinations of those vectors using scalars from the field the vector space is over.</p> <p>NOTE: Saying scalars from a “field” is important! There are other nonvector things we can use to “scale/multiply to” vectors, but being able to divide nonzero scalars is what makes linear algebra so nice (and what makes Modules–vector spaces over a ring instead of a field–much crazier).</p> <p>So, algebraists, then, are interested in functions that preserve the interactions of vectors (which are linear combinations in our case): called <strong>Linear Transformations</strong>.</p> \[T(v)=T(c_1v_1+\ldots+c_nv_n)=c_1T(v_1)+\ldots+c_nT(v_n)\] <p>But why are <em>you</em> (someone who is probably <em>not</em> an algebraist) interested in functions that preserve linear combinations? Well, if you’re taken calculus, then you definitely are! How do you take the derivative of a polynomial?</p> \[\frac{d}{dx}(ax^2+bx+c)=a\frac{d}{dx}x^2+b\frac{d}{dx}x+c\frac{d}{dx}1\] \[=a(2x)+b(1)+c(0)=2ax+b\] <p>You probably don’t think about it that way (with so many steps), and you can just <em>see</em> the derivative. But this is really what’s going on under the hood. We don’t have an explicit formula for the derivative of every single polynomial that exists, but we do know</p> <ul> <li>The derivative of any single power of \(x\): \(\frac{d}{dx}x^n=nx^{n-1}\).</li> <li>We can just take constants “along for the ride”. That is, we can pull scalars out of a derivative.</li> <li>We can <em>also</em> separate the derivative of a sum into the sum of derivatives.</li> </ul> \[\begin{gather*} \frac{d}{dx}(cf(x))=c\frac{d}{dx}f(x)\\ \frac{d}{dx}(f(x)+g(x))=\frac{d}{dx}f(x)+\frac{d}{dx}g(x)\\ \end{gather*}\] <p>We can combine these properties to just say the derivative “preserves” linear combinations:</p> \[\frac{d}{dx}(c_1f(x)+c_2g(x))=c_1\frac{d}{dx}f(x)+c_2\frac{d}{dx}g(x)\] <p>and so differentiating a polynomial is just breaking up the linear combination and applying the derivative to each power of \(x\). This is the key idea: we only need to know how the operation acts on each vector, and then we know exactly how it acts on any linear combination. We don’t need to compute it from scratch.</p> <p>And, in that way, the derivative is one of the most important examples of a linear transformation (<a href="../linalglinconstcoef/" target="_blank">and the study of the derivative as a linear operator on smooth functions is pretty much the most important part of an Ordinary Differential Equations course</a>). The integral can also be considered a linear operator.</p> <p>Hopefully, the derivative and integral are sufficient to convince you that “yeah, studying linear operators might be worth doing”, but in case you need a few more, the following are examples of things that are actually just applications of linear transformations:</p> <ul> <li>Systems of linear equations</li> <li>State changes (Markov chains, PageRank algorithms)</li> <li>Adjacency matrices for a graph</li> <li>Fourier and Laplace transforms</li> </ul> <h2 id="representing-a-linear-transformation">Representing a linear transformation</h2> <p>Alright, so I’ll assume you’re on board with “linear transformations are worth studying”. Now, we’re going to restrict ourselves to linear transformations between finite dimensional vector spaces (the main topic for an introductory course in linear algebra). For our purposes, a finite dimensional vector space is necessary because it has a finite basis, which is the key fact that makes matrices so useful.</p> <p>Now, what do I mean by finite basis? Well, let’s focus on the main dish of finite dimensional vector spaces: coordinate spaces! You might know them in the case of \(F=\mathbb{R}\) as “Euclidean vector spaces” of the form \(\mathbb{R}^n\). We’ll just call them \(F^n\): an array or list of \(n\) numbers from the underlying field of scalars (which we’re just calling \(F\) instead of specifying \(\mathbb{R}\) or \(\mathbb{C}\) or whatever other field have).</p> <p>Note: One of the reasons linear algebra is so powerful is that all the non \(F^n\) finite dimensional vector spaces are actually just isomorphic to \(F^n\) by the coordinate map. In layman’s terms: it’s all just \(F^n\). We can without loss of generality assume we’re taking about \(F^n\) coordinate spaces, which mesh the best with matrices. And then we only have to slightly adjust the execution for general finite dimensional vector spaces.</p> <h3 id="basis">Basis</h3> <p>Okay, so why do we care about a “basis”, and what is it? A <strong>basis</strong> is a “generating set” of vectors with unique representations. What does that mean?</p> <p>The fact that it’s a “generating set” means that the basis can “generate” or produce any vector in the space. If a vector exists in the space, I can give you a linear combination of the basis vectors that equals that vector. If our basis is \(\left\{v_1,\ldots,v_n\right\}\), then that means for all \(x\in F^n\), we can find constants \(c_1,\ldots, c_n\) depending on \(x\) such that</p> \[x=c_1v_1+\ldots+c_nv_n\] <p>You probably learned the word “span” to describe this concept, but “generating” is a more descriptive word (in my opinion), <em>and</em> the more general term used for similar concepts in abstract algebra. But it’s still the same thing: we’ll use “generates” as a synonym for “spans”.</p> <p>For example, take \(F^2\) with the standard basis \(\left\{\begin{pmatrix}1\\0\end{pmatrix},\begin{pmatrix}0\\1\end{pmatrix}\right\}\). For any vector in the space, \(x=\begin{pmatrix}x_1\\x_2\end{pmatrix}\), we can easily find a linear combination of the basis vectors to reach \(x\):</p> \[x=\begin{pmatrix}x_1\\x_2\end{pmatrix}=x_1\begin{pmatrix}1\\0\end{pmatrix}+x_2\begin{pmatrix}0\\1\end{pmatrix}\] <p>The constants in the linear combination are just the entries! That’s what makes this the “standard basis”. It’s the simplest possible basis, which is easiest to use. We’ll use \(e_i\) for the \(i\)th standard basis vector (that is, a \(1\) in the \(i\)th entry and zeroes everywhere else).</p> <h4 id="linear-independence">Linear independence</h4> <p>Now, there’s one other part from the definition for basis I gave above: the generating set has “unique representation”, which essentially means that it’s “linearly independent”. This means that the representation of a vector as a linear combination of the basis vectors is unique. So if we have</p> \[x=c_1v_1+\ldots+c_nv_n=d_1v_1+\ldots+d_nv_n\] <p>Then that implies that \(c_i=d_i\) for all \(i\).</p> <p>Now, if you’ve already learned linear independence, you might be thinking “WTF THIS IS NOT WHAT I LEARNED LINEAR INDEPENDENCE MEANS”. And, yes, this is not the standard definition. But it is the most <em>conceptually useful</em> definition, especially for our purposes here (though not the easiest to use for proofs). See, if we stick with just this idea of “unique representations”, then what if \(x=0\)? We can clearly use \(d_1=\ldots=d_n=0\) as a linear combination to get \(x\) because \(0v_1+\ldots+0v_n=0\). However, this means that, by our established uniqueness,</p> \[c_1v_1+\ldots+c_nv_n=0\implies c_1=\ldots=c_n=0\] <p>All the \(c_i\)’s must be zero! And <em>this</em> is now closer to what you probably learned before. In fact, you can actually read the more familiar definition as saying “the zero vector can only (uniquely) be represented as a linear combination of these vectors using the trivial linear combination of all zero coefficients (trivial solution)”. And, as it turns out, if \(0\) can only be represented by the trivial solution, then that means ALL representations are unique! This is not as obvious (and worth trying to prove if you can)!</p> <p>The other super important and useful fact: <strong>all bases have the same size!</strong> Don’t take this for granted, the fact we’re over a field is the only reason this is guaranteed to be true. There are Modules which can have a generating sets of any positive integer size which are <em>all</em> linearly independent. For us, it’s SUPER simple: <strong>\(F^n\) has \(n\) basis vectors</strong>. Don’t overthink it!</p> <p>Okay, so basically I took your familiar “basis” = “spans” + “linearly independent” definition, and slapped on confusing words: “spans” = “generates” and “linearly independent” = “unique representations”. Why? Because now everything is going to come together.</p> <h3 id="all-together-now">All together now</h3> <p>Okay, so to summarize so far:</p> <ul> <li>We have vector spaces over some field which contain vectors that we can take linear combinations of.</li> <li>We’re interested in functions called Linear Transformations that preserve linear combinations.</li> <li>If the vector space is finite dimensional, then we have some finite basis.</li> <li>A basis gives a way to uniquely write any vector in the space.</li> </ul> <p>We’re going to use these facts together to get the main reason linear transformations are so nice and easy to study: <strong>To know how it acts on ANY vector, we ONLY need to know how it acts on the BASIS vectors of the domain.</strong></p> <p>This is <strong>HUGE</strong>. Functions can be very complicated and complex! It can be computationally intense to even compute many functions. But with linear transformations between finite dimensional vector spaces? Easy-peasy. Because if \(T\) is a linear transformation from \(F^n\to F^m\), then we only need to know the images of the \(n\) basis vectors of \(F^n\) to know where \(T\) sends <strong>EVERY</strong> vector. (I feel like you aren’t getting as excited about this as I am)</p> <p>So suppose you are trying to tell your friend across the sea about your interesting linear transformation between \(\mathbb{R}^3\to \mathbb{R}^2\). How do you do it? \(\mathbb{R}^3\) has infinitely many vectors (uncountably many, in fact). You could never write down every image (output) for every vector in \(\mathbb{R}^3\). But, you don’t need to! For,</p> \[T\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=T(x_1e_1+x_2e_2+x_3e_3)=x_1T(e_1)+x_2T(e_2)+x_3T(e_3)\] <p>Therefore, all we need is \(T(e_1),T(e_2),T(e_3)\). That’s just three \(\mathbb{R}^2\) vectors: which is \(6\) simple numbers total. That means, all you need to send to your friend is those six numbers, and they will have EXACTLY your linear transformation. Let’s say the images are</p> \[T(e_1)=\begin{pmatrix}1\\1\end{pmatrix},\quad T(e_2)=\begin{pmatrix}0\\1\end{pmatrix},\quad T(e_3)=\begin{pmatrix}1\\-1\end{pmatrix}\] <p>So, if you were lazy, and you wanted to send those six numbers to your friend in as clear and simple a way as possible, how might you do it?</p> <p>Well sending just <code class="language-plaintext highlighter-rouge">1,1,0,1,1,-1</code> is kind of confusing and hard to parse. It’s also not immediately clear what this is a transformation between. It could be \(\mathbb{R}^1\) to \(\mathbb{R}^6\) or vice versa, or \(\mathbb{R}^2\) to \(\mathbb{R}^3\). So a one-dimensional array doesn’t really work.</p> <p>Okay, you know exactly what I’m getting at: YOU PUT IT IN A <strong>MATRIX</strong>. A <strong>TWO</strong>-dimensional array. And we have the vectors already in their little column form, why not just concatenate them? Let’s define</p> \[A=\begin{pmatrix}1&amp;0&amp;1\\1&amp;1&amp;-1\end{pmatrix}\] <p>to be our little package that <em>encodes</em> \(T\). This perfectly stores all the information we need to communicate \(T\). Each column tell us the image of each standard basis vector.</p> <p>Now, let’s take it one step further. What if we could use \(A\) <em>instead</em> of \(T\)? What if we didn’t have to write the linear transformation at all? What if we could stick to just these simple arrays of numbers to carry <strong>all</strong> the information of our linear transformation? How would we do it?</p> <p>Well we clearly want \(Ax=T(x)\). That much is obvious. And we know that</p> \[T\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=x_1\begin{pmatrix}1\\1\end{pmatrix}+x_2\begin{pmatrix}0\\1\end{pmatrix}+x_3\begin{pmatrix}1\\-1\end{pmatrix}\] <p>So… then… let’s just define</p> \[\begin{pmatrix}1&amp;0&amp;1\\1&amp;1&amp;-1\end{pmatrix}\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=x_1\begin{pmatrix}1\\1\end{pmatrix}+x_2\begin{pmatrix}0\\1\end{pmatrix}+x_3\begin{pmatrix}1\\-1\end{pmatrix}\] <p>And there, now we have matrix vector multiplication defined.</p> <p>Essentially, all we’re doing is</p> \[\begin{equation} A= \Bigg(\begin{matrix}T(e_1)&amp;\cdots&amp;T(e_n)\end{matrix}\Bigg) \end{equation}\] <p><img src="/assets/img/all coming together.png" alt="all coming together" style="width:100%; max-width:600px;"/></p> <p>Observe that this also, at a glance, tells us exactly what \(T\) is a function between. Each column has two entries (i.e. \(A\) has two rows), so the codomain must be \(\mathbb{R}^2\). We also have three columns, so our domain has three basis vectors. Clearly, the domain must be \(\mathbb{R}^3\). Thus, our construction of \(A\) as a “package” for \(T\) means that \(A\) being \(m\times n\) (\(m\) rows and \(n\) columns) means that the linear transformation that \(A\) encodes is from \(F^n\to F^m\).</p> <p>I’d like to acknowledge that with this perspective, a student doesn’t have to “memorize” the fact that \(m\times n\iff F^n\to F^m\). This isn’t a “theorem” or something that requires excessive scratch work. This is a damn <strong>feature</strong> of matrices, which makes them so informative and useful. Not only do the columns encode the outputs, the very <strong>shape</strong> itself tells you one of the most important aspects of the function: the dimension of the domain and codomain (AND their size relative to each other).</p> <p>So by our construction, this allows us to utilize other theorems to know from a glance that</p> <ul> <li>A wide matrix has a nontrivial kernel (is not injective)</li> <li>A tall matrix is not surjective</li> </ul> <h3 id="change-of-basis">Change of basis</h3> <p>Now, it’s time for my trump card. Why you should <em>definitely</em> view a matrix from the perspective that the columns are the images of the basis vectors. Because that’s how we define a matrix for a linear transformation with respect to <em>other</em> bases! If your course covers finding a matrix with respect to a certain basis, then you <em>absolutely</em> should start with this perspective.</p> <p>I see it, time and time again. A student asks how to find the matrix for \(T\) with respect to some given basis \(\beta\) (or maybe just the matrix for a described linear transformation), and they’re lost. And the inevitable first question someone asks the student is “do you know the ‘formula’ for the matrix with respect to the basis \(\beta\)?”</p> \[[T]_\beta^\gamma = \Bigg(\begin{matrix}\left[T(\beta_1)\right]_\gamma&amp;\cdots&amp;\left[T(\beta_n)\right]_\gamma\end{matrix}\Bigg)\] <p>And I scream “NO! IT’S NOT A FORMULA, IT’S A DEFINITION!” Okay, I don’t actually scream that. But, really, this is the <strong>definition</strong> for the matrix representation for a given linear transformation when bases are specified for the domain and codomain. Now, if you adopt the view I’m trying to push here, then this definition is just… the natural generalization. The columns are still just the images of the basis vectors of the domain, we’re just being more explicit that we’re using a specific basis. But now we make sure the images are the coordinates with respect to the proper basis of the codomain. That is literally the only change. If both \(\beta\) and \(\gamma\) are just the standard basis of \(F^n\) and \(F^m\) respectively, it’s <em>exactly</em> what we described above.</p> <p>If you haven’t learned coordinate vectors or change of basis, you can ignore this. But just know that this is <em>literally</em> how we define the matrix representation of a linear transformation!</p> <p><a href="https://youtu.be/t348e24vDyA?si=k3Df2elcbvqxdcnq" target="_blank">So why isn’t this view emphasized at the beginning of a course so that this generalization to other bases is only a minor change and not something students seem to need to learn from scratch? Why is this viewed as a formula and not the most basic definition?</a></p> <h2 id="column-perspective">Column Perspective</h2> <p>In case you’ve seen me rant online about how column perspective is the ultimate definition of matrix vector multiplication, perhaps now you can see why I think so. With our perspective of a matrix as a compact definition for a linear transformation, this definition is just obvious.</p> \[\begin{equation} \Bigg(\begin{matrix}a_1&amp;\cdots&amp;a_n\end{matrix}\Bigg) \begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} =x_1a_1+\ldots+x_na_n \end{equation}\] <p>where \(a_i\) is the \(i\)th column of \(A\) (which, by definition, is \(Ae_i\)). As a side note, the following two facts are things that most students I’ve worked with don’t find obvious, but really are with this perspective:</p> <ul> <li>The \(i\)th column of \(A\) has the preimage \(e_i\) (i.e. is equal to \(Ae_i\)). Again, literally <em>by definition</em>.</li> <li>If \(A\) is square and the \(i\)th column of \(A\) is a scalar multiple of \(e_i\), then \(e_i\) is an eigenvector.</li> </ul> <p>Observe, also, that taking some linear combination of the columns is the same as just taking the image of the vector with the scalars as the entries. That is, \(c_1a_1+c_2a_2+\ldots+c_na_n=A\begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}\). Hence, if you can find some combination of the column that gives you zero, then that gives you a vector in the kernel.</p> \[c_1a_1+c_2a_2+\ldots+c_na_n=0\implies \begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}\in\ker(A)\] <p>More generally, if you can find a combination of the columns to give you some vector \(b=c_1a_1+c_2a_2+\ldots+c_na_n\), then \(\begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}\) is a preimage of \(b\) under \(A\) (and thus a solution to \(Ax=b\)).</p> <p>So, then how does matrix-matrix multiplication fit into this perspective? Well, if \(T_A\) has the matrix \(A\), and \(T_B\) has the matrix \(B\) (and \(T_B\circ T_A\) is defined), then we want \(BAx=T_B(T_A(x))\). Once again, we just need to make sure that both \(BA\) and \(T_B\circ T_A\) map the standard basis vectors to the same thing.</p> \[T_B(T_A(e_j))=T_B(a_j),\quad BAe_j=Ba_j\] <p>And with our definition, there’s nothing immediately wrong or strange we have to do. We just define the \(j\)th column of \(BA\) to be \(Ba_j\). That is,</p> \[\begin{equation} BA=B\Bigg(\begin{matrix}a_1&amp;\ldots&amp;a_n\end{matrix}\Bigg) =\Bigg(\begin{matrix}Ba_1&amp;\ldots&amp;Ba_n\end{matrix}\Bigg) \end{equation}\] <p>If you think about it, this is actually quite reasonable. \(Ax\) is going to be some linear combination of the columns of \(A\). And \(B(Ax)\) is just applying \(B\) to every single one of those vectors in the linear combination. So the image of the standard basis vectors under \(BA\) is just \(B\) applied to the images of the standard basis vectors under \(A\).</p> <p>Am I going to talk about the row column rule now? No. Because there are a million videos on youtube about it, and your professor probably drilled it into your head. This post is focused on <em>intuition</em>. It doesn’t take that long to see that the row column rule follows directly from the column definition, once you just combine it all into one vector. To be clear, the row column rule is not evil, and I definitely use it because it’s quicker. But I think it’s better to keep in mind the column definition is really <em>the</em> ultimate definition, and it ties pretty much <em>everything</em> you do with matrices together.</p> <p><img src="/assets/img/all coming together.png" alt="yes i'm using it again" style="width:100%; max-width:600px;"/></p> <h3 id="a-remark-on-column-space">A remark on column space</h3> <p>Alright, so at the beginning I talked about how we call the set of all outputs of a function the “image” or “range”. For some reason, in linear algebra, we try to obscure the connection between the image/range of a linear transformation and the image/range of a matrix function by calling it a different name: “the column space of \(A\)”. This is an intuitive name for the span/set of all linear combinations of the columns of \(A\), but it distracts from the fact that we already define multiplication by \(A\) to be <em>taking linear combinations of the columns of \(A\)</em>.</p> <p>This is one of the most frustrating parts of tutoring linear algebra for me. Because for most students, span, column space, and range are basically separate concepts in their head, when it’s really just <em>one</em> thing. The fact that</p> <blockquote> <p>”\(Ax=b\) is consistent if and only if \(b\) is in the column space of \(A\)”</p> </blockquote> <p>has to be a <em>theorem</em> and is not as self-evident as “\(f(x)=y\) is possible if and only if \(y\) is an output of \(f\)” makes me want to tear my hair out. And it’s not the student’s fault! It’s how they’re being taught linear algebra.</p> <p>So screw “column space” (and “null space” for that matter). All matrix functions in linear algebra are linear transformations, and we’re interested in the image and kernel of them. Consider this yeeting two redundant terms from the subject entirely.</p> <p>(Plus, you’ll always sound smarter if you say “image” and “kernel” instead of “column space” and “null space”)</p> <h2 id="column-relationships">Column relationships</h2> <p>Alright, let’s talk about a new matrix \(A=\begin{pmatrix}1&amp;-1&amp;1&amp;2\\1&amp;-1&amp;2&amp;3\\1&amp;-1&amp;3&amp;4\end{pmatrix}\). This is a \(3\times4\) matrix, so it’s a transformation from \(F^4\to F^3\). Now, as is usual in linear algebra, we’re interested in the image and kernel of this linear transformation.</p> <p>The matrix is wider than it is tall, and so it’s trying to stuff a higher dimensional space into a lower dimensional one. Thus, it’s intuitive that we’ll have a nontrivial kernel. A wider matrix <em>can</em> map to the entire codomain, but it’s not immediately obvious if every vector in \(F^3\) <em>will</em> actually have a preimage under \(A\). It’s not even immediately obvious how you’d even determine that! All we theoretically know is that an output is a linear combination of the columns. So, can we reach any \(F^3\) vector with a linear combination of these columns? If not, how can we tell?</p> <p>Well, if we think about it, what we really <em>want</em> is a basis for the range of \(A\). Of course, every column is in the range, so we just want to know which columns to take for a basis, and which columns are redundant. If we look carefully, we can see one major relationship: Column 2 = -(Column 1). This means that if we use both columns one and two, then our representations won’t be unique (because they are linearly dependent). So we should definitely exclude column 2.</p> <p>If we look at Column 3, it doesn’t have any obvious relationship to Column 1. In fact, you can show they are linearly independent. But, you might just notice that Column 4 = Column 1 + Column 3. Another dependence relationship. Thus, it appears that we also have to exclude Column 4, leaving just Columns 1 and 3. Hence,</p> \[\left\{\begin{pmatrix}1\\1\\1\end{pmatrix},\begin{pmatrix}1\\2\\3\end{pmatrix}\right\}\] <p>is a basis for the image of \(A\). You can confirm this is true, because we can write every column of \(A\) as a linear combination of these vectors, so then we can write any linear combination of the columns as a linear combination of these vectors. And from here, we can see that the image is two-dimensional, so it won’t span or generate \(F^3\). Thus, \(A\) is not a surjective or onto transformation.</p> <p>But what of the kernel? Well, Column 2 = -(Column 1) is actually just saying</p> \[A(e_2)=-A(e_1)\implies A(e_1+e_2)=0\] <p>So \(\begin{pmatrix}1\\1\\0\\0\end{pmatrix}\) is in the kernel. Can you see how by similar logic \(\begin{pmatrix}1\\0\\1\\-1\end{pmatrix}\) is also in the kernel? You can verify it yourself by applying \(A\) to both vectors, and when you use column perspective, it should become clear. And we can justify through rank-nullity that these bases are complete because their sizes sum to the dimension of the domain (but let’s just vibe it).</p> <p>Okay, so I lead you through a problem you would have learned to do by row reduction without it. Any liquored-up hillbilly with a shotgun could have done that at the zoo. (TODO: remove this)</p> <p>Let’s try a much easier, <em>seemingly unrelated</em> problem:</p> \[R=\begin{pmatrix}1&amp;-1&amp;0&amp;1\\0&amp;0&amp;1&amp;1\\0&amp;0&amp;0&amp;0\end{pmatrix}\] <p>Here, it’s so much easier to find a basis for the image and kernel. Every column is a linear combination of the independent vectors \(e_1,e_2\), so they form a basis for the image. Oh, and look! Column 1 is \(e_1\) and Column 3 is \(e_2\). So Column 1 and Column 3 themselves form a basis for the image.</p> <p>As for the kernel, clearly if we take Column 1 + Column 2, we’ll cancel them out and get zero. So actually similar to above \(\begin{pmatrix}1\\1\\0\\0\end{pmatrix}\) is in the kernel. And, similarly, it’s pretty easy to see that if we take Column 4 and then subtract off Columns 1 and 3, then that’s also zero. So \(\begin{pmatrix}1\\0\\1\\-1\end{pmatrix}\) is also in the kernel. Now… that seems familiar. Both \(A\) and \(R\) have the same kernel, the same column relationships, and the same columns form a basis for the image*. What is this sorcery??</p> <p>Note*: It turns out that the kernel actually uniquely determines which columns can be a basis for the image.</p> <h2 id="row-reduction">Row reduction</h2> <p>It turns out that \(A\) and \(R\) are actually significantly linked. Because \(R\) can be obtained from \(A\) by applying an invertible operator on the left:</p> \[\begin{pmatrix} 0 &amp; 3 &amp; -2 \\ 0 &amp; -1 &amp; 1 \\ 1 &amp; -2 &amp; 1 \end{pmatrix}\begin{pmatrix}1&amp;-1&amp;1&amp;2\\1&amp;-1&amp;2&amp;3\\1&amp;-1&amp;3&amp;4\end{pmatrix} =\begin{pmatrix}1&amp;-1&amp;0&amp;1\\0&amp;0&amp;1&amp;1\\0&amp;0&amp;0&amp;0\end{pmatrix}\] <p>And since it’s invertible, we can also obtain \(A\) from \(R\) by applying the inverse on the left.</p> <p>Note: If you want to know how I found the matrix, you can just augment \(A\) with the identity and row reduce. You will get \(R\) augmented with a matrix that puts \(A\) into its RREF.</p> <p>So the question becomes “why does applying an invertible operator on the left not change the kernel or column relationships”? Well, like I said above, it turns out that the kernel actually uniquely determines the column relationships of the matrix. See <a href="#Kernel determines RREF">the end of the post</a> for an algorithm that gives you the unique set of nonzero RREF rows that has the specified kernel.</p> <p>So then why does applying an invertible operator on the left preserve the kernel? Well, this is actually a lot easier. If \(E\) is invertible, then</p> \[Ax=0\iff EAx=0\] <p>So if \(EA=R\), then \(Ax=0\iff Rx=0\implies \ker(A)=\ker(R)\).</p> <p>So, in summary, invertible operations on the left of a matrix don’t change the kernel, which also tell us which choices of columns give us a basis for the image.</p> <h3 id="row-space">Row space</h3> <p>There is another perspective for what an “invertible operator on the left” actually means using <a href="../columnperspective/#row-perspective" target="_blank">“row perspective”</a>. It’s essentially the transpose of column perspective.</p> \[\begin{equation} AB=\begin{pmatrix} {A}_1^T\\{A}_2^T\\\vdots\\{A}_m^T \end{pmatrix}{B}= \begin{pmatrix} {A}_1^T{B}\\{A}_2^T{B}\\\vdots\\{A}_m^T{B} \end{pmatrix} \end{equation}\] <p>Where we are denoting the \(i\)th row of \(A\) as \(A_i^T\).</p> <p>If we think about what \(x^TB\) means, it’s just the transpose of \(B^Tx\), which is taking a linear combination of the columns of \(B^T\), which are the rows of \(B\). Then \(x^TB\) is just a row vector which is a linear combination of the rows of \(B\). Thus, the rows of \(AB\) are just linear combinations of the rows of \(B\). This means that every row of \(AB\) is in the row space of \(B\), and if \(A\) is <em>invertible</em>, then the row space of \(B\) is going to be entirely preserved!</p> <p>This is where elementary matrices come into play. It’s a nifty theorem that every invertible matrix is some product of elementary matrices. That is, any invertible matrix is equivalent to some sequence of row operations. In a way, elementary matrices “generate” the invertible matrices (but not in exactly the same way a set of vectors “generates” or spans a subspace. Here it’s through noncommutative products and not linear combinations).</p> <p>So when we say an “invertible operator on the left”, that can be thought of as performing some elementary row operations. Which, through all we have thus far established, preserves the row space, and thus also the kernel, and thus also the column relationships.</p> <p>In case you forgot, the elementary row operations are the following. I encourage you to convince yourself that these three operations would preserve column relationships.</p> <ol> <li>Swapping rows</li> <li>Scaling a row by a nonzero amount</li> <li>Adding a multiple of one row to another</li> </ol> <p>So, in summary, this is why we do row operations. They preserve the most fundamental aspects of a matrix (besides the image itself): the kernel and which columns generate the image. They can therefore be used to “simplify” a matrix. To its reduced row echelon form, for instance. The RREF, as we have seen, being the clearest and simplest way to see the relationship between the columns of the matrix.</p> <h4 id="row-space-basis">Row space basis</h4> <p>Another quick remark: We’ve established that row reduction preserves the row space. And, looking at the structure of the RREF, it’s clear that the nonzero rows of the RREF are linearly independent. They thus form a basis for the row space. Note also that since we can switch around rows when row reducing, that we must take the RREF rows as our row space basis rather than the corresponding rows of the original matrix. I explain that more in-depth <a href="../rowcolspace/#row-space" target="_blank">here</a>. But I also want to point out the difference in how the RREF gives us a basis for the row space and column space of the original matrix.</p> <p>Since row reduction can swap and change rows, we usually don’t end up with rows from the original matrix, but we do end up with a much nicer basis. Columns, on the other hand, never change their relative position, and that’s one of the reasons the relationships are preserved. However, though the rows of the RREF are still in the row space (just linear combinations of the original), the resulting columns are almost always completely different. This is why we go back to the original matrix. But this means that our column space basis is usually not as nice.</p> <p>In summary: the RREF of \(A\) gives a nicer row space basis not usually containing the rows of \(A\), while it gives a not so nice column space basis using the original columns of \(A\). This means that the RREF of \(A^T\) gives a nicer column space basis for \(A\) not in terms of the original columns, but a not so nice basis for the row space of \(A\) in terms of the original rows.</p> <p>All this to say, that row reduction not only shows us the relationships between the columns, but it also gives us the nicest possible basis for the rows. That means, we can also use the RREF to get the nicest possible basis for the span of any arbitrary set of vectors by sticking the vectors as the rows of a matrix and row reducing. Functionally also giving us a way to determine dimension computationally: just count the number of nonzero rows of the RREF.</p> <h3 id="solving-systems-of-equations">Solving systems of equations</h3> <p>Now, you may realize that I’m <em>only</em> getting to <em>solving</em> systems of equations <em>now</em>. And this may be surprising because row operations and matrices are often introduced FOR solving systems of equations. But hear me out! Everything we have talked about thus far is going to make row reduction’s application to solving systems of equations extremely simple and intuitive.</p> <p>See, we have thus far established row operations and row reduction as a way to see and make clear the relationship between a matrix’s columns. That is, row reduction can allow us to more easily see how to write one column as a linear combination of the others. Let’s see how this applies to systems of equations.</p> <p>A system of equations has approximately four “forms”. The one we use most commonly is \(Ax=b\). Each perspective has its own intuitive benefit. \(Ax=b\), for example, emphasizes that we’re trying to find a preimage under the linear transformation defined by \(A\). For example,</p> \[\begin{pmatrix}1&amp;-1&amp;1\\1&amp;-1&amp;2\\1&amp;-1&amp;3\end{pmatrix}x=\begin{pmatrix}2\\3\\4\end{pmatrix}\] <p>But we can also write it in</p> <ul> <li>Equation form: Emphasizes that we are trying to simultaneously satisfy multiple constraints.</li> </ul> \[\begin{matrix}1x_1&amp;-&amp;1x_2&amp;+&amp;1x_3&amp;=&amp;2\\1x_1&amp;-&amp;1x_2&amp;+&amp;2x_3&amp;=&amp;3\\1x_1&amp;-&amp;1x_2&amp;+&amp;3x_3&amp;=&amp;4\end{matrix}\] <ul> <li>Vector form: Column perspective, basically. Emphasizes we are trying to write one vector as a linear combination of some starting set of vectors (the columns of \(A\))</li> </ul> \[x_1\begin{pmatrix}1\\1\\1\end{pmatrix}+x_2\begin{pmatrix}1\\1\\1\end{pmatrix} +x_3\begin{pmatrix}1\\2\\3\end{pmatrix}=\begin{pmatrix}2\\3\\4\end{pmatrix}\] <ul> <li>Augmented matrix form: The most compact form of the system, and is going to be our secret weapon using the view we’ve built up in this post.</li> </ul> \[\left(\begin{array}{ccc\|c}1&amp;-1&amp;1&amp;2\\1&amp;-1&amp;2&amp;3\\1&amp;-1&amp;3&amp;4\end{array}\right)\] <p>You may have noticed that this augmented matrix is the matrix \(A\) from the <a href="#column-relationships">Column relationships</a> section! Recall that we found that Column 4 = Column 1 + Column 3. That is,</p> \[\begin{pmatrix}1\\1\\1\end{pmatrix} +\begin{pmatrix}1\\2\\3\end{pmatrix}=\begin{pmatrix}2\\3\\4\end{pmatrix}\] <p>And, again, we can easily see this is true by looking at the RREF:</p> \[\left(\begin{array}{ccc\|c}1&amp;-1&amp;1&amp;2\\1&amp;-1&amp;2&amp;3\\1&amp;-1&amp;3&amp;4\end{array}\right) \sim \left(\begin{array}{ccc\|c}1&amp;-1&amp;0&amp;1\\0&amp;0&amp;1&amp;1\\0&amp;0&amp;0&amp;0\end{array}\right)\] <p>This may seem completely inconsistent with how you learned it. You probably learned to just row reduce and then turn it back into equation form. Which would be</p> \[\begin{matrix}1x_1&amp;-&amp;1x_2&amp;&amp;&amp;=&amp;1\\&amp;&amp;&amp;&amp;x_3&amp;=&amp;1\end{matrix}\] <p>And then solve for the pivot variables blah blah blah. But, instead, you could just look at the columns.</p> <p>The augmented column 4, or \(b\) column, is Column 1 + Column 3. So a <strong>particular solution</strong> is \(\begin{pmatrix}1\\0\\1\end{pmatrix}\). We can identify that the kernel vector, or <strong>homogeneous solution</strong>, associated with the nonpivot column (Column 2) of the coefficient matrix is \(\begin{pmatrix}1\\1\\0\end{pmatrix}\) because Column 1 + Column 2 = 0. And that gives us our <strong>general solution</strong>: a particular solution plus a linear combination of the homogeneous solutions (a basis of the kernel of the coefficient matrix).</p> \[x=\begin{pmatrix}1\\0\\1\end{pmatrix}+c\begin{pmatrix}1\\1\\0\end{pmatrix}\] <p>I’m not saying this is the one only good way to solve systems, but I’m trying to convey the fact that with this perspective of matrices, matrix multiplication, and row reduction I’ve set up, the application to solving system of equations is clear and intuitive, and it can be explained in a way that all of these concepts can be tied together.</p> <p><img src="/assets/img/all coming together.png" alt="yes i'm using it again again" style="width:100%; max-width:600px;"/></p> <h3 id="kernel-determines-rref">Kernel determines RREF</h3> <p>Alright, now I didn’t want to stick this in the middle of the post. I had a sort of rage flow going. But this is an algorithm that allows you to determine the RREF directly from the kernel. Specifically, from the kernel alone you can actually determine exactly what the nonzero rows of rref(\(A\)) must be. Here’s how:</p> <ol> <li>Put your basis for \(\ker(A)\) as the rows of a matrix</li> <li>row reduce and find the kernel of that new matrix</li> <li>take the basis of that new kernel as the rows of <em>another</em> matrix and row reduce</li> <li>the result will be the nonzero rows of rref(\(A\)).</li> </ol> <p>The algorithm is mostly just something for you to think about. It’s a little advanced to explain exactly why it works, but it has to do with the fact that the row space is the orthogonal complement of the kernel, and the nonzero rows of the RREF are just a specific form of basis for the row space. I encourage you to try to think it through.</p> <p><a href="https://youtu.be/NtiGP0MlF8k?si=T5evvei871Yl76GT" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor F.</name></author><category term="linear-algebra"/><category term="best"/><summary type="html"><![CDATA[What does the RREF tell us, and why do we so much time on it? Why do we define matrix multiplication the way we do? My absolute most aggresive post of all time.]]></summary></entry><entry><title type="html">Introduction to Least Squares Part 2 (Electric Boogaloo)</title><link href="https://smashmath.github.io/blog/leastsquarespart2/" rel="alternate" type="text/html" title="Introduction to Least Squares Part 2 (Electric Boogaloo)"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/leastsquarespart2</id><content type="html" xml:base="https://smashmath.github.io/blog/leastsquarespart2/"><![CDATA[<p>This is a short sequel to my previous post on <a href="../leastsquares/" target="_blank">least squares</a>.</p> <p>These are the questions I left at the end of that post (in the more general complex form).</p> <ol> <li>Why is \(\mathbf{A}^*\mathbf{A}\mathbf{x}=\mathbf{A}^*\mathbf{b}\) guaranteed a solution?</li> <li>Why would the solution to the normal equation actually be the “closest” solution?</li> </ol> <p>When I say “dot product” here, I mean the general euclidean inner product on \(\mathbb{C}^n\), where we multiply by the <em>conjugate</em> transpose (adjoint) rather than the regular transpose.</p> \[\langle w,v\rangle=v^*w=\left(\overline{v}\right)^Tw\] <p>Note that if we talk about \(\mathbb{R}^n\), then the adjoint is just, of course, the regular transpose.</p> <p>If you need a reminder for why we need the <em>conjugate</em>, it’s because complex numbers get weird when you just square them:</p> \[\begin{pmatrix}1&amp;i\end{pmatrix}\begin{pmatrix}1\\i\end{pmatrix}=1^2+i^2=0\] <p>In \(\mathbb{R}^n\), we say \(\left\lVert v \right\rVert^2=v^Tv\), but that would mean this nonzero vector has a magnitude of \(0\). This is a big no-no, as we should <em>only</em> have that the zero vector has a magnitude of \(0\).</p> <p>So we can’t just say \(w\cdot v=v^Tw\) and call it a day if we want to be as general as possible. But you are more than welcome to just interpret \(v^*\) as the transpose, and assume we’re talking in terms of real vectors (my complex compulsion prevents me from doing so).</p> <h2 id="why-is-it-consistent">Why is it consistent</h2> <p>Recall that a system of equations \({A}{x}={b}\) is consistent when \(b\) is in the image or column space of \(A\). So the column space, \(W\), is going to be the main character in our story today.</p> <p>We know that every vector space can be decomposed as the direct sum of any subspace and its orthogonal complement. That is,</p> \[\mathbb{C}^n=W\oplus W^\perp\] <p>This means that \(b\) can be written <em>uniquely</em> as \(b=w+w_\perp\), where \(w\) is \(W\) and \(w_\perp\) is orthogonal to everything in \(W\). Conceptually this can be understood as writing \(b\) as a sum of a vector which does make a consistent system, and a vector which can knock us out of being consistent (i.e. the system is consistent if and only if \(w_\perp=0\)).</p> <p>Remember that to be orthogonal in \(\mathbb{C}^n\), we need \(w\cdot v=v^*w=0\). This definition <em>does</em> mean the complex dot product is not commutative, but it is conjugate commutative (\(v\cdot w=\overline{w\cdot v}\)). So, since we are only interested in orthogonality and dot products of zero, we still get a sort of orthogonal symmetry/commutativity (\(v\cdot w=0\iff w\cdot v=0\)).</p> <p>This does add one complication. While in \(\mathbb{R}^n\), \(An=0\) implies that \(n\) is orthogonal to the rows of \(A\), that isn’t quite so in \(\mathbb{C}^n\), since the inner product requires one vector to be conjugated. That is, while in \(\mathbb{R}^n\), the row space is the orthogonal complement of the null space, in \(\mathbb{C}^n\), the null space is the orthogonal complement of the conjugated row space (\(\ker(B)^\perp=\operatorname{row}(\overline{B})\), which is also \(\operatorname{col}(B^*)\)). Note that if \(B\) is real, we do recover the result that the kernel is orthogonal to the row space.</p> <p>But if we let \(B=A^*\) in the expression above, this means the orthogonal complement of the null space of \(A^*\) is the column space of \(A\). That is, if a vector \(u\) is orthogonal to the column space of \(A\), then \(A^*u=0\).</p> <p>Thus, \(A^*w_\perp=0\) and so \(A^*b=A^*(w+w_\perp)=A^*w\). Essentially, this means that multiplying by the adjoint <em>sort of</em> projects us into the column space of \(A\) (or, at least, zeros out the part that isn’t in the column space).</p> <p>Since \(w\) is by definition in the column space of \(A\), we know \(Ax=w\) is consistent. Let \(x=c\) be the solution (that is, \(w=Ac\)).</p> <p>So let’s write out what happens when we multiply \(Ax=b\) by \(A^*\).</p> \[A^*Ax=A^*b=A^*w=A^*Ac\] <p>This clearly has a solution: \(x=c\), which actually implies that the solution to \(A^*Ax=A^*b\) is the same as the solution to \(Ax=w\).</p> <p>That is, the least squares solution is the solution to the system projected onto the column space. Which actually makes some sense intuitively. If we want the closest solution, we want it to be the solution to the system where the vector is projected orthogonally to the column space. Which actually answers both our questions.</p> <h2 id="why-is-it-the-closest-solution">Why is it the closest solution</h2> <p>Now, I said it answers both of our questions, but perhaps you aren’t quite convinced that just because it’s the solution to the system where \(b\) has been projected into the column space it actually minimizes our error. Let’s see if I can change your mind.</p> <p>We measure the “closest” solution using \(\left\lVert b-Ax \right\rVert^2\) (minimizing the squares of error: hence, ‘least squares’).</p> <p>Note that by orthogonality, we can say that \(\left\lVert w+w_\perp \right\rVert^2=\left\lVert w \right\rVert^2+\left\lVert w_\perp \right\rVert^2\). But, since \(w\) and \(Ax\) are both in \(W\), we can rewrite</p> \[\left\lVert b-Ax \right\rVert^2=\left\lVert w+w_\perp-Ax \right\rVert^2=\left\lVert w-Ax \right\rVert^2+\left\lVert w_\perp \right\rVert^2\] <p>Notice that no matter <em>what</em> \(x\) is, our squared error will always be \(\geq\left\lVert w_\perp \right\rVert^2\). This is hopefully somewhat intuitive. Since \(w_\perp\) is the part of \(b\) that makes our system inconsistent, its size acts as a lower bound for our error, telling us in a sense <em>how</em> inconsistent our system is. Cool, right?</p> <p>Thus, the only thing we <em>can</em> do to minimize error, is to minimize \(\left\lVert w-Ax \right\rVert^2\). The best we can do is make it zero. But… like we said before, \(w=Ac\) for some \(c\). Then, \(x=c\) will give us zero. So \(x=c\), the solution to \(Ax=w\), really <em>is</em> the solution that minimizes the error!</p> <p>One might object: what if \(A\) has dependent columns and there are multiple solutions to \(Ax=w\)? Well, we can see that <em>all</em> of those solutions will minimize the error. That is, even if the least squares solution is not unique, every single one will minimize the error! This is because the error is in terms of the magnitude of \(w-Ax\). And \(w-Ax=0\) if and only if \(x\) is a preimage of \(w\) of \(A\) (there is no restriction on <em>which preimage</em>).</p> <h2 id="the-smallest-closest-solution">The smallest closest solution</h2> <p>If you’re very picky, and you want to pick <em>just one</em> least squares solution, then you <em>could</em> theoretically pick the least squares solution with the minimum magnitude. If \(c\) is one least squares solution, then any other least squares solution will be of the form \(c+u\) where \(u\in\ker(A)\). Then, we can play a similar game in trying to minimize</p> \[\left\lVert c+u \right\rVert^2=\left\lVert k+k_\perp+u \right\rVert^2=\left\lVert k+u \right\rVert^2+\left\lVert k_\perp \right\rVert^2\] <p>where we are decomposing \(c\) in terms of \(k\in\ker(A)\) and \(k_\perp\in\ker(A)^\perp\). Since our degree of freedom is in choosing \(u\), we should pick \(u=-k\), and then we find that the ‘least squares least squares solution’ is the solution in the orthogonal complement of \(\ker(A)\).</p> \[x_{least}=\operatorname{proj}_{\ker(A)^\perp}(c)\] <p>If you are hankering for an expression for the projector into \(\ker(A)^\perp\), then one method one could use (especially if they’re in some sort of computing/programming environment) is to use the \(QR\) decomposition of \(A^*=QR\), and then</p> \[x_{least}=QQ^*c\] <p>This is because, as we said above, the orthogonal complement of \(\ker(A)\) is the column space of \(A^*\), and \(QQ^*\) from the \(QR\) decomposition projects orthogonally into the column space.</p> <p>We can actually do much better using the reduced SVD of \(A=USV^*\), where \(S\) is square and invertible. Then, you can show that</p> \[x_{least}=VS^{-1}U^*b\] <p>Which is why we call \(VS^{-1}U^*\) the “pseudo-inverse of \(A\)”. It always gives the smallest least squares solution! This is absolutely as good as it gets for a singular matrix.</p> <p>I think it’s a good exercise to try and show that \(VS^{-1}U^*b\) is in fact the smallest least squares solution! If you need some hints to get started,</p> <ul> <li>\(UU^*\) is a projector onto the column space of \(A\)</li> <li>\(VV^*\) is a projector onto the column space of \(A^*\), which is orthogonal to \(\ker(A)\)</li> <li>If \(b=w+w_\perp\), then what is \(w\) in terms of \(b\) and \(U\)?</li> </ul> <p><a href="https://youtu.be/M5CeQG1YfEQ?si=2J5M9Tdyq01GVAsc" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor F.</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[Why the heck do we multiply by the transpose]]></summary></entry><entry><title type="html">The Wonderful World of Projectors</title><link href="https://smashmath.github.io/blog/projectors/" rel="alternate" type="text/html" title="The Wonderful World of Projectors"/><published>2024-03-23T00:00:00+00:00</published><updated>2024-03-23T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/projectors</id><content type="html" xml:base="https://smashmath.github.io/blog/projectors/"><![CDATA[<p>The following is my final presentation for my numerical linear algebra class (Spring 2023), the first graduate math course I ever took! I was really proud of it, and I feel like there’s not enough good resources about projectors, which I think are really really cool.</p> <p>For simplicity, we will let \(\mathbb{F}\) be \(\mathbb{R}\) or \(\mathbb{C}\).</p> <h1 id="projectors">Projectors</h1> <h2 id="idempotent-matrices">Idempotent Matrices</h2> <p><strong>Definition:</strong> A matrix \(P \in \mathbb{F}^{n \times n}\) is said to be a <strong>Projector</strong> if \(P^2 = P\). This matrix is also said to be ‘idempotent’. We say that \(P\) projects vectors <em>onto</em> \(\text{im}(P)\) <em>along</em> \(\text{ker}(P)\).</p> <p>Note that the only invertible projector is the identity matrix (this is a simple thing to prove).</p> <p>Why would this be the <em>definition</em> for a projector? What does \(P^2 = P\) have to do with projections? The general idea is this: Say \(\mathbb{F}^{n}=W\oplus K\), and we have \(P\) projects onto \(W\) along \(K\). That means that every vector \(x\) can be written in the form \(x=w+k\) for some \(w\in W\) and \(k\in K\), this representation is unique, and we expect that \(Px=w\) and \(Pw=w\), implying that \(Px=Pw\). This is because \(w\) is already in \(W\), so it shouldn’t change under the projection. If we examine this more closely, it means that</p> \[P^2x=P(Px)=Pw=Px\] \[\implies P^2x=Px\] <p>for all \(x\). Therefore, a projector should satisfy \(P^2=P\).</p> <p>We remark that this construction of \(Pw=w\) and \(Pk=0\) implies that we can decompose \(\mathbb{F}^n\) into a direct sum of the eigenspaces with eigenvalues \(1\) and \(0\). We will prove this shortly.</p> <p>To summarize: \(P\) takes in \(x\) and outputs the component of \(x\) in the direction of \(W\) (or the component in \(W\)). Thus, repeatedly applying \(P\) shouldn’t change the output because the result is already <em>in</em> \(W\).</p> <p>It’s not necessarily obvious that if \(P^2=P\) then \(P\) is a projector, however. To prove that, we need some more groundwork. Some questions we need to answer first: How do we know that \(\mathbb{F}^n\) will be a direct sum of the image and kernel of \(P\)?</p> <p>From this point on, we assume that \(P\) satisfies \(P^2=P\).</p> <p><strong>Proposition:</strong> \(\text{im}(P) \oplus \text{ker}(P) = \mathbb{F}^n\)</p> <p><strong>Proof:</strong> For all \(x \in \mathbb{F}^n\),</p> \[x = Px + (x - Px)\] \[P(x - Px) = Px - P^2x = Px - Px = 0\] \[\implies x - Px \in \text{ker}(P)\] <p>Therefore, all \(x\) can be written as the sum of some \(Px \in \text{im}(P)\) and \(x - Px \in \text{ker}(P)\). Thus, \(\mathbb{F}^n = \text{im}(P) + \text{ker}(P)\). If</p> \[\begin{multline*} v \in \text{im}(P) \cap \text{ker}(P) \\\implies v = Px \land Pv = 0 = P(Px) = Px = v \\\implies v = 0 \end{multline*}\] <p>Therefore,</p> \[\mathbb{F}^n = \text{im}(P) \oplus \text{ker}(P)\] <h2 id="eigenspaces-of-projectors">Eigenspaces of Projectors</h2> <p>Since all but the trivial projector (\(I\)) are not invertible, then they usually have a nontrivial kernel, which is the eigenspace for zero.</p> <p><strong>Proposition:</strong> The only eigenvalues for a projector \(P\) are \(\lambda=0,1\).</p> <p><strong>Proof:</strong> Suppose that \(\lambda\) is an eigenvalue of \(P\) with eigenvector \(v \neq 0\). Then</p> \[Pv=\lambda v \implies P^2v=Pv=\lambda v=\lambda Pv=\lambda^2v\] \[\implies (\lambda^2-\lambda) v=0 \implies \lambda=0,1\] <p><strong>Proposition:</strong> For a projector \(P\), the eigenspace for \(\lambda=1\), \(E_1=\text{im}(P)\).</p> <p><strong>Proof:</strong> Suppose \(v \in \text{im}(P)\). Then there exists an \(x \in \mathbb{R}^n\) such that</p> \[v=Px \implies Pv=P^2x=Px=v\] <p>Then \(Pv=v\). If \(v \in \text{ker}(P)\), then \(Pv=0=v\). Thus, either \(v\) is an eigenvector with eigenvalue \(1\), or \(v=0\). That is, \(\text{im}(P) \subseteq E_1\).</p> <p>The other direction is quite simple. If \(v\) is an eigenvector with eigenvalue \(1\), then \(Pv=v\) implies \(v \in \text{im}(P)\). Therefore,</p> \[\text{im}(P)=E_1\] <p><strong>Proposition:</strong> All projectors are diagonalizable.</p> <p><strong>Proof:</strong> Since \(\text{im}(P)=E_1\) and \(\text{ker}(P)=E_0\), by Proposition, we can say that</p> \[\mathbb{F}^n=\text{im}(P) \oplus \text{ker}(P)=E_1 \oplus E_0\] <p>Since \(\mathbb{F}^n\) is a direct sum of eigenspaces of \(P\), then \(P\) must be diagonalizable (as this implies there exists a basis of eigenvectors).</p> <p>For a slightly slicker and less intuitive proof (something I definitely love), we can also use that</p> \[P^2=P \implies P^2-P=P(P-I)=0\] <p>A rather obscure linear algebra factoid: \(A\) is diagonalizable if and only if there is some polynomial \(f(x)\) that is a product of distinct linear factors such that \(f(A)=0\). This is basically saying that a matrix is diagonalizable matrix if its minimal polynomial is a product of distinct linear factors (we only want Jordan blocks of size 1).</p> <p>Hence, if \(f(x)=x(x-1)\), which is a product of distinct linear factors, then \(f(P)=0\). Since such a polynomial exists, that is enough to prove \(P\) is diagonalizable.</p> <h2 id="complementary-projectors">Complementary Projectors</h2> <p><strong>Theorem:</strong> Suppose \(P\) is the projector onto the subspace \(W \leq \mathbb{F}^n\), and let \(K=\ker(P)\). We previously proved that \(\mathbb{F}^n=W \oplus K\), so for all vectors \(v \in \mathbb{F}^n\), \(v\) can be written uniquely as a linear combination</p> \[v=w+k\] <p>where \(w \in W\) and \(k \in K\).</p> <p>How can we find \(w\) and \(k\)? To find \(w\), we can use the projector \(P\) for \(W\). But what about \(k\)? To find it, we can do</p> \[k=v-w=v-Pv=(I-P)v\] <p>We can verify as well that \(I-P\) is indeed also a projector.</p> \[\begin{multline*} (I-P)^2=I^2-2P+P^2\\=I-2P+P=I-P \end{multline*}\] <p>Therefore, we define the complementary projector of \(P\) as follows:</p> <p><strong>Definition:</strong> The <strong>complementary projector</strong> of the projector \(P\) is \(I-P\), which projects onto the kernel of \(P\).</p> <p>So if \(P\) is a projector, it projects onto its image along its kernel. While \(I-P\) is also a projector, which projects onto the kernel of \(P\) along the image of \(P\). It’s a good exercise to convince yourself of this. If \(Pv=\lambda v\), then what is \((I-P)v\)? Then see what happens if \(\lambda=0,1\).</p> <p>If you look back at the proof for Proposition, you’ll notice that we essentially wrote</p> \[x=Px+(I-P)x\] <p>which is writing \(x\) as the sum of its component in \(\text{im}(P)\) and its component in \(\text{im}(I-P)=\ker(P)\).</p> <h2 id="orthogonal-projectors">Orthogonal Projectors</h2> <p><strong>Definition:</strong> A projector \(P\) is an <strong>orthogonal projector</strong> if \(\ker(P)^\perp=\text{im}(P)\). That is, if the subspace it is projecting onto is orthogonal to the subspace it is projecting along. If \(P\) is not an orthogonal projector, then it is an <strong>oblique</strong> projector.</p> <p><strong>Proposition:</strong> A projector \(P\) is an orthogonal projector if and only if \(P\) is hermitian.</p> <p><strong>Proof:</strong> Suppose \(P \in \mathbb{F}^{n\times n}\) is an orthogonal projection matrix. That is, \(\text{im}(P)\) is the orthogonal complement to \(\ker(P)\). Suppose \(\beta_i=\{v_1,\ldots,v_r\}\) is an orthonormal basis for the image (\(E_1\)), and \(\beta_k=\{u_1,\ldots,u_k\}\) is an orthonormal basis for the kernel (\(E_0\)). As we proved previously, \(\beta=\beta_i \cup \beta_k\) is an eigenbasis for \(\mathbb{F}^n\). But, since they are orthogonal complements, \(\beta\) is an orthogonal basis. Since \(P\) has an orthonormal eigenbasis, and it has purely real eigenvalues, then \(P\) must be hermitian.</p> <p>If \(P\) is a hermitian projector, then its eigenspaces are orthogonal, because it is hermitian. Therefore, \(E_0^\perp=\ker(P)^\perp=E_1=\text{im}(P)\). Hence, \(\ker(P)^\perp=\text{im}(P)\). Therefore, \(P\) is an orthogonal projector.</p> <p>Since \(\ker(P)\) is the orthogonal complement to \(\text{im}(P)\), then \(I-P\) is specifically the projection onto the orthogonal complement of \(\text{im}(P)\). Thus, we get this next nice little corollary</p> <p><strong>Corollary:</strong> If \(P\) is the orthogonal projector onto a subspace \(W\), then \(I-P\) is the orthogonal projector onto \(W^\perp\).</p> <h2 id="constructing-projector-matrices">Constructing Projector Matrices</h2> <h3 id="diagonalization">Diagonalization</h3> <p>There are two ways to do this. If you have a basis for the image (suppose it is \(r\) dimensional) and kernel (making sure the union is a basis for \(\mathbb{F}^n\)), then put the vectors as columns in a matrix \(M\), and</p> \[P=M\begin{pmatrix}I_r &amp; 0 \\ 0 &amp; 0\end{pmatrix}M^{-1}\] <p>Which is… unwieldy at best. Especially since you actually need a basis for the kernel. But looking at it <em>this</em> way does make it a little more clear why \(P\) is a projector. This essentially tells us we can get a basis for \(\mathbb{F}^n\) which are either vectors in \(E_1\) or in \(E_0\). If we are in \(E_1\), then \(P\) doesn’t change it (which is how a projection should act), and if we are anywhere else but \(E_1\), then we are in \(E_0\), so \(P\) takes it to zero (also the way we would expect a projector to act).</p> <p>This form also makes it a little more clear why \(I-P\) is a projector. Since</p> \[I-P=M\begin{pmatrix}0 &amp; 0 \\ 0 &amp; I_{n-r}\end{pmatrix}M^{-1}\] <p>This is also pretty much one of the only ways to construct an oblique projector.</p> <p>But this is generally not how we generally compute projectors.</p> <h3 id="rank-one-oblique-projectors">Rank one oblique projectors</h3> <p>A special case of this is when the subspace we are projecting along is one-dimensional. So suppose that \(w\) is a basis for a one-dimensional subspace \(W\), \(v\) is a basis for the orthogonal complement of another \(n-1\) dimensional subspace \(K\) such that \(\mathbb{F}^n=W \oplus K\), and \(v^*w \neq 0\). Then</p> \[P=\frac{wv^*}{v^*w}\] <p>is the projector onto \(W\) along \(K\). You can also just start with any \(v\) not orthogonal to \(w\), and then the above matrix automatically projects onto \(W\) along \(\text{span}\{v\}^\perp\).</p> <p>For example,</p> \[P=\begin{pmatrix}1 \\ 1\end{pmatrix}\begin{pmatrix}3 &amp; -2\end{pmatrix}=\begin{pmatrix}3 &amp; -2 \\ 3 &amp; -2\end{pmatrix}\] <p>is the oblique projector onto \(\text{span}\left\{\begin{pmatrix}1 \\ 1\end{pmatrix}\right\}\) along \(\text{span}\left\{\begin{pmatrix}2 \\ 3\end{pmatrix}\right\}\).</p> <h3 id="column-space-orthogonal-projectors">Column space orthogonal projectors</h3> <p>The other way to construct a projector matrix is to use a basis for the image. Let’s say we want \(P\) to be a projector on a subspace \(W\), and we have a basis \(\{v_1, \ldots, v_r\}\) and we put them as the columns of a matrix \(A\). Note that \(A\) will have full column rank, so \(A^*A\) will be invertible. Then</p> \[P=A(A^*A)^{-1}A^*\] <p><strong>Proof:</strong> For all \(x \in \mathbb{F}^n\), we can write</p> \[x=x_W+x_{W^\perp}\] <p>uniquely, where \(x_W \in W\) and \(x_{W^\perp} \in W^\perp\). Our goal is to find some projector \(P\) such that \(Px=x_W\). <br/> Since the columns of \(A\) form a basis for \(W\), then there exists some \(c \in \mathbb{F}^r\) (the coordinate vector with respect to the basis) such that \(x_W=Ac\). Multiplying \(x=x_W+x_{W^\perp}=Ac+x_{W^\perp}\) on both sides by \(A^*\), we obtain</p> \[A^*x=A^*Ac+A^*x_{W^\perp}=A^*Ac+0\] <p>Since the entries of \(A^*x_{W^\perp}\) will be the inner product of vectors in \(W\) with a vector in \(W^\perp\), then that term zeros out. Because \(A\) is full rank, then \(A^*A\) is invertible so,</p> \[c=(A^*A)^{-1}A^*x\] \[\implies A(A^*A)^{-1}A^*x = Ac = x_W\] <p>Therefore, \(P=A(A^*A)^{-1}A^*\) is the projector.</p> <p>Now, while I think it makes it a little less intuitive why this should be a projector, this is definitely <em>much</em> easier to compute. And it makes the fact that it satisfies \(P^2=P\) pretty clear. Verify it!</p> <p>The only thing to note is that this always produces an orthogonal projector (since we can see this matrix is Hermitian by inspection). But that is actually a good thing because orthogonal projectors are actually much better than oblique projectors.</p> <p>Now, I think you’d agree that the \(A^*A\) inverse is pretty annoying. How can we get rid of it? Well, the easiest way would be to ensure that \(A^*A=I\). Then the projector is just \(AA^*\). So, how can we construct a matrix such that \(A^{-1}=A^*\)? The answer is: use an orthonormal basis.</p> <h1 id="orthonormal-bases">Orthonormal Bases</h1> <p><strong>Definition:</strong> A set of vectors is <strong>orthogonal</strong> if all of the vectors have inner product 0.</p> <p>Note that an orthogonal set of vectors may not be linearly independent, because a set can have the zero vector and still be orthogonal. This is because the zero vector is orthogonal to all vectors. However, we <em>can</em> say it is linearly independent if all the vectors are nonzero. We won’t prove this for an orthogonal set, but only for orthonormal sets. The proof is almost identical, however.</p> <p>For the sake of simplicity, though, when we say a set is “orthogonal” we will assume all the vectors are nonzero.</p> <p><strong>Definition:</strong> A set is <strong>orthonormal</strong> if it is an orthogonal set and all of the vectors are unit vectors.</p> <p><strong>Remark:</strong> One way we notate an orthonormal basis is as follows: Suppose \(\{u_1, \ldots, u_n\}\) is orthonormal, then</p> \[\langle u_i, u_j \rangle = u_j^*u_i = \begin{cases} 1, &amp; i=j \\ 0, &amp; i \neq j \end{cases}\] <p><strong>Theorem:</strong> An orthonormal set is linearly independent.</p> <p><strong>Proof:</strong> Suppose \(\{v_1, \ldots, v_n\}\) is an orthonormal set, and</p> \[c_1v_1 + \ldots + c_nv_n = 0\] <p>Dot both sides with \(v_i\), and all terms will cancel except for \(v_i \cdot c_iv_i\).</p> \[c_i(v_i \cdot v_i) = c_i||v_i||^2 = c_i = 0\] <p>Therefore, the vectors are linearly independent.</p> <h2 id="why-are-orthonormal-bases-so-great">Why are orthonormal bases so great?</h2> <p>Let’s list some reasons:</p> <ol> <li>We don’t have to check linear independence to verify it’s a valid basis. Only that we have the right number of unit vectors, and they are orthogonal. Orthogonality is much easier to check than linear independence or span.</li> <li>If a square matrix has orthonormal columns, then the inverse is as almost as easy as it gets. It’s just the adjoint! Even if it isn’t square, it will be easy to cancel.</li> <li>It is <em>extremely</em> easy to find the coefficients of a linear combination. We don’t have to solve a system of equations.</li> <li>It’s really easy to project onto a subspace when you have an orthonormal basis for it.</li> <li>Creating a projection matrix is <em>so much easier</em> if you have an orthonormal basis.</li> <li>You can always make a basis orthonormal.</li> </ol> <p>It is for these reasons that real symmetric/hermitian matrices are simply the best matrices, objectively. Because they are guaranteed to have an orthonormal eigenbasis. And we all know that an eigenbasis is the best basis, so an orthonormal eigenbasis is as good as it gets.</p> <p>The other classes of normal matrices (matrices which are unitarily diagonalizable) are also good for similar reasons (Unitary matrices are also fantastic), but <em>their</em> eigenvalues aren’t guaranteed to be real.</p> <h3 id="finding-coefficients-of-a-linear-combination">Finding coefficients of a linear combination</h3> <p>Imagine we have some vector \(x \in \mathbb{F}^n\), and we want to find the coefficients for a linear combination of an orthonormal basis \(B=\{v_1, \ldots, v_n\}\). Then we want</p> \[x = c_1v_1 + \ldots + c_nv_n\] <p>As we did before, we can dot both sides by \(v_i\) to obtain just \(c_i\) on the right-hand side.</p> \[v_i \cdot x = c_i\] <p>And there we go. The coefficients are obtained simply from the dot product. No system of equations required.</p> <p><strong>Theorem:</strong> If \(\{v_1, \ldots, v_n\}\) is an orthonormal basis for \(\mathbb{F}^n\), then for all \(x \in \mathbb{F}^n\)</p> \[x = (v_1 \cdot x)v_1 + \ldots + (v_n \cdot x)v_n\] <p>But notice that \(v_i \cdot x = v_i^*x\). If we let \(U = \left( v_1 \quad \cdots \quad v_n \right)\), then we can obtain the vector of coefficients from</p> \[\begin{pmatrix} v_1^*x \\ \vdots \\ v_n^*x \end{pmatrix} = U^*x\] <p><strong>Remark:</strong> It follows that</p> \[\begin{multline*} x = (v_1 \cdot x)v_1 + \ldots + (v_n \cdot x)v_n \\= U(U^*x) = UU^*x \end{multline*}\] <p>implying that \(UU^* = I\).</p> <h3 id="example">Example</h3> <p>Take the orthonormal basis for \(\mathbb{R}^2\)</p> \[\beta = \left\{ \frac{1}{\sqrt{2\pi^2+e^2}}\begin{pmatrix} \pi\sqrt{2} \\ e \end{pmatrix}, \frac{1}{\sqrt{2\pi^2+e^2}}\begin{pmatrix} -e \\ \pi\sqrt{2} \end{pmatrix} \right\}\] <p>and say we want to express the vector \(\begin{pmatrix} \sqrt{5} \\ \pi^2 \end{pmatrix}\) as a linear combination of those vectors.</p> \[\begin{pmatrix} \sqrt{5} \\ \pi^2 \end{pmatrix} =c_1 \frac{\begin{pmatrix} \pi\sqrt{2} \\ e \end{pmatrix}}{\sqrt{2\pi^2+e^2}} + c_2\frac{\begin{pmatrix} -e \\ \pi\sqrt{2} \end{pmatrix}}{\sqrt{2\pi^2+e^2}}\] <p>I do not care what you say, you simply <em>cannot</em> make me solve that system of equations. But, luckily, we don’t have to! The dot products are given by the transpose of the matrix</p> \[\frac{\begin{pmatrix} \pi\sqrt{2} &amp; e \\ -e &amp; \pi\sqrt{2} \end{pmatrix}}{\sqrt{2\pi^2+e^2}}\begin{pmatrix} \sqrt{5} \\ \pi^2 \end{pmatrix} = \frac{\begin{pmatrix} \pi\sqrt{10} + e\pi^2 \\ \pi^3\sqrt{2} - e\sqrt{5} \end{pmatrix}}{\sqrt{2\pi^2+e^2}}\] <p>Therefore,</p> \[\begin{multline*} \begin{pmatrix} \sqrt{5} \\ \pi^2 \end{pmatrix} \\= \frac{\pi\sqrt{10} + e\pi^2}{2\pi^2 + e^2}\begin{pmatrix} \pi\sqrt{2} \\ e \end{pmatrix} + \frac{\pi^3\sqrt{2} - e\sqrt{5}}{2\pi^2 + e^2}\begin{pmatrix} -e \\ \pi\sqrt{2} \end{pmatrix} \end{multline*}\] <p>As a bonus, we get the inverse of that nasty matrix to be</p> \[\begin{multline*} \left(\frac{1}{\sqrt{2\pi^2+e^2}}\begin{pmatrix}\pi\sqrt{2}&amp;-e\\e&amp;\pi\sqrt{2}\end{pmatrix}\right)^{-1}\\ =\frac{1}{\sqrt{2\pi^2+e^2}}\begin{pmatrix}\pi\sqrt{2}&amp;e\\-e&amp;\pi\sqrt{2}\end{pmatrix} \end{multline*}\] <h2 id="unitary-matrices">Unitary matrices</h2> <p>The matrix \(A^*A\) is Hermitian, and every entry is just the inner product of the columns. Thus, if the columns are orthonormal, then everything off the diagonal will be zero, and every diagonal entry will be the norm squared, which is one. Thus, \(A^*A=I\) if and only if the columns of \(A\) form an orthonormal basis. This means \(A^{-1}=A^*\) if \(A\) is square. But, even if it isn’t, \(A^*A\) will still yield an identity matrix, allowing us to cancel both sides.</p> <h3 id="fun-facts-about-unitary-matrices">Fun facts about unitary matrices</h3> <p>If \(U\) is a square unitary matrix, then</p> <ol> <li>\(U\) is unitarily diagonalizable</li> <li>All of the eigenvalues of \(U\) are of the form \(\lambda_j=e^{i\theta_j}\). That is, \(\vert \lambda_j\vert=1\).</li> <li>There exists a hermitian matrix \(H\) such that \(U=e^{iH}\). If \(U=VDV^*\), and \(\lambda_j=e^{i\theta_j}\), then one such matrix is \(H=V\text{diag}\{\theta_1, \ldots, \theta_n\}V^*\).</li> <li>\(\langle x, y \rangle = \langle Ux, Uy \rangle\). That is, \(U\) preserves the angle between vectors.</li> <li>\(\vert\vert x\vert\vert = \vert\vert Ux\vert\vert\). That is, \(U\) preserves the length of all vectors.</li> </ol> <p>Just know, this only scratches the surface. Unitary matrices are simply astounding.</p> <h2 id="orthogonal-projections-when-you-have-an-orthonormal-basis">Orthogonal Projections when you have an orthonormal basis</h2> <p><strong>Theorem:</strong> If the columns of \(Q\) form an orthonormal basis for a subspace \(W\), then</p> \[P=QQ^*\] <p>is the orthogonal projector onto \(W\).</p> <p>If \(\{v_1, \ldots, v_r\}\) is an orthonormal basis for a subspace \(W\), then we can simply obtain the projection as</p> \[\text{proj}_W(x) = (v_1 \cdot x)v_1 + \ldots + (v_r \cdot x)v_r\] <p>Since no matter what the other vectors in the orthogonal complement to \(W\) happen to be (which, if we completed the orthonormal basis, the rest of the vectors would have to be a basis for \(W^\perp\)), they would dot to zero. Therefore, if \(Q = \left( v_1 \quad \cdots \quad v_r \right)\), then the projection matrix is simply</p> \[P = QQ^*\] <p>No \(A^*A\) inverse shenanigans. The reason is that \(Q^*Q = I\), so the formula \(Q(Q^*Q)^{-1} Q^*\) just reduces to \(QQ^*\).</p> <p><strong>Remark:</strong> The \(Q\) in</p> \[P = QQ^*\] <p>can actually be a \(Q\) from the \(QR\) factorization of any matrix \(A\) with full column rank and the same column space. Since \(R\) will be invertible,</p> \[\begin{multline*} A(A^*A)^{-1} A^* \\ = QR((QR)^*QR)^{-1} (QR)^* \\ = QR(R^*Q^*QR)^{-1}(R^*Q^*)\\ = QR(R^*R)^{-1} R^*Q^* \\ = QR(R^{-1} (R^*)^{-1})R^*Q^* \\ = QQ^* \end{multline*}\] <p><strong>Remark:</strong> If \(A\) has orthogonal columns instead of orthonormal, then \(A^*A \neq I\), but it will be diagonal. So \(A(A^*A)^{-1} A^*\) is not that bad. However, the diagonal entries are just the norm squared, so putting \((A^*A)^{-1}\) is equivalent to dividing the columns of \(A\) (and the rows of \(A^*\)) by the norm, normalizing the columns. Thus, we still get the same answer.</p> <p><a href="https://youtu.be/T2kOj-GFN8k?si=So_pJbTwG_n3-BWH" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor Fisher</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[They're cool, I promise!]]></summary></entry><entry><title type="html">Constant Coefficient ODEs Made Simple with Linear Operators</title><link href="https://smashmath.github.io/blog/linalglinconstcoef/" rel="alternate" type="text/html" title="Constant Coefficient ODEs Made Simple with Linear Operators"/><published>2023-11-11T00:00:00+00:00</published><updated>2023-11-11T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/linalglinconstcoef</id><content type="html" xml:base="https://smashmath.github.io/blog/linalglinconstcoef/"><![CDATA[<p>Last year, I made a post about <a href="../linconstcoef/" target="_blank">this same topic</a>, but since then, I have developed a new way of understanding this topic through the lens of linear algebra which makes many of the seemingly arbitrary choices seem almost stupidly obvious. One time, I was discussing differential equations with a friend of mine at a bar who was taking a class in it at the time. He asked me, “why do we know that the solutions to these equations <em>have</em> to be exponentials?” My explanation was not great, but years later I presented the following explanation to him at a different bar which I think was much better. In his words, it was “mindblowing”. I’m hoping you will feel similarly. <strong>It does, however, require linear algebra.</strong></p> <p>We will address the following questions in the context of constant coefficient linear ordinary differential equations:</p> \[a_ny^{(n)}+\ldots+a_1y'+a_0y=0\] <ol> <li>Why is our guess for the solution \(e^{\lambda t}\)?</li> <li>Why do we multiply by \(t\) when we have a repeated root of the characteristic polynomial?</li> <li>Why does an \(n\)th order equation have exactly \(n\) linearly independent homogeneous solutions?</li> </ol> <p>And as a bonus, I’m going to explain why the <a href="../alphamethod/" target="_blank"><strong>exponential response formula</strong></a> is actually absurdly trivial!</p> <h1 id="linear-algebra-recap">Linear Algebra Recap</h1> <p>Let us review some basic linear algebra facts and terminology.</p> <p>A <strong>subspace</strong> is a nonempty subset of a vector space that is closed under linear combinations.</p> <h2 id="linear-transformations">Linear Transformations</h2> <p>A linear transformation \(T\), is a function that preserves linear combinations.</p> <p>\begin{equation} T(c_1x_1+\ldots+c_kx_k)=c_1T(x_1)+\ldots+c_kT(x_k) \end{equation}</p> <p>The derivative \(\frac{d}{dt}\) is an example of a linear transformation. This is by the linearity of differentiation. That is, by the fact that we can pull out constants from derivatives, and that the derivative of a sum is just the sum of derivatives.</p> <p>If \(T(x)=y\), then we say that \(y\) is the <strong>image</strong> of \(x\) under \(T\) (often, we just say the image of \(x\)). The image is unique because \(T\) is a function. We call \(x\) the <strong>preimage</strong> of \(y\) under \(T\). A preimage is not necessarily unique.</p> <p>If the image of an element \(x\) is \(0\) (\(T(x)=0\)), then we say that \(x\) is in the <strong>kernel</strong> of \(T\) (\(x\in\ker(T)\)). The kernel is the set of all preimages of \(0\). We sometimes call the kernel the set of all <strong>homogeneous solutions</strong> to the equation \(T(x)=0\). The kernel is a subspace.</p> <h3 id="preimage-theorem">Preimage Theorem</h3> <p>Assume the equation \(T(x)=b\) (which is asking the question “what is a preimage of \(b\)?”) has at least one solution, \(x=x_p\). Then <em>every</em> solution is of the form \(x=x_p+x_h\) where \(x_h\) is an element in \(\ker(T)\). That is, <strong>all preimages of a linear transformation are off from a single particular preimage by something in the kernel</strong>.</p> <p><strong>Proof:</strong> If \(x_0\) is a vector of the form \(x_0=x_p+x_h\), then \(T(x_0)=T(x_p+x_h)=T(x_p)+T(x_h)=b+0=b\). Hence, \(x_0\) is also a preimage of \(b\), showing that any vector of this form is also a solution.</p> <p>Suppose we have some other solution \(x_1\). Then \(T(x_1-x_p)=T(x_1)-T(x_p)=b-b=0\). Hence, \(x_1-x_p\in\ker(T)\), so \(x_1-x_p=x_h\) for some \(x_h\in\ker(T)\), and thus \(x_1=x_p+x_h\), concluding the proof. \(\square\)</p> <p>This last step actually implies that a <strong>general solution</strong> to \(T(x)=b\) is an expression of the form</p> \[x=c_1x_1+\ldots+c_nx_n+x_p\] <p>where \(x_1,\ldots,x_n\) form a basis for \(\ker(T)\) and \(x_p\) is any particular solution. This is because if for any solution \(x-x_p\in\ker(T)\), then by establishing a basis \(x_1,\ldots,x_n\) we are saying that \(x-x_p\) can be written in the form \(c_1x_1+\ldots+c_nx_n\).</p> <p>For the derivative operator, the kernel is just the set of all constant functions. Any preimage under the operator is also just an antiderivative. And one of the important theorems of calculus is that any two antiderivatives differ by a constant, which is exactly what the preimage theorem states for this example.</p> <h2 id="eigenvectors">Eigenvectors</h2> <p>An <strong>eigenvector</strong> of \(T\) is a nonzero vector \(v\) for which \(T\) just scales \(v\). That is, \(T(v)=\lambda v\) for some \(\lambda\in F\) (note we are using \(F\) here because this works for any field. Eventually we are going to be assuming \(F\) is \(\mathbb{R}\) or \(\mathbb{C}\)). \(\lambda\) is called the <strong>eigenvalue</strong> of \(v\). Note that this means that</p> \[(T-\lambda I)v=0\implies v\in\ker(T-\lambda I)\] <p>As a very mundane but useful fact, if the eigenvalue of \(v\) is nonzero, then we can just divide both sides of \(T(v)=\lambda v\) by \(\lambda\) to get that \(T\left(\frac{v}{\lambda}\right)=v\implies\frac{v}{\lambda}\) is an easy preimage of \(v\).</p> <p>\begin{equation}\label{divide} T(v)=\lambda v\neq0\implies T\left(\frac{v}{\lambda}\right)=v \end{equation}</p> <p>Another important fact is that \(T^n(v)=\lambda^n v\).</p> <p>We call the set of all eigenvectors with eigenvalue \(\lambda\) the <strong>eigenspace of \(\lambda\)</strong>. The <strong>generalized eigenspace of \(\lambda\)</strong> is the set of all generalized eigenvectors which we denote \(E_\lambda\). Recall that generalized eigenvectors are all vectors \(w\) for which</p> \[(T-\lambda I)^kw=0\] <p>for some integer \(k\).</p> <h2 id="linear-operators-and-polynomials">Linear Operators and Polynomials</h2> <p>Given a polynomial \(p(x)=a_0+a_1x+\ldots+a_nx^n\), we define</p> <p>\begin{equation} p(T)=a_0I+a_1T+\ldots+a_nT^n \end{equation}</p> \[\implies p(T)x=a_0x+a_1T(x)+\ldots+a_nT^n(x)\] <p>Here are some insanely important facts</p> <ol> <li>Polynomials in an operator commute. That is, for all polynomials \(p,q\), \(p(T)q(T)=q(T)p(T)\). This includes the specific case of first order factors \((T-aI)(T-bI)=(T-bI)(T-aI)\).</li> <li>The kernel is just the eigenspace of \(0\). That is, finding the kernel of an operator can be done by finding all eigenvectors with eigenvalue zero.</li> <li>If \(v\) is an eigenvector of \(T\) with eigenvalue \(\lambda\), then \(v\) is also an eigenvector of \(p(T)\) with eigenvalue \(p(\lambda)\) (this is a very good thing to prove, and it’s a very short proof).</li> <li>\(p(T)\) can have eigenvectors that are not eigenvectors of \(T\). One example is that \(\begin{pmatrix}1\\0\end{pmatrix}\) is an eigenvector of \(J=\begin{pmatrix}\lambda&amp;1\\0&amp;\lambda\end{pmatrix}\), but \(\begin{pmatrix}0\\1\end{pmatrix}\) is an eigenvector of \((J-\lambda I)^2=\begin{pmatrix}0&amp;0\\0&amp;0\end{pmatrix}\) and not of \(J\).</li> <li>Combining the above three points tells us that finding all eigenvectors of \(T\) with eigenvalue \(\lambda\) such that \(p(\lambda)=0\) will definitely give a subset of the kernel of \(p(T)\). However, it may not necessarily give a full basis for \(\ker(p(T))\).</li> </ol> <h1 id="the-differential-operator">The Differential Operator</h1> <p>We are going to focus in on the differential operator \(D\) defined by</p> <p>\begin{equation} Dy=y’ \end{equation}</p> <p>This is a linear operator as mentioned before. As a note, we will omit the \(I\) when writing something like \((D-\lambda I)\). We will just write \((D-\lambda)\). Further, \(y^{(k)}=D^ky\).</p> <p>Okay, so now, let’s bring everything we talked about above together:</p> <p>The differential equation</p> <p>\begin{equation} a_ny^{(n)}+\ldots+a_1y’+a_0y=g(t) \end{equation}</p> <p>where the \(a_i\)’s can be complex numbers.</p> \[a_nD^ny+\ldots+a_1Dy+a_0y=g(t)\] \[\left(a_nD^n+\ldots+a_1D+a_0\right)y=g(t)\] <p>is just the equation \(p(D)y=g(t)\), where \(p(x)=a_nx^n+\ldots+a_1x+a_0\). That is, we are looking for preimages of \(g(t)\) under \(p(D)\), and every solution will be of the form \(y=y_p+y_h\) where \(y_p\) is some preimage of \(g(t)\), and \(y_h\) is any vector in the kernel of \(p(D)\). Note that because the kernel is always a subspace, this gives us the superposition property of homogeneous solutions for free.</p> <p>i.e. If \(y_1,\ldots,y_n\) are solutions to \(p(D)y=0\), then so will \(c_1y_1+\ldots+c_ny_n\).</p> <p>As before, to find the <strong>general solution</strong>, we need a basis \(\{y_1,\ldots,y_k\}\) of the kernel of \(p(D)\). Then, any solution \(y\) can be written as</p> \[y=y_p+c_1y_1+\ldots+c_ky_k\] <p>So how can we start to find a basis for the kernel? And how many vectors will be in it? Well, we know that any eigenvector of \(D\) with eigenvalue \(\lambda\) will be an eigenvector of \(p(D)\) with eigenvalue \(p(\lambda)\). So we can try to find some solutions by solving \(p(\lambda)=0\).</p> \[p(\lambda)=a_n\lambda^n+\ldots+a_1\lambda+a_0=0\] <p>And there we go: the characteristic polynomial is now something obvious to try, and we have yet to even <em>mention</em> exponentials.</p> <p>So let \(\lambda\) be any solution to \(p(\lambda)=0\). Now we are looking for eigenvectors \(y\) of \(D\) with eigenvalue \(\lambda\). That is, we want \(Dy=\lambda y\implies y'=\lambda y\).</p> <h1 id="the-most-important-ode">The Most Important ODE</h1> <p>\begin{equation} y’=\lambda y \end{equation}</p> <p>Yeah, I said it.</p> <p>Just getting down to business solving it, we can divide both sides by \(y\) to get help us utilize the chain rule to reverse the differentiation. However, dividing by \(y\) eliminates the valid solution \(y=0\), so keep that in mind.</p> \[\frac{y'}{y}=\lambda\] <p>But \(\frac{y'}{y}=\frac{d}{dt}\ln\left\lvert y \right\rvert\). So \(\frac{d}{dt}\ln\left\lvert y \right\rvert=\lambda\). That is,</p> \[\ln\left\lvert y \right\rvert=\lambda t+c\implies \left\lvert y \right\rvert=e^ce^{\lambda t}\] <p>Eliminating the absolute value would give us a \(\pm\), and since we also know \(y=0\) is a soultion, we can just say</p> \[\implies y=Ce^{\lambda t}\implies y\in\operatorname{span}(e^{\lambda t})\] <p>Remember, \(y\in\operatorname{span}(e^{\lambda t})\) just means that \(y\) is some linear combination of \(e^{\lambda t}\), which just means \(y=c_1e^{\lambda t}\).</p> <p>We have demonstrated that \(y'=\lambda y\implies y\in\operatorname{span}(e^{\lambda t})\), and I encourage you to verify that \(y'=\lambda y\impliedby y\in\operatorname{span}(e^{\lambda t})\). However, how do we <em>know</em> this provides us with <em>every</em> solution? No, get that Picard–Lindelöf s#!% out of here (it’s beautiful, yes, but not particularly intuitive). Suppose we have two solutions \(y_1,y_2\) with \(y_1\neq0\). Then</p> \[\frac{d}{dt}\left(\frac{y_2}{y_1}\right)=\frac{y_1y_2'-y_2y_1'}{y_1^2}\] \[=\frac{y_1(\lambda y_2)-y_2(\lambda y_1)}{y_1^2}=0\] <p>So \(\frac{y_2}{y_1}\) is just some constant \(C\), which implies that \(y_2=Cy_1\), which implies \(y_2\) is necessarily linearly dependent on \(y_1\). Therefore, we’ve shown that the solution space of \(y'=\lambda y\) (which is just \((D-\lambda)y=0\): the kernel of \((D-\lambda)\)) has dimension exactly 1, and it has a basis \(e^{\lambda t}\). In other words,</p> <p>\begin{equation}\label{kerda} \ker(D-\lambda)=\operatorname{span}(e^{\lambda t}) \end{equation}</p> <p>This demonstrates that <strong>every eigenvector of the differential operator is an exponential function \(Ce^{\lambda t}\)</strong>.</p> <p>Now, we have answered our first question:</p> <p><strong>Why is our guess for the solution \(e^{\lambda t}\)?</strong></p> <p><em>Because exponentials are the unique functions that can be eigenvectors of the differential operator</em>. Therefore, they will also be eigenvectors of the polynomial differential operator.</p> \[p(D)e^{\lambda t}=p(\lambda)e^{\lambda t}\] <p>So if \(p(\lambda)=0\), then \(e^{\lambda t}\) is a solution. That is why we consider</p> \[p(\lambda)=a_n\lambda^n+\ldots+a_1\lambda+a_0=0\] <p>Thus, because the kernel is a subspace closed under linear combinations, we can take a linear combination of exponentials \(e^{\lambda_1t},\ldots,e^{\lambda_nt}\), where \(\lambda_1,\ldots,\lambda_n\) are the roots of \(p(x)\), to say</p> \[y=c_1e^{\lambda_1t}+\ldots+c_ke^{\lambda_nt}\] <p>will be a solution to \(p(D)y=0\). Note: we do not say the <em>general</em> solution quite yet, as any repeated roots will make the set of these exponentials trivially linearly dependent.</p> <p>We remark that \eqref{kerda} equivalently tells us that if \(q(x)\) is a degree one polynomial, then \(\ker(q(D))\) has a basis \(\{e^{\lambda t}\}\) for \(\lambda\) such that \(q(\lambda)=0\), implying that the general solution to \(q(D)y=0\) is \(y=Ce^{\lambda t}\).</p> <p>We actually have enough now to prove that an \(n\)th order equation has homogeneous solution space of dimension exactly \(n\), but I want to take a detour to repeated roots first.</p> <h1 id="repeated-roots">Repeated Roots</h1> <p>i.e. why in the sweet heavenly cheese husk do we multiply by \(t\) of all things to get our other solutions?</p> <p>I brushed over this topic with a meme about reduction of order last time, but I actually have a good explanation this time.</p> <p>Let’s start with the simplest case. What’s the solution to the differential equation</p> \[D^ky=0\implies y^{(k)}=0?\] <p>This is one of the easiest differential equations, because we can just integrate \(n\) times to get</p> \[y=c_1+c_2t+\ldots+c_kt^{k-1}\] <p>Okay, easy enough. Now you may be thinking, “okay, when you put it like that, yeah repeated roots means repeated differentiation so that’s where the \(t\)’s come from <em>in this case</em>. But what about \((D-\lambda)^k\)?”</p> <h2 id="exponential-shift">Exponential Shift</h2> <p>Consider the derivative of \(e^{\lambda t}f(t)\).</p> \[D\left(e^{\lambda t}f(t)\right)=e^{\lambda t}(f'(t)+\lambda f(t))=e^{\lambda t}(D+\lambda)f(t)\] <p>If you’ve taken enough derivatives, you may have caught on to this little shortcut. Since exponentials just get scaled by derivatives, we can just sum up the derivatives of the function multiplying it and scale by the constant appropriately. But let me tell you, this generalizes incredibly. We call this property the <strong>exponential shift</strong>:</p> <p>\begin{equation} De^{\lambda t}=e^{\lambda t}(D+\lambda),\quad e^{\lambda t}D=(D-\lambda)e^{\lambda t} \end{equation}</p> <p>I encourage you to prove that linearity actually guarantees that</p> \[\begin{gather} p(D)e^{\lambda t}=e^{\lambda t}p(D+\lambda)\\ e^{\lambda t}p(D)=p(D-\lambda)e^{\lambda t} \end{gather}\] <p>And this is going to make our lives <em>so</em> much easier. Let’s go back to</p> \[(D-\lambda)y=0\] <p>If we multiply by \(e^{-\lambda t}\), then we can change that pesky \((D-\lambda)\) into just \(D\). That is,</p> \[e^{-\lambda t}(D-\lambda)y=(D-(-\lambda)-\lambda)e^{-\lambda t}y\] \[=D(e^{-\lambda t}y)=0\] <p>And if \(e^{-\lambda t}y\) is in the kernel of \(D\), then it’s just a constant. So \(e^{-\lambda t}y=C\implies y=Ce^{\lambda t}\).</p> <p>To summarize,</p> <p>\begin{equation} e^{-\lambda t}(D-\lambda)=De^{-\lambda t} \end{equation}</p> <p>If you look at what we did closely, you’ll notice this is basically just an <a href="../integratingfactor/" target="_blank">integrating factor</a>.</p> <p>The takeaway from this is: <strong>we can leverage the exponential shift to look at the kernel of just \(D\), which is equivalent to integration</strong>.</p> <h2 id="exponential-shift-into-overdrive">Exponential Shift Into Overdrive</h2> <p>Alright, so let’s answer the question. What happens when we have a repeated root in \(p(x)\) in general? Say that \((x-\lambda)^k\) divides \(p(x)\). That is, \(p(x)=q(x)(x-\lambda)^k\). Then if we multiply \(e^{-\lambda t}\) to \(p(D)y=0\), we get</p> \[e^{-\lambda t}q(D)(D-\lambda)^ky=q(D+\lambda)D^k(e^{-\lambda t}y)\] <p>If we let \(u=e^{-\lambda t}y\), then we get \(q(D+\lambda)D^ku=0\). Thus, any solution to \(D^ku=0\) will also be a solution to \(q(D+\lambda)D^ku=0\). And we know the solution to that is just \(c_1+c_2t+\ldots+c_kt^{k-1}\). Therefore,</p> \[u=e^{-\lambda t}y=c_1+c_2t+\ldots+c_kt^{k-1}\] \[y=e^{\lambda t}\left(c_1+c_2t+\ldots+c_kt^{k-1}\right)\] <p>is a solution to \(p(D)y=0\).</p> <p>And we have now answered our second question, <strong>“Why do we multiply by \(t\) when we have a repeated root of the characteristic polynomial?”</strong></p> <p><em>Because exponentials shift derivatives, and the kernel of repeated differentiation is a polynomial.</em></p> <p>What we have shown also essentially implies that every \(n\)th order differential equation of the form \(p(D)y=g(t)\) can be solved using \(n\) integrating factors. The first of which can be \(e^{-\lambda t}\), where \(\lambda\) is any root of \(p(x)\). After doing \(n\) integrations, we will be left with \(n\) arbitrary constants. This is consistent with the dimension of the kernel of an \(n\)th order linear equation’s solution space being dimension \(n\), but it does not answer why the functions on each arbitrary constant will necessarily be linearly independent. That is what we intend to prove in <a href="#second-order-two-independent-solutions">a later section</a>.</p> <p>But, this actually <em>does</em> imply something else very important: A particular solution to \(p(D)y=Be^{\lambda t}\) is guaranteed to exist, and can be obtained by factoring \(p(D)\), applying integrating factors, and integrating (taking all integration constants to be zero) until it is solved. However, we can actually find this particular solution directly using the exponential response formula, which is what we are going to do next.</p> <p>As a final remark for this section: for constant coefficients, reduction of order <em>is</em> just the exponential shift, but more roundabout. This is because you basically insert an exponential into the equation with \(y=ve^{\lambda t}\), which will cause things to shift. For example, if you’ve done reduction of order on \(y''-2ky'+k^2y=0\), \(y=e^{kt}v\), then you do end up with \(v''=0\). Which is exactly what we would expect:</p> \[(D-k)^2e^{kt}v=e^{kt}D^2v=0\implies v''=0\] <p>Except we didn’t have to do the product rule a bunch of times, plug in, and simplify.</p> <h2 id="exponential-response-formula">Exponential Response Formula</h2> <p>Alright, I’m going to prove and motivate the exponential response formula (ERF) in one line. Please try not to laugh too hard at how absurdly simple it is. By \eqref{divide} (yeah, remember <em>that</em> one?), if \(p(\lambda)\neq0\), then</p> \[p(D)e^{\lambda t}=p(\lambda)e^{\lambda t}\implies p(D)\left(\frac{Be^{\lambda t}}{p(\lambda)}\right)=Be^{\lambda t}\] <p>so \(\frac{Be^{\lambda t}}{p(\lambda)}\) is a particular solution. Okay, that’s all folks. Seeya next time!</p> <hr/> <p>No, but really, it is that simple. We get \(\frac{Be^{\lambda t}}{p(\lambda)}\) as a preimage of \(Be^{\lambda t}\) because it’s an eigenvector, and we can just divide by the eigenvalue when it’s nonzero.</p> <p>We can use some of our techniques here to get the generalized exponential response formula (GERF) as well. However, it requires a result I don’t feel like proving:</p> <p><strong>\(p^{(k)}(\lambda)=0\) for \(0\leq k&lt;m\) and \(p^{(m)}(\lambda)\neq0\) if and only if \(\lambda\) is a root of \(p(x)\) with multiplicity exactly \(m\). And \(p(x)=q(x)(x-\lambda)^m\) where \(q(\lambda)=\frac{p^{(m)}(\lambda)}{m!}\neq0\)</strong></p> <p>Then for \(p(D)y=q(D)(D-\lambda)^my=Be^{\lambda t}\), we can use our good ol’ exponential shift with \(e^{-\lambda t}\).</p> \[q(D+\lambda)D^m(e^{-\lambda t}y)=B\] <p>Letting \(u=D^m(e^{-\lambda t}y)\), we get \(q(D+\lambda)u=B\), where we know that plugging in \(x=0\) into \(q(x+\lambda)\) will give us something nonzero. Hence, we can use our ERF to say a particular solution is \(u_p=\frac{B}{q(0+\lambda)}=\frac{Bm!}{p^{(m)}(\lambda)}\). That is,</p> \[D^m(e^{-\lambda t}y_p)=u_p=\frac{Bm!}{p^{(m)}(\lambda)}\] <p>Now, since we’re just looking for a single particular solution, we can just integrate \(m\) times and take all the constants to be \(0\). And since the \(m\)th integral of \(1\) is \(\frac{t^m}{m!}\), we basically just multiply by \(t^m\) and divide by \(m!\). Thus,</p> <p>\begin{equation}\label{gerf} y_p=\frac{Bt^me^{\lambda t}}{p^{(m)}(\lambda)} \end{equation}</p> <p>and there we go. If \(p^{(k)}(\lambda)=0\) for \(0\leq k&lt;m\) and \(p^{(m)}(\lambda)\neq0\), then \eqref{gerf} is a particular solution to \(p(D)y=Be^{\lambda t}\).</p> <h1 id="nth-order-n-independent-solutions">nth Order. n Independent Solutions</h1> <p>We can prove that \eqref{kerda}’s implication that a first order equation has kernel of dimension exactly one actually generalizes to \(n\)th order equations by induction. \eqref{kerda} is our base case, so let us assume that an \(n\)th order equation has a kernel of exactly degree \(n\). That is, if \(q(x)\) is degree \(n\), then \(\ker(q(D))\) has a basis \(y_1,\ldots,y_n\).</p> <p>Suppose \(p(x)\) is an \(n+1\)th degree polynomial. Then \(p(x)=(x-\lambda)q(x)\) for some \(q(x)\) that is degree \(n\) (this is guaranteed by the fundamental theorem of algebra, meaning \(\lambda\) might be complex). Hence,</p> \[p(D)y=0\implies (D-\lambda)q(D)y=0\] \[\implies q(D)y\in\ker(D-\lambda)\] <p>\eqref{kerda} then tells us that \(q(D)y=Ce^{\lambda t}\) for some \(C\). Let us first consider \(q(D)y=e^{\lambda t}\) (as we can multiply the particular solution by \(C\) to get a solution to the preceding equation). We know we can get some particular solution \(y_p\) to \(q(D)y=e^{\lambda t}\) using the GERF.</p> <p>That is, \(y_p\) is a preimage of \(e^{\lambda t}\) under \(q(D)\), so the general solution to \(q(D)y=e^{\lambda t}\) is \(y=c_1y_1+\ldots+c_ny_n+y_p\), where \(y_1,\ldots,y_n\) is a basis for \(\ker(q(D))\) (guaranteed to be size \(n\) by the inductive hypothesis) by the preimage theorem.</p> <p>But, if \(y_p\) is a preimage of \(e^{\lambda t}\) under \(q(D)\), then \(Cy_p\) is a preimage of \(Ce^{\lambda t}\) under \(q(D)\). Hence, the general solution to \(q(D)y=Ce^{\lambda t}\) is</p> \[y=c_1y_1+\ldots+c_ny_n+Cy_p\] <p>We claim that \(y\) is a solution to \(p(D)y=0\). If we take the image of \(y\) under \(q(D)\), we get \(Ce^{\lambda t}\) as we already defined \(y_1,\ldots,y_n\) to be a basis for its kernel and \(y_p\) to be a preimage of \(e^{\lambda t}\). That is,</p> \[q(D)\left(c_1y_1+\ldots+c_ny_n+Cy_p\right)=Ce^{\lambda t}\] <p>But if we then take the image of the result under \((D-\lambda)\), we will get zero by \eqref{kerda}. Hence, \(y\) is in the kernel of \((D-\lambda)\circ q(D)=(D-\lambda)q(D)=p(D)\).</p> <p>Now, \(y_p\) cannot be linearly dependent with \(y_1,\ldots,y_n\), because then it would be in the kernel of \(q(D)\) and thus could not be a preimage of \(e^{\lambda t}\neq0\) under \(q(D)\). Hence, \(\left\{y_1,\ldots,y_n,y_p\right\}\) form a set of \(n+1\) linearly independent vectors in \(\ker(p(D))\), making its dimension at least \(n+1\). We now need to show that there cannot be another linearly independent solution.</p> <p>Suppose that \(y_{n+2}\) is also a solution to \(p(D)y=0\) which is linearly independent with \(\{y_1,\ldots,y_n,y_p\}\). \(y_{n+2}\) can’t be in the kernel of \(q(D)\), because then it would be linearly dependent with \(y_1,\ldots,y_n\). So</p> \[q(D)y_{n+2}\neq0\implies q(D)y_{n+2}\in\ker(D-\lambda)\] <p>That is, \(q(D)y_{n+2}=c_{n+2}e^{\lambda t}\). But, like before, we know that \(c_{n+2}y_p\) will be a preimage of \(c_{n+2}e^{\lambda t}\) under \(q(D)\), so by the preimage theorem \(y_{n+2}=c_{n+2}y_p+y_h\) where \(y_h\in\ker(q(D))\). But, since \(y_h=d_1y_1+\ldots+d_ny_n\) for some constants \(d_i\), that means \(y_{n+2}=c_{n+2}y_p+d_1y_1+\ldots+d_ny_n\), contradicting that \(y_{n+2}\) is linearly independent from \(\{y_1,\ldots,y_n,y_p\}\). Therefore, the dimension of \(\ker(p(D))\) is exactly \(n+1\)!</p> <p>This answers our third question, <strong>“Why does an \(n\)th order equation have exactly \(n\) linearly independent homogeneous solutions?”</strong></p> <p>Because a homogeneous \(n\)th order equation is equivalent to a nonhomogeneous \((n-1)\)th order equation, for which the particular solution is necessarily linearly independent from the homogeneous solution in the dimension \(n-1\) kernel. And the particular solution for the reduced equation will then be a homogeneous solution for the \(n\)th order equation. And by the preimage theorem, any other solution will be linearly dependent on the general solution.</p> <p>As an unfortunate remark: this argument does not work for every type of differential equation. You do generally need more heavy duty techniques like reduction to a first order system of differential equations to prove this when the coefficients are not constant. Still, I like this very concrete proof for this very special case.</p> <h2 id="a-remark-on-undetermined-coefficients">A remark on undetermined coefficients</h2> <p>You can also utilize the exponential shift to more easily solve for more difficult particular solutions. If you have something like</p> \[p(D)y=q(t)e^{\alpha t}\] <p>where \(q(t)\) is some polynomial, then you can multiply by \(e^{-\alpha t}\) to make the RHS a simple polynomial. This will shift the equation to</p> \[p(D+\alpha)e^{-\alpha t}y=q(t)\] <p>In general, it’s slightly easier to solve this equation since your “guess” for a particular solution is just also a polynomial, removing the need to do product rules. Also, you can use <a href="../synthetictaylor/" target="_blank">synthetic division</a> to more easily calculate the coefficients of \(p(D+\alpha)\) without having to expand out the expressions and collect like terms. This is arguably a waste of time, and doesn’t cut down the computation time <em>too</em> much overall. I like to do it though.</p> <h2 id="complex-stuff">Complex Stuff</h2> <p>You may have noticed that I didn’t really specify what all the constants where in any of what I did. That’s because this general theory works for <em>all</em> polynomials, even complex ones. It’s only the very specific real polynomial case where you <em>can</em> always convert complex roots to sines and cosines. In most textbooks, they present this stuff in operator notation, but I’ll just speedrun through it for completion’s sake.</p> <p>If \(p(x)\) is a real polynomial (it has real coefficients, not complex ones), then \(p(D)\) is an operator that maps real functions to real functions. That is, if \(f(x)\) is a real valued function, then \(p(D)f(x)\) is also always real valued.</p> <p>Thus, if \(F(x)=u(x)+iv(x)\), then by linearity of linear transformations</p> \[p(D)F(x)=p(D)u(x)+ip(D)v(x)\] <p>So the real part of \(p(D)F(x)\) is \(p(D)\) applied to the real part of \(F(x)\), and similarly with the imaginary part.</p> <p>Hence, if \(p(D)F(x)=0\), then both the real and imaginary parts must be zero, so \(p(D)u(x)=p(D)v(x)=0\). That is, the real and imaginary parts of any complex element of the kernel are also individually in the kernel.</p> <p>It follows from <a href="../eulersformula/" target="_blank">Euler’s Formula</a> that if \(\alpha=a+bi\) is a root of \(p(x)\), then</p> \[e^{(a+bi)t}=e^{at}\left(\cos(bt)+i\sin(bt)\right)\] <p>will be a solution, so we can take the real and imaginary parts blah blah blah</p> \[y=e^{at}\left(c_1\cos(bt)+c_2\sin(bt)\right)\] <p>will be a solution.</p> <p>So, similar with repeated roots, the case of</p> \[y=(c_1+\ldots+c_kt^{k-1})e^{\alpha t}\] <p>just means two individual sets of solutions</p> \[\begin{align*} y_1=(c_1+\ldots+c_kt^{k-1})e^{at}\cos(bt)\\ y_2=(d_1+\ldots+d_kt^{k-1})e^{at}\sin(bt)\\ \end{align*}\] <p>It may seem like we’re doubling the number of solutions, but nah. This is because if \(p(x)\) is real valued, then any complex root will also have a corresponding complex conjugate root. And, since Euler’s formula basically just makes the conjugate flip the sign of the imaginary part, the two solutions taken from the conjugate solution will be linearly dependent on the two solutions obtained from the original complex root. That is, normally we have</p> \[y=(c_1+\ldots+c_kt^{k-1})e^{\alpha t}+(d_1+\ldots+d_kt^{k-1})e^{\overline\alpha t}\] <p>so we’re still left with \(2k\) solutions in the end.</p> <hr/> <h2 id="final-thoughts">Final Thoughts</h2> <p>I love this perspective. The ideas of “exponentials would be an expected guess”, “to get the other solutions, multiply one solution by an arbitrary function \(v(t)\)”, or “just reduce the \(n\)th order equation down to a system of first order equations” would be mildly intuitive, but only in retrospect. As someone who questions a lot, and wants to know <em>why</em> something is true, the answers felt very contrived.</p> <p>But once we change from a differential equation to a linear operator, suddenly everything is a lot simpler (at least to me). The progression from first order to \(n\)th order is smooth and things just work out exactly the way you would expect them to.</p> <ul> <li>Exponentials are an expected guess because they are eigenvectors of the differential operator.</li> <li>To get the other solutions, we do an exponential shift, which effectively has us multiply by our first given solution.</li> <li>The dimension is \(n\) because each linear differential factor increases the dimension of the kernel by one.</li> </ul> <p>Granted, these justifications <em>only</em> work for constant coefficients. And, in general, reduction of order and reduction to a system of first order equations is necessary to generalize these ideas. But I greatly relish in the idea that we can make a much more elementary argument for this most special of cases that doesn’t end up relying on Picard–Lindelöf.</p> <p>But this is coming from someone who thinks a lot about these subjects, so perhaps it isn’t much easier for you. Especially if you don’t know any linear algebra (and if you got here without knowing linear algebra, uh… why? thank you for reading, but why?) I hope this was helpful, though.</p> <p>Also, this is a heavily condensed version of a chapter from a Linear Algebra + Differential Equations textbook I’m working on in my spare time (for which I have almost none now that I am in a PhD program). I hope you liked it, and maybe you’ll want to check out the textbook when it’s done (ETA: probably never). Alright, thanks for reading.</p> <p><a href="https://youtu.be/Tptx8boeGhE?si=1G60x2ZMJgUPC8Gr" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor F.</name></author><category term="differential-equations"/><category term="best"/><summary type="html"><![CDATA[No more guessing. Let's make it intuitive with linear algebra.]]></summary></entry><entry><title type="html">In Defense of Cramer’s Rule</title><link href="https://smashmath.github.io/blog/cramers/" rel="alternate" type="text/html" title="In Defense of Cramer’s Rule"/><published>2023-06-27T00:00:00+00:00</published><updated>2023-06-27T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/cramers</id><content type="html" xml:base="https://smashmath.github.io/blog/cramers/"><![CDATA[<p>In this post, I am going to defend my dear determinants. I am a fan of them, and I think they are useful. I am <em>not</em> saying that they are always the best tool for the job.</p> <h2 id="the-problems-with-determinants">The Problems with Determinants</h2> <p>Now, to discuss why determinants <em>can</em> be great, I must first discuss why people don’t like them. Sheldon Axler famously hates them. Why? Well, there are a few main reasons:</p> <ul> <li>While very manageable when small, they get out of hand very quickly and become very computationally expensive</li> <li>They are not very sensitive tools. When the determinant is zero, it tells you nothing about the rank besides that it is not full. For example, if the determinant of a \(100\times100\) matrix is zero, you know basically nothing about its rank. And the computation power required to calculate that will really not have been worth it, when you could have just row reduced it to learn the same thing and more with much less work.</li> <li>It is difficult to define them without sounding like a raving lunatic</li> <li>They cannot be easily used for non-square systems</li> </ul> <p>All of these are absolutely valid criticisms. However, in the vast majority of cases where you would be expected to do them by hand, these issues simply do not matter!</p> <h2 id="2x2-cramers-rule">2x2 Cramer’s Rule</h2> <p>\(2\times2\) determinants are <em>easy</em>. The formula is easy enough to do in your head.</p> \[\begin{equation} \det\begin{pmatrix}a&amp;b\\c&amp;d\end{pmatrix}=ad-bc \end{equation}\] <p>And using Cramer’s rule, we can solve any \(2\times2\) system \(\mathbf{A}\mathbf{x}=\mathbf{b}\) by computing exactly three \(2\times2\) determinants!</p> <p>First, we define the notation: For the system \(\mathbf{A}\mathbf{x}=\mathbf{b}\), where \(\mathbf{A}\) is a square matrix, we define \(\mathbf{A}_i\) to be the matrix obtained by replacing the \(i\)th column with \(\mathbf{b}\). For example, if we have</p> \[\begin{equation} \begin{pmatrix}a&amp;b\\c&amp;d\end{pmatrix}\begin{pmatrix}x_1\\x_2\end{pmatrix}=\begin{pmatrix}B_1\\B_2\end{pmatrix} \end{equation}\] <p>then</p> \[\mathbf{A}_1=\begin{pmatrix}B_ 1&amp;b\\B_ 2&amp;d\end{pmatrix},\quad \mathbf{A}_2=\begin{pmatrix}a&amp;B_1\\c&amp;B_2\end{pmatrix}\] <p>Then, Cramer’s rule tells us that if \(\det(A)\neq0\), then the solution to the above system is</p> \[\begin{equation} x_1=\frac{\begin{vmatrix}B_1&amp;b\\B_2&amp;d\end{vmatrix}}{ad-bc},\quad x_2=\frac{\begin{vmatrix}a&amp;B_1\\c&amp;B_2\end{vmatrix}}{ad-bc} \end{equation}\] <p>The more general result being</p> \[\begin{equation} x_i=\frac{\det(\mathbf{A}_i)}{\det(\mathbf{A})} \end{equation}\] <p>And this is actually very quick! First you do \(ad-bc\), and see if it is zero. If it is, then you can check by inspection if the \(\mathbf{b}\) vector is a scalar multiple of the columns of \(\mathbf{A}\). If not, then you can replace the columns with the \(\mathbf{b}\) vector and take the determinant. Personally, what I do, is cover up the columns of an augmented matrix, and negate the first determinant I take.</p> \[\left( \begin{array}{cc|c} a&amp;b&amp;B_1\\ c&amp;d&amp;B_2 \end{array} \right)\] <h3 id="a-2x2-example">A 2x2 example</h3> <p>Consider the system of equations</p> \[\left( \begin{array}{cc|c} 6&amp;-5&amp;26\\ 14&amp;3&amp;2 \end{array} \right)\] <p>Now, this does not look fun to row reduce. Especially if you are like me and avoid fractions like the plague. So, let us use Cramer’s rule instead. I will show the work the way I would do it (avoiding large multiplications)</p> \[\begin{vmatrix}6&amp;-5\\14&amp;3\end{vmatrix}=2\begin{vmatrix}3&amp;-5\\7&amp;3\end{vmatrix}=2(9+35)=88\] <p>This tells us that</p> \[x_1=-\frac{}{88},\quad x_2=\frac{}{88}\] <p>Now, we cover up the first column:</p> \[\begin{vmatrix}-5&amp;26\\3&amp;2\end{vmatrix}=2\begin{vmatrix}-5&amp;13\\3&amp;1\end{vmatrix}=2(-5-39)=-88\] \[\implies x_1=-\frac{-88}{88}=1\] <p>Next, we cover up the second column:</p> \[\begin{vmatrix}6&amp;26\\14&amp;2\end{vmatrix}=2^2\begin{vmatrix}3&amp;13\\7&amp;1\end{vmatrix}=4(3-91)=-4(88)\] \[\implies x_2=\frac{-4(88)}{88}=-4\] <p>Done.</p> <h3 id="a-side-note-about-3x3s">A side note about 3x3s</h3> <p>Certainly there are cases where Cramer’s rule can be optimal for \(3\times3\) systems and larger, if the matrix and \(\mathbf{b}\) vector are particularly simple. But this is so rare, I am not going to bother creating a magical example.</p> <h2 id="a-defense-of-determinants">A Defense of Determinants</h2> <p>Finally, I am going to stay up on my soap box and tell y’all why determinants <em>can</em> be very useful.</p> <p>Mostly, this is focused on the \(2\times2\) and \(3\times3\) cases, which are the vast majority of problems you are expected to do by hand.</p> <ul> <li>They are very easy to compute when small, or at least not difficult to enter into a calculator</li> <li>There are methods to do them in your head</li> <li>They can tell you a lot for small matrices</li> <li>They provide the easiest method for \(3\times3\) inverses</li> <li>They can give you an explicit formula for the solution to a system</li> </ul> <p>I will briefly detail my reasonings for these points.</p> <h3 id="computing-small-determinants">Computing small determinants</h3> <p>As we mentioned above, \(2\times2\) determinants are no problems at all. \(3\times3\)’s are similarly not too bad. If they cannot be done in one’s head, a few steps of row reduction can often bring it to that point. And plugging it into a calculator usually isn’t that bad.</p> <h3 id="information-for-small-matrices">Information for small matrices</h3> <p>Especially for \(3\times3\) matrices, the effort of computing the determinant can have big gains. Specifically, because it is often very difficult to see that a matrix has rank 2 by inspection. Rank 1 is obvious, because every column is a scalar multiple, but seeing that one of the rows is a linear combination of the other two isn’t so easy. Therefore, for the \(3\times3\) case, the determinant’s lack of sensitivity is not a problem. Usually, a zero determinant means that the rank is exactly two.</p> <h3 id="3x3-inverses">3x3 inverses</h3> <p>I <em>will</em> make a blog post about tricks for matrix inverses someday, I promise. And the adjugate matrix is generally the method of choice for \(3\times3\) matrices. You do <em>not</em> have to do that awful row reduction of a super augmented matrix.</p> <h3 id="giving-an-explicit-formula">Giving an explicit formula</h3> <p>Finally, I want to shout out Cramer’s rule for its applications. Variation of parameters for ordinary differential equations relies on Cramer’s rule to make the formula compact.</p> <p>If you only need a single variable from a system of equations, Cramer’s rule can also save you from having to row reduce the entire thing.</p> <p>Additionally, I have used it <a href="../functioninterp/" target="_blank">in my own personal research</a>.</p>]]></content><author><name>Taylor F.</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[(and determinants)]]></summary></entry><entry><title type="html">Change of Basis</title><link href="https://smashmath.github.io/blog/changeofbasis/" rel="alternate" type="text/html" title="Change of Basis"/><published>2022-10-06T00:00:00+00:00</published><updated>2022-10-06T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/changeofbasis</id><content type="html" xml:base="https://smashmath.github.io/blog/changeofbasis/"><![CDATA[<h3 id="recommended-viewing">Recommended viewing</h3> <p>Before you subject yourself to this post, I recommend watching the following two linear algebra videos from 3blue1brown’s “Essence of linear algebra” series. Grant Sanderson is the superior Grant when it comes to math explanations, and I think it’s entirely possible that watching these two videos will answer pretty much every question you may have.</p> <ul> <li><a href="https://youtu.be/kYB8IZa5AuE" target="_blank">Linear transformations and matrices</a></li> <li><a href="https://youtu.be/P2LTAUO1TdA" target="_blank">Change of basis</a></li> <li>I’d also recommend reading my post on <a href="../columnperspective/" target="_blank"><em>column perspective</em></a>. Understanding column perspective is going to make a lot of this much easier, but it’s not required.</li> </ul> <h3 id="preface">Preface</h3> <p>This topic is so infrequent that when it comes up in my tutoring practice, I have to rederive it for myself as I’m explaining it every single time. I recommend trying to focus on <em>understanding</em> where these formulas comes from, rather than memorizing what they are. In my opinion, if you intuitively understand what a change of basis matrix and a coordinate vector are, and how they interact, it all sort of comes together very naturally.</p> <p>Not to say change of basis is natural, absolutely not. This is possibly the most confounding topic I have had to consistently tutor. I just mean that once you speak the language of change of basis, then forming sentences is relatively intuitive.</p> <h2 id="notation">Notation</h2> <p>This is important to get out of the way because there are <em>so</em> many different notations for all this stuff. So let’s get it all straight right up front so the amount of confusion experienced later is (hopefully) minimized.</p> <p>To be more general, we’re going to talk about the coordinates being in \(F^n\) rather than \(\mathbb{R}^n\), like you will probably see in a lower division class. Just think \(\mathbb{R}^n\) if you have no idea what a field is.</p> <p>We will be denoting vectors as simply lowercase letters \(v\). Vectors are also commonly denoted \(\textbf{v}\) or \(\vec{v}\), but this is linear algebra and there are a ton of vectors and it’s a lot easier to type “v” than it is “\textbf{v}” or “\vec{v}” (and I am a lazy person). For a similar reason, we won’t be distinguishing between column vectors and row vectors. As far as we are concerned here,</p> \[(x,y)=\begin{pmatrix}x\\y\end{pmatrix}\] <p>The vectors in this post are lazy and generally prefer to lay down on their sides. They will stand up when they need to be multiplied by a matrix.</p> <h3 id="the-standard-basis">The standard basis</h3> <p>We denote the standard basis “\(\varepsilon\)”. In the case of \(F^n\), we say \(\varepsilon=\{e_1,\ldots,e_n\}\), where \(e_i\) is the \(i\)th column of the \(n\times n\) identity matrix (that is, zeros everywhere except for a \(1\) in the \(i\)th entry). For polynomial rings \(\varepsilon=\{1,x,\ldots,x^n\}\), and so on.</p> <h3 id="transformation-matrices-and-coordinate-vectors">Transformation matrices and coordinate vectors</h3> <ul> <li>The coordinate vector for a vector \(v\) with respect to a basis \(\beta\) is denoted \([v]_\beta\).</li> <li>The matrix for a linear transformation \(T:V\to V\) with respect to a basis \(\beta\) is denoted \([T]_\beta\)</li> <li>The matrix for a linear transformation \(T:V\to W\) with respect to bases \(\alpha\) of \(V\) and \(\beta\) of \(W\) is denoted \([T]_\alpha^\beta\)</li> <li>The matrix for a linear transformation \(T:V\to W\) with respect to <strong>standard</strong> bases \(\varepsilon_v\) of \(V\) and \(\varepsilon_w\) of \(W\) is denoted \([T]_\varepsilon^\varepsilon\)</li> </ul> <h3 id="change-of-basis-matrices">Change of basis matrices</h3> <p>For “the matrix which changes coordinates from a basis \(\alpha\) to a basis \(\beta\)”, there are a bunch of different notations. Here are a few:</p> \[[I]_\alpha^\beta,\quad P_{\alpha\to\beta},\quad P_{\beta\gets\alpha}\] <p>We will use \(P_{\beta\gets\alpha}\) for this post, because I like it best. To explain why, I am going to show a formula which will be explained later with all three different versions</p> \[\begin{array}{ccc} [I]_\alpha^\beta&amp;=&amp;[I]_\varepsilon^\beta[I]_\alpha^\varepsilon\\ P_{\alpha\to\beta}&amp;=&amp;P_{\varepsilon\to\beta} P_{\alpha\to\varepsilon}\\ P_{\beta\gets\alpha}&amp;=&amp;P_{\beta\gets\varepsilon} P_{\varepsilon\gets\alpha}\\ \end{array}\] <p>Personally, I like the third one because you can easily read it right to left (which is important because transformations are always applied right to left). The second one is jarring for me because it mixes right to left with left to right. And while the first one isn’t that bad, it’s a little jumpy and dense, in my opinion. Use whatever you like, but I will use the third.</p> <h2 id="how-coordinates-work">How coordinates work</h2> <h3 id="transformation-matrix-definition">Transformation matrix definition</h3> <p>A change of basis is just another linear transformation. So it is worth at least writing out the general formula for constructing a transformation matrix. <em>Rarely</em> it is easier to directly compute a change of basis matrix using the following formulation. However, it is slightly more common that computing \([T]_\alpha^\beta\) directly is faster, because the systems of equations that come up when finding it may have very obvious solutions (for example, if a transformation just scales or permutes vectors. This happens when using an eigenbasis). For this reason, <strong>it is often worth applying the transformation to the basis vectors of the domain and seeing if there is an obvious relationship between the outputs and the vectors in the basis of the codomain.</strong></p> <p>If \(T:V\to W\) is a linear transformation, and \(\alpha=(v_1,\ldots,v_n)\) is a basis for \(V\), then the matrix for the transformation \(T\) with respect to the bases \(\alpha\) of \(V\) and \(\beta\) of \(W\) is given by the \(\dim(W)\times n\) matrix</p> \[\begin{equation}\label{transformation} [T]_\alpha^\beta=\Big([T(v_1)]_\beta\quad\cdots\quad [T(v_n)]_\beta\Big) \end{equation}\] <h3 id="linear-transformation-example">Linear transformation example</h3> <p>Here is an example where you don’t actually need to do change of basis, really.</p> <p>Consider the transformation \(T:P_2(F)\to F^2\) defined by</p> \[T(a_0+a_1x+a_2x^2)=\begin{pmatrix}2a_1-a_2\\a_0+3a_1-3a_2\end{pmatrix}\] <p>Find the matrix for the transformation with respect to the bases \(\alpha=\{1+x+x^2,1+x^2,2+x+x^2\}\) and \(\beta=\left\{\begin{pmatrix}1\\1\end{pmatrix},\begin{pmatrix}1\\2\end{pmatrix}\right\}\).</p> <p>First, we can apply \(T(\alpha)\) just to see what happens. You can either directly plug in each basis vector into the transformation, or you could find the matrix for the transformation with respect to the standard bases and apply that to the coordinate vectors. We will do the latter as practice:</p> \[T(1)=\begin{pmatrix}0\\1\end{pmatrix},\quad T(x)=\begin{pmatrix}2\\3\end{pmatrix},\quad T(x^2)=\begin{pmatrix}-1\\-3\end{pmatrix}\] <p>Thus, \([T]_\varepsilon^\varepsilon=\begin{pmatrix}0&amp;2&amp;-1\\1&amp;3&amp;-3\end{pmatrix}\)</p> <p>Applying that matrix to the coordinate vectors of \(\alpha\) with respect to the standard basis (\(P_{\varepsilon\gets\alpha}\))</p> \[\begin{pmatrix}0&amp;2&amp;-1\\1&amp;3&amp;-3\end{pmatrix}\begin{pmatrix}1&amp;1&amp;2\\1&amp;0&amp;1\\1&amp;1&amp;1\end{pmatrix}=\begin{pmatrix}1&amp;-1&amp;1\\1&amp;-2&amp;2\end{pmatrix}\] <p>And as it so happens, the columns are just scalar multiples of our \(\beta\) basis vectors (wow it’s almost like this example was designed for that to happen). We can see that</p> \[T(\alpha_1)=1\beta_1,\quad T(\alpha_2)=-1\beta_2,\quad T(\alpha_3)=1\beta_2\] <p>Therefore, the matrix \([T]_\alpha^\beta\) is just</p> \[[T]_\alpha^\beta=\begin{pmatrix}1&amp;0&amp;0\\0&amp;-1&amp;1\end{pmatrix}\] <p>I encourage you to try this example with the tools later outlined in this post. It isn’t <em>that</em> bad, but the purpose of this example is to let you know that there are often simpler ways to go about these problems. You shouldn’t always bust out the formula just because you <em>can</em>.</p> <h3 id="change-of-basis-matrix-properties">Change of basis matrix properties</h3> <p>Just to get this out of the way because it’s quick, change of basis matrices are basically designed to have the following property</p> \[\begin{equation}\label{changebasis} P_{\beta\gets\alpha}[v]_\alpha=[v]_\beta \end{equation}\] <p>This is another reason I like the right to left notation. The \(\alpha\)s are on the same side and seem to cancel out.</p> <p>We also have a property of when you do successive changes of basis, it’s all just one big change of basis. That is, going from \(\alpha\) to \(\beta\) and then from \(\beta\) to \(\gamma\) is the same as just going from \(\alpha\) to \(\gamma\).</p> \[\begin{equation}\label{basiscombine} P_{\gamma\gets\beta}P_{\beta\gets\alpha}=P_{\gamma\gets\alpha} \end{equation}\] <p>One more important property that follows directly from \eqref{changebasis} (by multiplying the inverse matrix to both sides) is</p> \[\begin{equation}\label{basisinverse} P_{\alpha\gets\beta}=(P_{\beta\gets\alpha})^{-1} \end{equation}\] <p>That is, to reverse the change of basis, you just invert the matrix.</p> <h3 id="wait-what-is-a-coordinate-vector">Wait what IS a coordinate vector?</h3> <p>Given a vector \(v=(1,1)\) and the basis \(\beta=\{(1,-1),(-1,2)\}\), the coordinate vector of \(v\) with respect to the basis \(\beta\) is</p> \[[v]_\beta=(3,2)\] <p>But what does that <em>mean</em> and how could we find that?</p> <p>The idea is that if we have a basis \(\alpha=\{v_1,\ldots,v_n\}\), and \([x]_\alpha=(c_1,\ldots,c_n)\), then that is defined to mean</p> \[\begin{equation}\label{coordinate} x=c_1v_1+\ldots+c_nv_n \end{equation}\] <p>In words, \([x]_\alpha=(c_1,\ldots,c_n)\) tells us that “to get the vector \(x\), we need \(c_1\) of \(v_1\), \(c_2\) of \(v_2\), …, and \(c_n\) of \(v_n\)”.</p> <p>For the example above, we can observe that it’s true that</p> \[\begin{pmatrix}1\\1\end{pmatrix}= 3\begin{pmatrix}1\\-1\end{pmatrix} +2\begin{pmatrix}-1\\2\end{pmatrix}\] <p>The question becomes, then, “how could we find the numbers \(3\) and \(2\)? Don’t worry, we will answer that <a href="#computing-coordinate-vectors" target="_blank">in time</a>.</p> <p>One observation to make is that for all \(v\in F^n\), \(v=[v]_\varepsilon\). To show an example in \(F^2\), where \(\varepsilon=\left\{\begin{pmatrix}1\\0\end{pmatrix},\begin{pmatrix}0\\1\end{pmatrix}\right\}\).</p> \[\begin{pmatrix}x\\y\end{pmatrix}= x\begin{pmatrix}1\\0\end{pmatrix} +y\begin{pmatrix}0\\1\end{pmatrix}\] <p>Which is to say</p> \[\left[\begin{pmatrix}x\\y\end{pmatrix}\right]_\varepsilon= \begin{pmatrix}x\\y\end{pmatrix}\] <p>In words: “every vector in \(F^n\) is its own coordinate vector with respect to the standard basis”. In a sense, that is <em>why</em> it’s the standard basis.</p> <p>This gives us a method to compute coordinate vectors</p> \[[x]_\beta=(P_{\varepsilon\gets\beta})^{-1}x\] <p>We will see in the next section that finding the matrix \(P_{\varepsilon\gets\beta}\) is actually very easy.</p> <h3 id="coordinate-vectors-with-matrices">Coordinate vectors with matrices</h3> <p>Now, if you’re comfortable with <a href="../columnperspective/" target="_blank"><em>column perspective</em></a>, you may have seen \eqref{coordinate} and thought, “that looks like matrix multiplication!” If so, yes, it is! If you have no idea what I’m talking about, basically</p> \[c_1v_1+\ldots+c_nv_n= \Bigg(v_1\quad \cdots\quad v_n\Bigg) \begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}\] <p>For a more concrete example, take the previous example of \(v=(1,1)\) and the basis \(\beta=\{(1,-1),(-1,2)\}\), for which \([v]_\beta=(3,2)\).</p> \[3\begin{pmatrix}1\\-1\end{pmatrix} +2\begin{pmatrix}-1\\2\end{pmatrix}= \begin{pmatrix}1&amp;-1\\-1&amp;2\end{pmatrix}\begin{pmatrix}3\\2\end{pmatrix}= \begin{pmatrix}1\\1\end{pmatrix}\] <p>It appears that \(\begin{pmatrix}1&amp;-1\\-1&amp;2\end{pmatrix}\) is the matrix for which</p> \[\begin{pmatrix}1&amp;-1\\-1&amp;2\end{pmatrix}[v]_\beta=[v]_\varepsilon\] <p>And it is! \(\begin{pmatrix}1&amp;-1\\-1&amp;2\end{pmatrix}\) <em>is</em> the change of basis matrix \(P_{\varepsilon\gets\beta}\).</p> <p><strong>For a given basis \(\beta=\{v_1,\ldots,v_n\}\), the change of basis matrix \(P_{\varepsilon\gets\beta}\) is constructed by just sticking the vectors into the columns.</strong></p> <p>So, for example, if the basis is \(B=\{(1,1,1),(1,-1,0),(1,1,-2)\}\), then the change of basis matrix \(P_{\varepsilon\gets B}\) is just</p> \[\begin{pmatrix}1&amp;1&amp;1\\1&amp;-1&amp;1\\1&amp;0&amp;-2\end{pmatrix}\] <h3 id="computing-coordinate-vectors">Computing coordinate vectors</h3> <p>A few sections ago, we stated the following formula for a coordinate vector</p> \[\begin{equation}\label{coordinatecompute} [x]_\beta=(P_{\varepsilon\gets\beta})^{-1}x \end{equation}\] <p>Now that we know how to get \(P_{\varepsilon\gets\beta}\) (put the vectors as the columns), we can say more directly that the coordinate vector for a basis \(\beta=\{v_1,\ldots,v_n\}\) can be computed by</p> \[\begin{equation}\label{coordinatecomputetwo} [x]_\beta=\bigg(v_1\quad \cdots\quad v_n\bigg)^{-1}x \end{equation}\] <p>Column perspective makes this formula very intuitive, as the definition of a coordinate vector is essentially to be the solution to the equation</p> \[c_1v_1+\ldots+c_nv_n=x\] <p>which can be expressed as the matrix equation</p> \[\Bigg(v_1\quad \cdots\quad v_n\Bigg)\begin{pmatrix}c_1\\\vdots\\c_n\end{pmatrix}=x\] <h3 id="coordinate-vector-example">Coordinate vector example</h3> <p>Let’s calculate the coordinate vector from before: \(v=(1,1)\) with the basis \(\beta=\left\{\begin{pmatrix}1\\-1\end{pmatrix},\begin{pmatrix}-1\\2\end{pmatrix}\right\}\). We have that</p> \[P_{\varepsilon\gets\beta}=\begin{pmatrix}1&amp;-1\\-1&amp;2\end{pmatrix} \implies P_{\beta\gets\varepsilon}=\begin{pmatrix}2&amp;1\\1&amp;1\end{pmatrix}\] <p>using the formula of \(\begin{pmatrix}a&amp;b\\c&amp;d\end{pmatrix}^{-1}=\frac{1}{ad-bc}\begin{pmatrix}d&amp;-b\\-c&amp;a\end{pmatrix}\). Thus,</p> \[[v]_\beta=P_{\beta\gets\varepsilon}v=\begin{pmatrix}2&amp;1\\1&amp;1\end{pmatrix}\begin{pmatrix}1\\1\end{pmatrix}=\begin{pmatrix}3\\2\end{pmatrix}\] <h2 id="how-to-change-bases">How to change bases</h2> <p>We will be going into how to directly compute a general change of basis matrix <a href="#Finding-change-of-basis-matrices">in a later section</a>. This is a more practical formulation which works better for two and three dimensional vector spaces.</p> <p>First, as an analogy, let’s say you want to translate something from French into German, but you don’t speak either all that well. It would be a lot easier if you could translate the sentence into English first, and then translate that English sentence into German (assuming you are more comfortable with \(\varepsilon\)nglish). That is the fundamental idea behind the following corollary to \eqref{basiscombine} (which is <em>incredibly</em> useful).</p> \[\begin{equation}\label{ezchange} P_{\beta\gets\alpha}=P_{\beta\gets\varepsilon}P_{\varepsilon\gets\alpha}=(P_{\varepsilon\gets\beta})^{-1}P_{\varepsilon\gets\alpha} \end{equation}\] <p>Why are these formulas great, you may ask? Well, it’s because the matrix \(P_{\varepsilon\gets\alpha}\) is <em>really</em> easy to find (it’s just the vectors as the columns!), while \(P_{\beta\gets\alpha}\) is <em>really</em> computationally intensive to find directly if neither \(\alpha\) or \(\beta\) are the standard basis.</p> <h3 id="change-of-basis-example">Change of basis example</h3> <p>Let’s find the matrix, for example, which changes from the basis \(\alpha=\left\{\begin{pmatrix}2\\1\end{pmatrix},\begin{pmatrix}3\\2\end{pmatrix}\right\}\) to the basis \(\beta=\left\{\begin{pmatrix}1\\1\end{pmatrix},\begin{pmatrix}1\\2\end{pmatrix}\right\}\).</p> <p>The matrix with the vectors as the columns will give us the matrix which changes from the basis to the standard basis.</p> \[P_{\varepsilon\gets\alpha}=\begin{pmatrix}2&amp;3\\1&amp;2\end{pmatrix}\] \[P_{\varepsilon\gets\beta}=\begin{pmatrix}1&amp;1\\1&amp;2\end{pmatrix}\implies P_{\beta\gets\varepsilon}=\begin{pmatrix}2&amp;-1\\-1&amp;1\end{pmatrix}\] <p>Using our formula of \(P_{\beta\gets\alpha}=P_{\beta\gets\varepsilon}P_{\varepsilon\gets\alpha}\)</p> \[P_{\beta\gets\alpha}= \begin{pmatrix}2&amp;-1\\-1&amp;1\end{pmatrix} \begin{pmatrix}2&amp;3\\1&amp;2\end{pmatrix}= \begin{pmatrix}3&amp;4\\-1&amp;-1\end{pmatrix}\] <p>How do we know we’re right, though? Basically, you just need to ensure that the columns of your resulting matrix \(\begin{pmatrix}3\\-1\end{pmatrix},\begin{pmatrix}4\\-1\end{pmatrix}\) are the correct coordinates of the old basis (\(\alpha\)) vectors in the new basis (\(\beta\)):</p> \[\begin{array}{cccccc} 3\begin{pmatrix}1\\1\end{pmatrix}&amp;-&amp;\begin{pmatrix}1\\2\end{pmatrix}&amp;=&amp;\begin{pmatrix}2\\1\end{pmatrix}\\ 4\begin{pmatrix}1\\1\end{pmatrix}&amp;-&amp;\begin{pmatrix}1\\2\end{pmatrix}&amp;=&amp;\begin{pmatrix}3\\2\end{pmatrix} \end{array}\] <p>And those are indeed the vectors of \(\alpha\)!</p> <p>Note that (by column perspective) this computation is equivalent to multiplying \(P_{\varepsilon\gets\beta}P_{\beta\gets\alpha}\) and making sure you get \(P_{\varepsilon\gets\alpha}\).</p> <h2 id="transformations-on-other-bases">Transformations on other bases</h2> <p>One of the most common applications of change of basis is to find the matrix of a transformation with respect to other bases.</p> <h3 id="linear-operators">Linear operators</h3> <p>One of the most common cases is that you have a matrix for a linear operator \(T:V\to V\) with respect to some known basis \(\alpha\) of \(V\) (usually the standard basis \(\alpha=\varepsilon\)), and want to find the matrix with respect to another basis of \(V\), \(\beta\). The formula is as follows:</p> \[\begin{equation} [T]_\beta=P_{\beta\gets\alpha}[T]_\alpha P_{\alpha\gets\beta} \end{equation}\] <p>The basic idea is this: suppose you cannot speak French, so you are using a translator to speak to your eight-year-old nephew who only speaks French. In the conversation, your nephew says something in French, which the translator has to convey to you in English (\(P{\text{Eng}\gets\text{Fr}}\)). Then, you can respond in English \([T]{\text{Eng}}\). But, for your nephew to understand what you said, the translator must translate it back into French (\(P{\text{Fr}\gets\text{Eng}}\)). Giving the full “conversation” as</p> \[[T]_{\text{Fr}}=P_{\text{Fr}\gets\text{Eng}}[T]_{\text{Eng}}P_{\text{Eng}\gets\text{Fr}}\] <h3 id="linear-operator-example">Linear operator example</h3> <p>Take the transformation which is a reflection about the line \(y=x\). That is, \(T:F^2\to F^2,\quad T(x,y)=(y,x)\). From the definition of \(T(x,y)=(y,x)\), you can verify very easily that the matrix for this transformation on the standard basis is</p> \[[T]_\varepsilon=\begin{pmatrix}0&amp;1\\1&amp;0\end{pmatrix}\] <p>Let’s say that I want the matrix for the transformation on the basis \(\beta=\left\{\begin{pmatrix}1\\1\end{pmatrix},\begin{pmatrix}1\\2\end{pmatrix}\right\}\) (from before). We previously computed the change of basis matrices between \(\beta\) and the standard basis to be</p> \[P_{\varepsilon\gets\beta}=\begin{pmatrix}1&amp;1\\1&amp;2\end{pmatrix},\quad P_{\beta\gets\varepsilon}=\begin{pmatrix}2&amp;-1\\-1&amp;1\end{pmatrix}\] <p>This tells us that the transformation matrix with respect to \(\beta\), \([T]_\beta\), should be</p> \[[T]_\beta=P_{\beta\gets\varepsilon}[T]_\varepsilon P_{\varepsilon\gets\beta}\] \[[T]_\beta=\begin{pmatrix}2&amp;-1\\-1&amp;1\end{pmatrix}\begin{pmatrix}0&amp;1\\1&amp;0\end{pmatrix}\begin{pmatrix}1&amp;1\\1&amp;2\end{pmatrix}=\begin{pmatrix}1&amp;3\\0&amp;-1\end{pmatrix}\] <p>We can check that this is indeed the correct answer by transforming the basis vectors of \(\beta\).</p> \[\begin{array}{cccccc} T\begin{pmatrix}1\\1\end{pmatrix}&amp;=&amp;\begin{pmatrix}1\\1\end{pmatrix}&amp;=&amp;1\begin{pmatrix}1\\1\end{pmatrix}&amp;+&amp;0\begin{pmatrix}1\\2\end{pmatrix}\\ T\begin{pmatrix}1\\2\end{pmatrix}&amp;=&amp;\begin{pmatrix}2\\1\end{pmatrix}&amp;=&amp;3\begin{pmatrix}1\\1\end{pmatrix}&amp;-&amp;1\begin{pmatrix}1\\2\end{pmatrix} \end{array}\] <p>We can confirm that the coordinate vector for \(T(v_1)=\begin{pmatrix}1\\1\end{pmatrix}\) with respect to the basis \(\beta\) is \(\begin{pmatrix}1\\0\end{pmatrix}\), the first column of \([T]_\beta\), and similarly \([T(v_2)]_\beta=\begin{pmatrix}3\\-1\end{pmatrix}\), the second column of \([T]_\beta\).</p> <h3 id="v-to-w">V to W</h3> <p>If \(T:V\to W\) is a linear transformation to a different vector space \(W\), then you have to deal with two different bases for each space: let’s say \(\alpha,\alpha'\) for \(V\), and \(\beta,\beta'\) for \(W\).</p> <p>Here’s the thing… A formula for \([T]_{\alpha'}^{\beta'}\) given \([T]_\alpha^\beta\) is simply <em>not</em> worth memorizing, in my opinion. So, instead of just giving it to you, I’m going to explain the intuition behind what the formula <em>does</em>. Hopefully, that can give you the capability to come up with the formula on your own.</p> <p>We can transform \(\alpha\) vectors in terms of \(\beta\) vectors using \([T]_\alpha^\beta\), and we want to transform \(\alpha'\) vectors in terms of \(\beta'\) vectors. Thus, we could translate our input of \(\alpha'\) vectors to \(\alpha\) vectors first (\(P_{\alpha\gets\alpha'}\)), and apply the transformation which gives us the results in terms of \(\beta\) vectors (\([T]_\alpha^\beta\)). Finally, to get things in terms of \(\beta'\) vectors, we translate one last time from \(\beta\) to \(\beta'\) (\(P_{\beta'\gets\beta}\)). Putting it all together:</p> \[\begin{equation} [T]_{\alpha'}^{\beta'}=P_{\beta'\gets\beta}[T]_\alpha^\beta P_{\alpha\gets\alpha'} \end{equation}\] <p>Imagine you have a magical French to German translation machine, and you want to translate English to Spanish. Then you could put the magical machine between intermediary machines that take English to French and German to Spanish. That is, translate English to what your magical machine takes in (French), so that it can take the input. Then, the output of the magical machine will be German. So, we take the magical machine output of German, and translate it to Spanish, which is what we want. The resulting combination of the three machines is something that takes in English, and outputs something in Spanish.</p> \[[T]_\text{Eng}^\text{Spa}=P_{\text{Spa}\gets\text{Ger}}[T]_\text{Fr}^\text{Ger} P_{\text{Fr}\gets\text{Eng}}\] <h2 id="finding-change-of-basis-matrices">Finding change of basis matrices</h2> <p>While \eqref{ezchange} is great for two and three dimensional vector spaces (because \(2\times2\) and \(3\times3\) matrix inverses are relatively easy), if you have a larger vector space, it can be quite difficult to invert the matrix and multiply it all out. Instead, there is a direct process that uses row reduction of a super-augmented matrix instead.</p> <p>To find \(P_{\beta\gets\alpha}\),</p> \[\begin{eqnarray} \big[\begin{array}{c|c} P_{\varepsilon\gets\beta}&amp;P_{\varepsilon\gets\alpha} \end{array}\big] &amp;\quad\underrightarrow{\text{row reduce}}\quad&amp; \big[\begin{array}{c|c} I&amp;P_{\beta\gets\alpha} \end{array}\big]\\ \big[\begin{array}{c|c} \text{new basis}&amp;\text{old basis} \end{array}\big] &amp;\quad\underrightarrow{\text{row reduce}}\quad&amp; \big[\begin{array}{c|c} I&amp;P_{\text{new}\gets\text{old}} \end{array}\big] \end{eqnarray}\] <p>It’s similar to finding the inverse of a matrix, but instead we have a non-identity matrix on both sides.</p> <p>If you want to know why this works, there are two explanations. If you don’t, then idk the post is over you can leave now.</p> <ol> <li>You can get to this by solving \(n\) systems of equations at once: trying to find the coordinate vectors of the vectors in \(\beta\) with respect to the vectors in \(\alpha\)</li> <li>Using <a href="../columnperspective/" target="_blank"><em>column perspective</em></a>, you can see that if you multiply the matrix by \((P_{\varepsilon\gets\alpha})^{-1}\), you get \((P_{\varepsilon\gets\beta})^{-1}\big[\begin{array}{c|c} P_{\varepsilon\gets\beta}&amp;P_{\varepsilon\gets\alpha} \end{array}\big]= \big[\begin{array}{c|c} (P_{\varepsilon\gets\beta})^{-1}P_{\varepsilon\gets\beta}&amp;P_{\beta\gets\varepsilon}P_{\varepsilon\gets\alpha} \end{array}\big]= \big[\begin{array}{c|c} I&amp;P_{\alpha\gets\beta} \end{array}\big]\) and, if you didn’t know, row reducing an invertible matrix is exactly equivalent to multiplying on the left by the inverse.</li> </ol> \[A^{-1}\big[\begin{array}{c|c} A&amp;I \end{array}\big]= \big[\begin{array}{c|c} A^{-1}A&amp;A^{-1}I \end{array}\big]= \big[\begin{array}{c|c} I&amp;A^{-1} \end{array}\big]\] <p>(so that’s why that works if you were wondering)</p> <p><a href="https://youtu.be/GxPSApAHakg" target="_blank">hyperlink</a></p>]]></content><author><name>Taylor F.</name></author><category term="linear-algebra"/><summary type="html"><![CDATA[the most confusing thing in linear algebra (don't @ me)]]></summary></entry><entry><title type="html">Linear Constant Coefficient ODEs</title><link href="https://smashmath.github.io/blog/linconstcoef/" rel="alternate" type="text/html" title="Linear Constant Coefficient ODEs"/><published>2022-07-21T00:00:00+00:00</published><updated>2022-07-21T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/linconstcoef</id><content type="html" xml:base="https://smashmath.github.io/blog/linconstcoef/"><![CDATA[<p>Ah, “second order constant coefficient linear homogeneous ordinary differential equations”. What a wonderfully concise name for one of the most common types of problems in differential equations.</p> <p>This is, in my opinion, the most important topic in an intro to ODEs class. Probably the best thing about these kind of equations is that they are easily solvable, and can be thoroughly understood. The same cannot be said for most ODEs, even linear ones!</p> <p>We will start with second order homogeneous equations, and then briefly discuss how these topics generalize to higher order equations. Finally, I will leave some resources to discuss nonhomogeneous equations (or make another post on that later).</p> <h2 id="intro">Intro</h2> <p>First, we need to talk about exponentials, because they are <em>crucial</em>. Specifically, what I am going to call a “simple (complex) exponential”. A simple exponential is an exponential function like</p> \[y=Ce^{\lambda t}\] <p>Where \(\lambda\) is some complex number. That’s right! We are going to definitely need complex numbers for this topic. But let that be for later. For now, you can imagine \(\lambda\) being some real number like \(2\).</p> <p>For now, just observe that \(y'=\lambda Ce^{\lambda t}=\lambda y\). We can rearrange that to say that</p> \[y'-\lambda y=0\] <p>One way to articulate this (in linear algebra terms), is to say that simple exponentials are linearly dependent on their derivatives. That just means that we can scale them by numbers and add them up in a way that gives zero. For example,</p> \[\left(\frac{d^2}{dt^2}e^{2t}\right)-5\left(\frac{d}{dt}e^{2t}\right)+6e^{2t}=0\] <p>Because when we evaluate those derivatives we end up with</p> \[2^2e^{2t}-5(2e^{2t})+6e^{2t}=0\] <p>So, this means that \(e^{2t}\) is a solution to</p> \[y''-5y'+6y=0\] <p>How could we have found that? Well, I’ll tell you.</p> <h2 id="the-characteristic-polynomial">The characteristic polynomial</h2> <p>Take the example equation we had before</p> \[y''-5y'+6y=0\] <p>How might we solve this? Well, we already observed that exponentials are linearly dependent on their derivatives, so we might expect that our solution is a simple exponential. Then, we will use the idea behind most techniques in differential equations: “<em>the solution probably looks like this, so let’s plug it in and solve for the unknowns</em>”.</p> <p>That is, plug in \(y=e^{\lambda t}\) and see what happens! When we do so, we get</p> \[\lambda^2e^{\lambda t}-5\lambda e^{\lambda t}+6e^{\lambda t}=0\] <p>We can factor out the exponential to get</p> \[(\lambda^2-5\lambda+6)e^{\lambda t}=0\] <p>Now, one of the cool things about exponentials is how they are never zero. So, clearly, we need \(\lambda^2-5\lambda+6=0\). And this is just a quadratic equation! We call it the “characteristic polynomial”.</p> <p>In general, we get the characteristic polynomial from the differential equation like so</p> \[ay''+by'+cy=0 \implies a\lambda^2+b\lambda+c=0\] <p>Pretty easy, huh? Just take the coefficients directly and factor the quadratic. Eventually (assuming you can factor polynomials or complete the square in your head…), you can solve these problems without writing down anything.</p> <p>Back to our example,</p> \[\lambda^2-5\lambda+6=(\lambda-2)(\lambda-3)=0\] <p>Then it appears that we need \(\lambda=2\) or \(\lambda=3\). Meaning that it seems that two solutions are \(y_1=e^{2t}\) (like we saw before!) and \(y_2=e^{3t}\).</p> <p>We can check to see if the same thing that happened with \(e^{2t}\) happens with \(e^{3t}\).</p> \[\left(\frac{d^2}{dt^2}e^{3t}\right)-5\left(\frac{d}{dt}e^{3t}\right)+6e^{3t}=3^2e^{3t}-5(3e^{3t})+6e^{3t}=0\] <p>And it does! So we have two solutions, now. But are there more? Yes!</p> <p>Take, for example, \(y_3=e^{2t}+e^{3t}\). Plugging this in gives</p> \[\begin{multline} (2^2e^{2t}+3^2e^{3t})-5(2e^{2t}+3e^{3t})+6(e^{2t}+e^{3t})\\ =(2^2e^{2t}-5(2e^{2t})+6e^{2t}) +(3^2e^{3t}-5(3e^{3t})+6e^{3t})=0 \end{multline}\] <p>Notice exactly what happened here. We added up the solutions to get \(y_3\), but when we plugged it all in, we were able to just separate them, and they both went to zero like they did individually. We call this the Principle of Superposition!</p> <p>Now, you may choose to read my incoherent explanations of linear independence and superposition. You could read my explanation on why we need two solutions for second order equations. <strong>Or, you could just <a href="#real-roots">skip all of it</a> and get into how we solve these equations.</strong></p> <h2 id="superposition">Superposition</h2> <p>“Principle of Superposition” might sound scary and intimidating, but it’s just a result of two important properties of the derivative</p> \[(cf(t))'=cf'(t),\quad (f(t)+g(t))'=f'(t)+g'(t)\] <p>Basically, you can pull a constant outside of a derivative, and you can split up the derivative of a sum as the sum of derivatives. We can combine these facts to say</p> \[\begin{equation} (c_1f(t)+c_2g(t))'=c_1f'(t)+c_2g'(t) \end{equation}\] <p>Note: When we start adding and scaling stuff we call it a “linear combination”. So \(c_1f(t)+c_2g(t)\) is a “linear combination of \(f(t)\) and \(g(t)\)”.</p> <p>We can apply this to linear differential equations. For example, if we define the “differential operator” for the generic second order linear differential equation</p> \[L[y]=y''+p(t)y'+q(t)y\] <p>Then watch what happens when we evaluate \(L[c_1y_1+c_2y_2]\):</p> \[L[c_1y_1+c_2y_2]=(c_1y_1+c_2y_2)''+p(t)(c_1y_1+c_2y_2)'+q(t)(c_1y_1+c_2y_2)\] \[L[c_1y_1+c_2y_2]=c_1y_1''+c_2y_2''+p(t)(c_1y_1'+c_2y_2')+q(t)(c_1y_1+c_2y_2)\] <p>Now, if we separate the \(c_1\) and \(c_2\) terms,</p> \[L[c_1y_1+c_2y_2]=c_1(y_1''+p(t)y_1'+q(t)y_1)+c_2(y_2''+p(t)y_2'+q(t)y_2)\] \[L[c_1y_1+c_2y_2]=c_1L[y_1]+c_2L[y_2]\] <p>Nifty. We can use this to say that if \(L[y_1]=L[y_2]=0\) (that is to say, \(y_1\) and \(y_2\) are solutions to the differential equation), then</p> \[L[c_1y_1+c_2y_2]=c_1L[y_1]+c_2L[y_2]=c_10+c_20=0\] <p>Meaning that any arbitrary linear combination of solutions is also a solution.</p> <p>However, this may all look like mumbo jumbo to you. What you should get from this is the following:</p> <blockquote> <p>“ya scale a solution, ya get a solution. ya add solutions together, ya get a solution. ya do both at once, ya get a solution.”</p> </blockquote> <p>So, to get <em>every</em> solution, we take an arbitrary linear combination of our individual solutions. As long as you got all the individual solutions you were supposed to get (see next section), we call this the “general solution”.</p> <p>For our example of \(y''-5y'+6y=0\), this would mean that the general solution is</p> \[y=c_1e^{2t}+c_2e^{3t}\] <h2 id="linear-independence">Linear independence</h2> <p>I will admit, for the sake of simplicity, I was a little vague about how we get our general solution. You may, for example, say that \(y_1=e^{2t}\) is a solution, and \(y_2=2e^{2t}\) is also a solution. So can we not say that our general solution is \(y=c_1e^{2t}+c_22e^{2t}\)?</p> <p>No, we can’t. The reason should hopefully make sense: that doesn’t give us every solution! There is no constants \(c_1,c_2\) such that \(c_1e^{2t}+c_22e^{2t}=e^{3t}\). We need a \(y_1\) and \(y_2\) such that \(c_1y_1+c_2y_2\) can give us every possible solution.</p> <p>So how do we check if our choice of solutions is gucci? Well, we have a test for that! It’s called the Wronskian (do <strong>not</strong> look Wronskian up on Urban Dictionary).</p> \[\begin{equation} W[y_1,y_2](t)=\det\left(\begin{bmatrix}y_1(t)&amp;y_2(t)\\y_1'(t)&amp;y_2'(t)\end{bmatrix}\right) =y_1(t)y_2'(t)-y_1'(t)y_2(t) \end{equation}\] <blockquote> <p>If the Wronskian of two functions is nonzero at a point \(t_0\), then the functions are linearly independent at that point, and they form a fundamental set of solutions which can solve any initial value problem centered at \(t_0\).</p> </blockquote> <p><strong>One important result that you can verify with this is that \(y_1=e^{at}\) and \(y_2=e^{bt}\) are linearly independent if and only if \(a\neq b\).</strong> Meaning that we don’t have to worry and check if \(e^{2t}\) and \(e^{3t}\) are independent or not using the wronskian. They are independent because \(2\neq3\).</p> <p>How does this tell us anything? Where does it come from? <strong>If you couldn’t care less, then skip <a href="#how-many-solutions-to-expect">here</a></strong>! If you are genuinely curious, though, we can see it like this:</p> <p>Imagine you are trying to find the solution to the initial value problem with initial conditions \(y(t_0)=y_0, y'(t_0)=y'_0\). Given your two solutions \(y_1,y_2\), you would find the constants \(c_1,c_2\) such that \(y=c_1y_1+c_2y_2\) by solving the system</p> \[\begin{array}{ccccc} c_1y_1(t_0)&amp;+&amp;c_2y_2(t_0)&amp;=&amp;y_0\\ c_1y_1'(t_0)&amp;+&amp;c_2y_2'(t_0)&amp;=&amp;y'_0\\ \end{array}\] <p><a href="http://mathb.in/72311" target="_blank">If you would like to see a derivation that does not use linear algebra, please see here.</a> Otherwise, we’re just going to shift the burden of proof to linear algebra.</p> <h3 id="thank-you-linear-algebra">Thank you, linear algebra</h3> <p>The system of equations can be written as a matrix system</p> \[\begin{bmatrix}y_1(t_0)&amp;y_2(t_0)\\y_1'(t_0)&amp;y_2'(t_0)\end{bmatrix} \begin{bmatrix}c_1\\c_2\end{bmatrix}= \begin{bmatrix}y_0\\y'_0\end{bmatrix}\] <p>Let’s denote this matrix \(\mathbf{W}(t_0)=\begin{bmatrix}y_1(t_0)&amp;y_2(t_0)\\y_1'(t_0)&amp;y_2'(t_0)\end{bmatrix}\) and rewrite this system as</p> \[\mathbf{W}(t_0)\mathbf{c}=\mathbf{y}_0\] <p>The invertible matrix theorem tells us this system of equations has exactly one solution if and only if \(\det(\mathbf{W}(t_0))\neq0\). Therefore, we are <em>guaranteed</em> to have a unique solution to the initial value problem if \(\det(\mathbf{W}(t_0))=y_1(t_0)y_2'(t_0)-y_1'(t_0)y_2(t_0)=W[y_1,y_2](t_0)\neq0\).</p> <p>Emphasis on <em>guaranteed</em>. It’s possible to solve an initial value problem if you don’t have every solution. It’s just not <em>always</em> possible. For example, using the “not general solution” of \(y=c_1e^{2t}+c_22e^{2t}\), you <em>can</em> solve the initial value problem</p> \[y''-5y'+6y=0,\quad y(0)=2, y'(0)=4\] <p>The solution is just \(y=2e^{2t}\). But you can no longer solve it if we change it to</p> \[y''-5y'+6y=0,\quad y(0)=2, y'(0)=5\] <p>This is why \(y=c_1e^{2t}+c_22e^{2t}\) doesn’t work as a <em>general</em> solution. We need \(e^{3t}\) to solve this one.</p> <p>As a final note, when a determinant is nonzero, it means that the columns are linearly independent. This is why we say that the Wronskian tells us if a set of given functions are linearly independent or not.</p> <h2 id="how-many-solutions-to-expect">How many solutions to expect</h2> <p>I have not yet explained why only two solutions are sufficient for a second order equation. The short answer is that an \(n\)th order linear equation will have \(n\) linearly independent solutions. So, for second order, that means two. One way to think about it is “two derivatives means two arbitrary constants”. <strong>If that’s good enough for you, you can <a href="#repeated-roots">skip</a> this section</strong>. But if you’re like me and you want to know <em>why</em>, then continue.</p> <p>The answer can get pretty technical, so I will leave a relatively simple explanation that cites the all important <strong>Picard–Lindelöf existence and uniqueness theorem</strong>. We don’t have to worry at all about when solutions will exist or if the theorem will apply, because constant coefficients guarantees existence everywhere (since it trivially satisfies the requirements of the theorem).</p> <ol> <li>The existence and uniqueness theorem guarantees that a second order initial value problem with two conditions <strong>must</strong> have a <strong>unique</strong> solution.</li> <li>Since this must be true for any two arbitrary initial conditions, one function is not enough. However, two functions which are linearly independent will be able to satisfy any two arbitrary initial conditions (thanks linear algebra).</li> <li>If there was a third linearly independent solution \(y_3\) (i.e. it is not a linear combination of the first two), then that would contradict the uniqueness of the solution \(y=c_1y_1+c_2y_2\) which satisfies \(y(0)=y_3(0), y'(0)=y_3'(0)\).</li> </ol> <h1 id="real-roots">Real Roots</h1> <p>Okay! So, either you read through all or some of my convoluted attempts at explaining superposition, linear independence, and the fundamental assumptions underlying how we solve these problems, or you didn’t. If you didn’t, here is a tl;dr:</p> <blockquote> <p>second order means two different solutions. gotta have different power constants or it ain’t gucci.</p> </blockquote> <p>Either way, let’s get onto the different cases we encounter when solving the characteristic polynomial!</p> <h2 id="distinct-roots">Distinct roots</h2> <p>This is the case we were dealing with before. It’s the simplest and is the most straightforward. As mentioned back in <a href="#linear-independence">Linear independence</a>, if the power constants are different, then they are trivially linearly independent.</p> <p>In summary, if the roots to the characteristic polynomial are \(a\) and \(b\), then the general solution is</p> \[y=c_1e^{at}+c_2e^{bt}\] <h2 id="repeated-roots">Repeated roots</h2> <p>Consider the differential equation</p> \[y''+4y'+4y=0\] <p>Characteristic polynomial is \(\lambda^2+4\lambda+4=(\lambda+2)^2\). To get it to be zero we need \(\lambda=-2\), and there is no other solution for \(\lambda\). So we get the solution \(y_1=e^{-2t}\). But now we have run into a problem.</p> <p>Where is the second solution?</p> <p>In general, for a second order equation, to get a second solution when you only have one, you can use <strong>reduction of order</strong> to get another. But we’re not going to worry about that at all! Instead, I will impart the wisdom that reduction order bestows upon you when you apply to it to constant coefficient equations:</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/redoford.jpg" data-zoomable="true"/> </div> </div> \[\] <p>yeah so the second solution is just \(y_2=te^{-2t}\).</p> <p>In general, if you get a repeated root of \(a\), then the general solution is just</p> \[y=c_1e^{at}+c_2te^{at}\] <h2 id="complex-roots">Complex roots</h2> <p>Take, for example,</p> \[y''+4y'+13y=0\] <p>Characteristic polynomial is \(\lambda^2+4\lambda+13=0\). We can complete the square to get</p> \[(\lambda+2)^2+9=0\] <p>So, it appears we have no real solutions. If we solve for \(\lambda\) though, we get that \(\lambda=-2\pm3i\). Then, is the solution going to be this?</p> \[y=c_1e^{(-2+3i)t}+c_2e^{(-2-3i)t}\] <p>Well, yes and no. See, the mathematics community is a bit discriminatory against complex solutions. They’re not seen as being <em>real</em> solutions. This is especially true when the original problem involves exclusively real terms. So we’re going to have to make our solutions real before our professor gives us any credit.</p> <p>So what do we do? Well, as it turns out, we can just take the real and imaginary parts of either solution, and that will give us two linearly independent solutions!</p> <h3 id="real-and-imaginary-parts-of-solutions">Real and imaginary parts of solutions</h3> <p>Why is this true? Well, if the differential equation is all real, and you put in a complex solution, we can use linearity. Suppose that the solution is \(y(t)=u(t)+iv(t)\), where \(u(t)\) and \(v(t)\) are real functions. Then,</p> \[L[y]=L[u+iv]=L[u]+iL[v]\] <p>Under the assumption that \(L[y]=0\), we get that</p> \[L[u]+iL[v]=0\] <p>Since the differential equation is real, \(L[u]\) and \(L[v]\) are both real as well. And the only way that \(a+bi=0\) is if \(a=b=0\). So,</p> \[L[u]=0,\quad L[v]=0\] <p>Meaning they are two solutions individually. Note that this <em>also</em> means that \(u-iv\) is a solution as well.</p> <p>One thing that isn’t too difficult to prove with the Wronskian is that</p> \[\begin{equation} W[u+iv,u-iv](t)\neq0\iff W[u,v](t)\neq0 \end{equation}\] <p>What about our example? What are the real and imaginary parts of \(e^{(-2+3i)t}\)? For that, we need Euler’s formula!</p> <h3 id="eulers-formula">Eulers formula</h3> <p>If you are unfamiliar with what it means to raise \(e\) to a complex number, you can read <a href="../eulersformula/" target="_blank">this</a> (and ignore the circular logic that comes from that post mentioning linear constant coefficient differential equations). But the basic summary which you can take as fact is that</p> \[\begin{equation} e^{ix}=\cos(x)+i\sin(x) \end{equation}\] <p>Based on properties of exponentials, we can then say that</p> \[\begin{equation} e^{(a+bi)x}=e^{ax}\cos(bx)+ie^{ax}\sin(bx) \end{equation}\] <h3 id="the-final-solution">The final solution</h3> <p>Onto the final solution (to second order linear constant coefficient homogeneous ordinary differential equations)! If we take the real and imaginary parts of \(e^{(-2+3i)t}=e^{-2t}\cos(3t)+ie^{-2t}\sin(3t)\), we get</p> \[y=c_1e^{-2t}\cos(3t)+c_2e^{-2t}\sin(3t)\] <p>And, in general, if the roots of the characteristic polynomial are \(a\pm bi\), then the general solution is</p> \[y=c_1e^{at}\cos(bt)+c_2e^{at}\sin(bt)\] <h2 id="summary">Summary</h2> <p>In summary, given the differential equation</p> \[ay''+by'+cy=0\] <p>Where \(a,b,c\) are real (and \(a\neq0\)). If the roots of \(a\lambda^2+b\lambda+c=0\) are \(\lambda=\lambda_1,\lambda_2\) which are</p> <ol> <li>Real and distinct (\(\lambda_1\neq\lambda_2\)), then the solution is \(\begin{equation} y=c_1e^{\lambda_1t}+c_2e^{\lambda_2t} \end{equation}\)</li> <li>Real and identical (\(\lambda_1=\lambda_2\)), then the solution is \(\begin{equation} y=c_1e^{\lambda_1t}+c_2te^{\lambda_1t} \end{equation}\)</li> <li>Complex conjugates (\(\lambda=\alpha\pm i\beta\)), then the solution is \(\begin{equation} y=c_1e^{\alpha t}\cos(\beta t)+c_2e^{\alpha t}\sin(\beta t) \end{equation}\)</li> </ol> <h2 id="higher-order-equations">Higher order equations</h2> <p>What about third, fourth, or twentieth order equations? Actually, nothing changes. You just have more solutions. But the rules are exactly the same.</p> <p>You still look at the characteristic polynomial, and the roots tell you the solutions.</p> <h3 id="more-complex-roots">More complex roots</h3> <p>If you get complex roots, you still just get a \(\cos\) and a \(\sin\). For example, in</p> \[y^{(4)}-2y'''+2y''-10y'+25y=0\] <p>the roots are \(\lambda=-1\pm2i,2\pm i\). So our solutions will be</p> \[\begin{array}{cccc} \lambda=-1\pm2i&amp;\implies&amp; y_1=e^{-t}\cos(2t),&amp;y_2=e^{-t}\sin(2t)&amp;\\ \lambda=2\pm i&amp;\implies&amp; y_3=e^{2t}\cos(t),&amp;y_4=e^{2t}\sin(t)&amp;\\ \end{array}\] <p>The general solution is then</p> \[y=c_1e^{-t}\cos(2t)+c_2e^{-t}\sin(2t)+c_3e^{2t}\cos(t)+c_4e^{2t}\sin(t)\] <h3 id="more-repeated-roots">More repeated roots</h3> <p>If a root is repeated, just keep slapping \(t\) on there. For example, in</p> \[y'''+3y''+3y'+y=0\] <p>The only root of the characteristic polynomial is \(\lambda=-1\) (repeated three times). Thus, the general solution is simply</p> \[y=c_1e^{-t}+c_2te^{-t}+c_3t^2e^{-t}\] <p>It’s also possible to have repeated complex roots. In the equation</p> \[y^{(4)}+4y'''+14y''+20y'+25y=0\] <p>The roots are \(\lambda=-1\pm2i\), both repeated. So we still get solutions \(y_1=e^{-t}\cos(2t)\) and \(y_2=e^{-t}\sin(2t)\), but we also slap a \(t\) on there.</p> \[y=c_1e^{-t}\cos(2t)+c_2e^{-t}\sin(2t)+c_3te^{-t}\cos(2t)+c_4te^{-t}\sin(2t)\] <p>And, that’s it! Really, this topic is more about factoring polynomials and doing algebra more than it is differential equations (once you get past the derivation). After that, it’s just converting the roots into solutions from the formulas.</p>]]></content><author><name>Taylor F.</name></author><category term="differential-equations"/><summary type="html"><![CDATA[the most important topic in an ODE class]]></summary></entry><entry><title type="html">Integrating Factors Explained</title><link href="https://smashmath.github.io/blog/integratingfactor/" rel="alternate" type="text/html" title="Integrating Factors Explained"/><published>2022-07-18T00:00:00+00:00</published><updated>2022-07-18T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/integratingfactor</id><content type="html" xml:base="https://smashmath.github.io/blog/integratingfactor/"><![CDATA[<p>This is one of the first things taught in an intro to differential equations class, and I think it is very accessible to students who have taken basic calculus. The only snag might be that you may not know how to do the integral you inevitably obtain, but the overall idea for the process is straightforward enough, I think.</p> <p>As usual I kind of go overboard on the additional facts which you don’t really <em>need</em> to know. So just ignore them if you don’t care. I’ll put <strong>Aside</strong> in front of those kind of things.</p> <h2 id="what-is-a-first-order-linear-ode">What is a first order linear ODE?</h2> <p>A first order linear differential equation is one of the form</p> \[\begin{equation} y'+p(t)y=g(t) \end{equation}\] <p>Maybe you have a function on the \(y'\) term, but dividing it out gives the standard form above. An initial condition such as \(y(0)=3\) is also common, but that only needs to be worried about near the end of the solving process.</p> <p>We also won’t be concerning ourselves about where solutions to the equation (initial value problems) exist. That’s a bit too technical for the purposes of this post.</p> <p>We will also be using some compact prime notation. Basically</p> \[\begin{equation} \frac{d}{dt}(f(t)h(t))=(fh)' \end{equation}\] <h2 id="how-do-we-solve-them">How do we solve them?</h2> <p>Well, first, let’s review some basic calculus. Specifically, the product rule!</p> \[\begin{equation} (fh)'=fh'+f'h \end{equation}\] <p>The order doesn’t matter because it’s a sum. Now, observe what happens if we replace \(h\) with \(y\)</p> \[(fy)'=fy'+f'y\] <p>That looks sort of close to the left side of our original differential equation!</p> <p>So, the motivating idea is to find some function <a href="https://puu.sh/JbL0d/bbacfdb2ca.mp4" target="_blank">\(\mu(t)\)</a> which will make the left side of our equation a simple product rule. Because then we can integrate both sides and solve for \(y\).</p> <h2 id="finding-the-integrating-factor">Finding the integrating factor</h2> <p>Now, this is the idea behind almost every single technique you learn in a differential equations class: “<em>the solution probably looks like this, so let’s plug it in and solve for the unknowns</em>”.</p> <p>When we multiply our original equation by \(\mu\), we get</p> \[\mu y'+\mu p(t)y=\mu g(t)\] <p>And our goal, remember, is to get that left side to look like</p> \[\mu y'+p(t)\mu y=(fy)'\] <p>Well, if we expand it out, we get</p> \[\mu y'+p(t)\mu y=fy'+f'y\] <p>So, matching up the terms, we get that \(\mu=f\) and \(p(t)\mu=f'=\mu'\).</p> <p>Then, to solve for \(\mu\), we need to solve</p> \[\begin{equation} \mu'=p(t)\mu \end{equation}\] <p>Now, you can solve this differential equation in a lot of different ways. Usually as a separable equation. But instead of teaching you that technique which you may not know, I’m just going to show you the differentiation rule for exponentials:</p> <h3 id="a-detour-with-exponentials">A detour with exponentials</h3> \[\begin{equation} \left(e^{f(t)}\right)'=f'(t)e^{f(t)} \end{equation}\] <p>In words, the observation to make is that differentiating an exponential function is the same as multiplying it by some function. That is,</p> \[y'=f'(t)y\iff y=Ce^{f(t)}\] <p>Note that we can multiply by a constant since it doesn’t change anything about the differentiation.</p> <h2 id="solving-for-μ">Solving for μ</h2> <p>All this to say, we just need some function \(P(t)\) such that \(P'(t)=p(t)\). Basically, \(P(t)=\int p(t)dt\). Any one will do! <strong>Aside</strong>: This is because \(e^{P(t)+K}=Ce^{P(t)}\), and any scalar multiple of an integrating factor is also an integrating factor. So just pick one antiderivative and go with it.</p> <p>So, if we plug that into our differential equation,</p> \[e^{P(t)}y'+p(t)e^{P(t)}y=e^{P(t)}g(t)\] <p>We can verify that, under the assumption that \(P'(t)=p(t)\), the left side is indeed \(e^{P(t)}y'+p(t)e^{P(t)}y=\left(e^{P(t)}y\right)'\).</p> \[\left(e^{P(t)}y\right)'=e^{P(t)}g(t)\] <p>In summary, the basic equation for the integrating factor is</p> \[\begin{equation} \mu(t)=e^{\int p(t)dt} \end{equation}\] <h2 id="solving-the-equation">Solving the equation</h2> <p>Now, we can just integrate both sides (not forgetting our constant of integration).</p> \[e^{P(t)}y=C+\int e^{P(t)}g(t)dt\] <p>Finally, we can solve for \(y\) by dividing both sides by \(e^{P(t)}\) (or multiplying by \(e^{-P(t)}\)).</p> \[y=Ce^{-P(t)}+e^{-P(t)}\int e^{P(t)}g(t)dt\] <p><strong>Aside</strong>: One interesting observation is that we can express this solution more compactly in terms of \(\mu\).</p> \[y=\frac{C}{\mu}+\frac{\int \mu g}{\mu}\] <p>And if you’re in differential equations or basic calculus now, your professor may not like this more compact way of writing the integral without the d(whatever). But the textbook I used for my intro to analysis class (Understanding Analysis by Stephen Abbott) used it, so just tell your professor that you’re using Abbott (i.e. big brain advanced baller) notation (and then don’t blame me if your professor isn’t having that).</p> <h2 id="unnecessary-further-simplifications">Unnecessary further simplifications</h2> <p>The rest of the post is an <strong>aside</strong>. This is just gravy if you’re still interested in knowing more and going further with these ideas. The post is over; you can leave and go home now if you want.</p> <p>There are some other simplifications you can make which I personally like. They aren’t really super practical in terms of actually solving these equations, usually, but I think they’re cool!</p> <p>Personally, I like to use dummy variables for integration.</p> \[y=Ce^{-P(t)}+e^{-P(t)}\int_{t_0}^t e^{P(s)}g(s)ds\] <p>We can also use another dummy variable \(P(t)\):</p> \[P(t)=\int_{t_0}^tp(r)dr\] <p>Also using that \(P(s)=\int_{t_0}^sp(r)dr\). Changing our solution to</p> \[y=Ce^{-\int_{t_0}^tp(r)dr}+e^{-\int_{t_0}^tp(r)dr}\int_{t_0}^t e^{\int_{t_0}^sp(r)dr}g(s)ds\] <p>This has the added benefit of making it so that we can replace \(C\) with \(y(t_0)\). This is because plugging in \(t=t_0\) makes all the integrals zero, leaving us with just \(C\). This eliminates the need to solve for \(C\) in an initial value problem. Just be careful that the integrals do not diverge when integrating from \(t_0\). If they do, choose any other arbitrary starting bound that gives a convergent (gucci) integral.</p> \[y=y(t_0)e^{-\int_{t_0}^tp(r)dr}+e^{-\int_{t_0}^tp(r)dr}\int_{t_0}^t e^{\int_{t_0}^sp(r)dr}g(s)ds\] <p>We may also distribute the exponential into the integral now, because we are using a dummy variable for the integration. Note that in the exponent we will get</p> \[\int_{t_0}^sp(r)dr-\int_{t_0}^tp(r)dr\] <p>Which we can simplify to \(\int_t^{t_0}p(r)dr+\int_{t_0}^sp(r)dr=\int_{t}^sp(r)dr\) using our rules for integration.</p> \[y=y(t_0)e^{-\int_{t_0}^tp(r)dr}+\int_{t_0}^t e^{\int_{t}^sp(r)dr}g(s)ds\] <p>Another personal preference is to use \(\exp(x)\) instead of \(e^x\) when the exponent is big and messy (such as when we stick a whole integral up in there).</p> \[y=y(t_0)\exp\left(-\int_{t_0}^tp(r)dr\right)+\int_{t_0}^t\exp\left(\int_{t}^sp(r)dr\right)g(s)ds\] <h2 id="further-notes">Further notes</h2> <p>Here’s some more information on how this concept generalizes to second order equations, if you’re interested.</p> <p>Did you know every linear second order differential equation also has an integrating factor? The reason nobody ever uses this, though, is that, usually, finding it is just as hard as solving the equation normally.</p> <p>For example, trying to use an integrating factor to solve</p> \[y''+y=0\] <p>ends up giving you the following differential equation for the integrating factor \(\mu\):</p> \[\mu''+\mu=0\] <p>Which means that the integrating factor for the differential equation must be a solution to the differential equation. Making this method absolutely useless for this problem.</p> <p>If you want to know how the method works, though, basically the idea is to find a function \(\mu\) and \(f\) such that when you multiply \(P(t)y''+Q(t)y'+R(t)y\) by \(\mu\), you get</p> \[P(t)\mu y''+Q(t)\mu y'+R(t)\mu y=(P(t)\mu y'+fy)'\] <p>Setting equal the coefficients gives you</p> \[f'=R\mu,\quad P\mu'+P'\mu+f=Q\mu\] <p>If you differentiate the right equation and substitute in \(R\mu\) for \(f'\), you get</p> \[P\mu''+(2P'-Q)\mu'+(P''-Q'+R)\mu=0\] <p>This is also called the Adjoint Equation, and it’s usually not much nicer than the original. If \(P''-Q'+R=0\), though, then the adjoint equation is first order (and linear) in \(\mu'\), allowing you to solve it either with another integrating factor or as a separable equation. When this happens, the original equation is called “exact” (yes, this is the second order version of exact equations!).</p>]]></content><author><name>Taylor F.</name></author><category term="differential-equations"/><summary type="html"><![CDATA[the most common way to solve first order linear ODE's]]></summary></entry><entry><title type="html">Bases for the fundamental spaces of a matrix</title><link href="https://smashmath.github.io/blog/rowcolspace/" rel="alternate" type="text/html" title="Bases for the fundamental spaces of a matrix"/><published>2022-06-11T00:00:00+00:00</published><updated>2022-06-11T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/rowcolspace</id><content type="html" xml:base="https://smashmath.github.io/blog/rowcolspace/"><![CDATA[<p>In case you need a refresher on what things like pivots and the column space are. Otherwise, you can just skip to <a href="#row-reduction">Row Reduction</a>.</p> <h2 id="preliminary-vocab-and-notation">Preliminary vocab and notation</h2> <p>As a note on notation, we typically say \(A\) is an \(m\times n\) matrix which we are looking at. And we will call its reduced row echelon form (or rref for short) \(R\).</p> <p>The matrix we will use as an example will be the following</p> \[A=\begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix}\] <p>And its rref</p> \[R=\begin{pmatrix} 1&amp;2&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;1&amp;1&amp;0\\ 0&amp;0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0&amp;0 \end{pmatrix}\] <p>In the discussion of this topic, it often helps to assign variables to each column, as if we were solving some arbitrary system \(A\textbf{x}=\textbf{b}\). To explicitly define our variables,</p> \[\begin{equation} \label{aeq} \begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix}\begin{pmatrix} x_1\\x_2\\x_3\\x_4\\x_5 \end{pmatrix}=\textbf{b} \end{equation}\] <p>Which is row reduced to</p> \[\begin{equation} \label{req} \begin{pmatrix} 1&amp;2&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;1&amp;1&amp;0\\ 0&amp;0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0&amp;0 \end{pmatrix}\begin{pmatrix} x_1\\x_2\\x_3\\x_4\\x_5 \end{pmatrix}=\textbf{b}' \end{equation}\] <p>When we refer to “variables” in “the system”, we will be referring to \(x_1,\ldots,x_5\) in the above.</p> <p>Other important vocab includes</p> <ul> <li><strong>Pivot</strong>: A pivot is a leading one (the first nonzero entry of each nonzero row) in the rref. The pivots of \(R\) are in the first, third, and fifth column.</li> <li><strong>Pivot column</strong>: A column with a pivot. In rref, pivot columns are just columns of the identity matrix. The pivot columns of \(R\) are then the first, third, and fifth columns. Pivot columns are also linearly independent.</li> <li><strong>Nonpivot column</strong>: A column that doesn’t have a pivot.</li> <li><strong>Leading variable</strong>: The variables corresponding to pivot columns. They can be solved for in terms of free variables. So for the system \eqref{req}, the leading variables are \(x_1,x_3,x_5\).</li> <li><strong>Free variable</strong>: A variable corresponding to nonpivot columns. For the system mentioned above, they would be \(x_2,x_4\).</li> <li><strong>Rank</strong>: The dimension of the column and row space. Also the number of pivot columns.</li> <li><strong>Nullity</strong>: The dimension of the nullspace. Also the number of nonpivot columns.</li> </ul> <p>Next, the four fundamental spaces of a matrix \(A\):</p> <ul> <li><strong>Column Space</strong>: The range of the columns. Also every vector \(\textbf{b}\) such that \(A\textbf{x}=\textbf{b}\) is consistent.</li> <li><strong>Nullspace</strong>: Every vector \(\textbf{n}\) such that \(A\textbf{n}=\textbf{0}\). Also called the kernel of \(A\).</li> <li><strong>Row Space</strong>: The range of the rows. Also the column space of \(A^T\).</li> <li><strong>Left Nullspace</strong>: Every vector \(\textbf{n}\) such that \(\textbf{n}^TA=\textbf{0}^T\). Also the kernel of \(A^T\).</li> </ul> <h3 id="dimensions-of-the-spaces">Dimensions of the spaces</h3> <p>Finally, a quick summary on the dimensions of the four spaces of an \(m\times n\) matrix \(A\) with \(\operatorname{rank}(A)=r\) using the rank-nullity theorem:</p> <ul> <li>\(\dim(\operatorname{col}(A))=\dim(\operatorname{row}(A))=\operatorname{rank}(A)=r=\) the number of pivot columns.</li> <li>\(\dim(\operatorname{null}(A))=\operatorname{nullity}(A)=n-r=\) the number of nonpivot columns.</li> <li>\(\dim\left(\operatorname{null}\left(A^T\right)\right)=\operatorname{nullity}(A^T)=m-r\).</li> </ul> <p>As a brief aside, I personally tend to go for a different kind of row echelon form other than rref. Because I hate fractions in my matrix, I don’t force “leading ones”. I do, however, try to keep pivot columns having the pivot be the only nonzero entry. That way, leading variables are still easy to solve for, and you avoid fractions.</p> <p>To illustrate this point, basically I prefer \(\begin{pmatrix} 2&amp;0&amp;1\\ 0&amp;3&amp;-1\\ \end{pmatrix}\) to \(\begin{pmatrix} 1&amp;0&amp;\frac{1}{2}\\ 0&amp;1&amp;-\frac{1}{3}\\ \end{pmatrix}\)</p> <h2 id="row-reduction">Row reduction</h2> <p>First, It’s important to make clear what row reduction <em>does</em> and <em>does not</em> change about a matrix.</p> <ul> <li>Row reduction does <em>not</em> change the row space or the null space.</li> <li>Row reduction <em>usually</em> changes the column space</li> </ul> <p>Let’s explain why the row and null spaces are unaffected real quick.</p> <p>The row space is not changed because the rows of \(R\) are just linear combinations of the rows of \(A\). Thus, the rows of \(R\) are in the span of the rows of \(A\), leaving them contained in the row space. Since row operations are reversible (invertible), we don’t lose any information about the row space either.</p> <p>The reason the null space is not affected is a bit more complicated. The short answer is that column dependencies remain the same in both \(A\) and \(R\) (we will discuss this at length further on). The slightly longer and possibly more intuitive answer is that if \(R\) is the rref of \(A\), then there is an invertible matrix \(P\) such that \(PA=R\). It follows that if \(A\textbf{n}=\textbf{0}\), then \(R\textbf{n}=PA\textbf{n}=P\textbf{0}=\textbf{0}\).</p> <p>Hopefully, it is obvious that the column space is usually completely different. Most of the columns of \(A\) have a nonzero fourth entry, but all of the columns of \(R\) have a zero fourth entry. So \((1,1,1,1)\), the first column of \(A\) which is by definition in the column space, cannot be in the column space of \(R\). Thus, the column spaces are not always the same.</p> <h2 id="row-space">Row Space</h2> <p>So, this is actually pretty easy. A basis for the row space of \(A\) is just the nonzero rows of \(R\). As mentioned before, they are in the span of the rows of \(A\). And they are also obviously linearly independent.</p> <p>From our \(R=\begin{pmatrix} 1&amp;2&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;1&amp;1&amp;0\\ 0&amp;0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0&amp;0 \end{pmatrix}\) matrix, the nonzero rows are</p> \[(1,2,0,-1,0),\quad (0,0,1,1,0),\quad (0,0,0,0,1)\] <p>The first vector is the only one with a nonzero first entry, so it’s definitely linearly independent from the others. The second is the only one with a nonzero third entry, so it’s definitely linearly independent from the others. And the third is the only one with a nonzero fifth entry, so (etc.)</p> <p>Our Row Space basis is then</p> \[\operatorname{row}(A)=\operatorname{span}\left\{(1,2,0,-1,0),(0,0,1,1,0),(0,0,0,0,1)\right\}\] <p>Those who remember that you have to look at the columns of the original matrix for the column space may be tempted to say that the first three rows of \(A\) must be a basis for the row space as well. But that would be wrong here! Most of the time you are okay doing that, but in this case, the first two rows of \(A\) are identical, and thus linearly dependent. They cannot be a basis, then.</p> <h2 id="column-space">Column Space</h2> <p>Before, I mentioned that “column dependencies remain the same in both \(A\) and \(R\)”. Let’s go over exactly what I mean by that.</p> <p>Look at \(R\):</p> \[R=\begin{pmatrix} 1&amp;2&amp;0&amp;-1&amp;0\\ 0&amp;0&amp;1&amp;1&amp;0\\ 0&amp;0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0&amp;0 \end{pmatrix}\] <p>The columns are very simple, so it’s very easy to see the column dependencies of the nonpivot columns in terms of the pivot columns.</p> <p>The second column, \((2,0,0,0)\) is twice the first column (col2=2col1)</p> \[(2,0,0,0)=2(1,0,0,0)\] <p>And the fourth column \((-1,1,0,0)\) is the difference of the first and third columns (col4=col3-col1)</p> \[(-1,1,0,0)=(0,1,0,0)-(1,0,0,0)\] <p>Now, this blew my mind when I first saw it. But the same relationships are true for \(A\) as well!</p> \[\begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix}\] <p>(col2=2col1)</p> \[(2,2,2,2)=2(1,1,1,1)\] <p>(col4=col3-col1)</p> \[(0,0,2,0)=(1,1,3,1)-(1,1,1,1)\] <p>This isn’t just coincidence either! This is always true of a matrix and its rref. In other words, <em>the rref tells us the relationships between the columns</em>.</p> <p>So, we know that column two is linearly dependent on column one, and column four is linearly dependent on columns one and three. Thus, we exclude them from our basis.</p> <p><strong>This is why we take the pivot columns of the original matrix!</strong> It’s because the rref tells us they are <em>linearly independent</em>.</p> <p>Then our pivot columns are columns one, three, and five. So a basis for the column space is</p> \[\operatorname{col}(A)=\operatorname{span}\left\{ \begin{pmatrix} 1\\1\\1\\1 \end{pmatrix}, \begin{pmatrix} 1\\1\\3\\1 \end{pmatrix}, \begin{pmatrix} 1\\1\\3\\2 \end{pmatrix} \right\}\] <p>Note: the nonpivot columns of \(R\) are the coordinate vectors of the nonpivot columns of \(A\) with respect to this pivot column basis!</p> <h2 id="nullspace">Nullspace</h2> <p>So, we actually already did all the work we needed to do in the previous section!</p> <p>We found the following column dependencies:</p> <p>(col2=2col1 \(\implies\) 2col1-col2=0) and (col4=col3-col1 \(\implies\) col1-col3+col4=0)</p> <p>This actually gives us a basis for the null space, using <a href="../columnperspective/" target="_blank">column perspective</a>.</p> <p>If we take \(2\) of column one, and \(-1\) of column two, we get zero. So \((2,-1,0,0,0)\) is in the null space.</p> \[\begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix} \begin{pmatrix} 2\\-1\\0\\0\\0 \end{pmatrix}= 2\begin{pmatrix} 1\\1\\1\\1 \end{pmatrix}- \begin{pmatrix} 2\\2\\2\\2 \end{pmatrix}= \begin{pmatrix} 0\\0\\0\\0 \end{pmatrix}\] <p>The other relationship tell us that \(1\) of column one, \(-1\) of column three, and \(1\) of column four will also give us zero. Putting \((1,0,-1,1,0)\) in the null space as well.</p> \[\begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix} \begin{pmatrix} 1\\0\\-1\\1\\0 \end{pmatrix}= \begin{pmatrix} 1\\1\\1\\1 \end{pmatrix}- \begin{pmatrix} 1\\1\\3\\1 \end{pmatrix}+ \begin{pmatrix} 0\\0\\2\\0 \end{pmatrix}= \begin{pmatrix} 0\\0\\0\\0 \end{pmatrix}\] <p>\((2,-1,0,0,0)\) and \((1,0,-1,1,0)\) are pretty clearly independent, and since we have two nonpivot columns, we know the dimension of the null space is two. Thus, we have a proper basis for the null space!</p> \[\operatorname{null}(A)=\operatorname{span}\left\{ \begin{pmatrix} 2\\-1\\0\\0\\0 \end{pmatrix}, \begin{pmatrix} 1\\0\\-1\\1\\0 \end{pmatrix} \right\}\] <h2 id="left-nullspace">Left Nullspace</h2> <p><strong>PRO TIP</strong>: You don’t have to start at square one and row reduce the entirety of \(A^T\). You can just take the pivot columns, tranpose them, and row reduce that!</p> <p>For our example,</p> \[\operatorname{rref}\left( \begin{pmatrix} 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;1&amp;0&amp;1\\ 1&amp;2&amp;3&amp;2&amp;3\\ 1&amp;2&amp;1&amp;0&amp;2 \end{pmatrix}^T \right)= \begin{pmatrix} 1&amp;1&amp;0&amp;0\\ 0&amp;0&amp;1&amp;0\\ 0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0 \end{pmatrix}\] <p>Whilst</p> \[\operatorname{rref}\left( \begin{pmatrix} 1&amp;1&amp;1\\ 1&amp;1&amp;1\\ 1&amp;3&amp;3\\ 1&amp;1&amp;2 \end{pmatrix}^T \right)= \begin{pmatrix} 1&amp;1&amp;0&amp;0\\ 0&amp;0&amp;1&amp;0\\ 0&amp;0&amp;0&amp;1\\ \end{pmatrix}\] <p>Neat, huh? It’s just the nonzero rows! This is because we already know the nonpivot columns are linearly dependent on the pivot columns. So, when row reducing the transpose, those columns (now rows) will end up zeroing out as we row reduce.</p> <p>From the summary at the beginning, we know that since we have three pivot columns and four rows in \(A\), the dimension of the left nullspace will be \(4-3=1\). Thus, we will only get one vector in the left nullspace. The only nonpivot column is column two, and we can see that if we take col1-col2 we will get zero. So our null vector is \((1,-1,0,0)\).</p> <p>Thus,</p> \[\operatorname{null}(A^T)=\operatorname{span}\left\{ (1,-1,0,0) \right\}\]]]></content><author><name>Taylor F.</name></author><category term="linear-algebra"/><category term="best"/><summary type="html"><![CDATA[a lot of students struggle with this so here. row space, column space, null space, and left null space.]]></summary></entry><entry><title type="html">Column and Row Perspective</title><link href="https://smashmath.github.io/blog/columnperspective/" rel="alternate" type="text/html" title="Column and Row Perspective"/><published>2022-06-06T00:00:00+00:00</published><updated>2022-06-06T00:00:00+00:00</updated><id>https://smashmath.github.io/blog/columnperspective</id><content type="html" xml:base="https://smashmath.github.io/blog/columnperspective/"><![CDATA[<h1 id="column-perspective">Column Perspective</h1> <p>First, let’s start with general matrix column vector multiplication. We’ll focus on \(2\times2\) matrices first, as the principles extend very naturally for larger matrices.</p> <p>If we define the matrices</p> \[\textbf{A}=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix},\quad \textbf{x}=\begin{pmatrix} x_1\\x_2 \end{pmatrix}\] <p>Then,</p> \[\begin{equation} \textbf{A}\textbf{x}=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} x_1\\x_2 \end{pmatrix} =\begin{pmatrix} a_{11}x_1+a_{12}x_2\\ a_{21}x_1+a_{22}x_2\\ \end{pmatrix} \end{equation}\] <p>But look what happens when we separate this result by the \(x\) values.</p> \[\begin{pmatrix} a_{11}x_1+a_{12}x_2\\ a_{21}x_1+a_{22}x_2\\ \end{pmatrix}= \begin{pmatrix} a_{11}x_1\\ a_{21}x_1\\ \end{pmatrix}+\begin{pmatrix} a_{12}x_2\\ a_{22}x_2\\ \end{pmatrix}\] \[\begin{pmatrix} a_{11}x_1+a_{12}x_2\\ a_{21}x_1+a_{22}x_2\\ \end{pmatrix}= x_1\begin{pmatrix} a_{11}\\ a_{21}\\ \end{pmatrix}+x_2\begin{pmatrix} a_{12}\\ a_{22}\\ \end{pmatrix}\] \[\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} x_1\\x_2 \end{pmatrix}=x_1\begin{pmatrix} a_{11}\\ a_{21}\\ \end{pmatrix}+x_2\begin{pmatrix} a_{12}\\ a_{22} \end{pmatrix}\] <p>Which are just the original columns of \(\textbf{A}\). So, if we denote the columns of \(\textbf{A}\) by \(\textbf{A}_1,\textbf{A}_2\), we can say that</p> \[\begin{equation} \textbf{A} \begin{pmatrix} x_1\\x_2 \end{pmatrix}=x_1\textbf{A}_1+x_2\textbf{A}_2 \end{equation}\] <p>Which we can interpret as saying: “we want \(x_1\) of the first column of \(\textbf{A}\), and \(x_2\) of the second column of \(\textbf{A}\).”</p> <p>And, in general, if we denote the columns of an \(m\times n\) matrix \(\textbf{A}\) by \(\textbf{A}_1,\ldots,\textbf{A}_n\), we can say that</p> \[\textbf{A}\begin{pmatrix} x_1\\x_2\\\vdots\\x_n \end{pmatrix}=x_1\textbf{A}_1+x_2\textbf{A}_2+\ldots+x_n\textbf{A}_n\] <p>Thus, when multiplying a matrix on the right by a column vector, the column vector tells us how many of each column we are taking.</p> <h2 id="applying-this-to-general-matrix-multiplication">Applying this to general matrix multiplication</h2> <p>Suppose we have that</p> \[\textbf{A}=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix},\quad \textbf{B}=\begin{pmatrix} b_{11}&amp;b_{12}\\ b_{21}&amp;b_{22} \end{pmatrix}\] <p>Consider the product \(\textbf{A}\textbf{B}\),</p> \[\textbf{A}\textbf{B}=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} b_{11}&amp;b_{12}\\ b_{21}&amp;b_{22} \end{pmatrix}\] \[\textbf{A}\textbf{B} =\begin{pmatrix} a_{11}b_{11}+a_{12}b_{21}&amp;a_{11}b_{12}+a_{12}b_{22}\\ a_{21}b_{11}+a_{22}b_{21}&amp;a_{21}b_{12}+a_{22}b_{22} \end{pmatrix}\] <p>Now, look at each column of the product. Notice that the second column of \(\textbf{B}\) has no bearing on the first column of \(\textbf{A}\textbf{B}\). Similarly, the first column of \(\textbf{B}\) has no effect on the second column of \(\textbf{A}\textbf{B}\).</p> <p>If you don’t quite see what I mean, look at what happens if we separate the multiplication by the columns of \(\textbf{B}\).</p> \[\textbf{A}\textbf{B}_1=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} b_{11}\\b_{21} \end{pmatrix}\] \[\textbf{A}\textbf{B}_1 =\begin{pmatrix} a_{11}b_{11}+a_{12}b_{21}\\ a_{21}b_{11}+a_{22}b_{21} \end{pmatrix}= b_{11}\textbf{A}_1+b_{21}\textbf{A}_2\] <p>Which is the first column of \(\textbf{A}\textbf{B}\).</p> \[\textbf{A}\textbf{B}_2=\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} b_{12}\\b_{22} \end{pmatrix}\] \[\textbf{A}\textbf{B}_2 =\begin{pmatrix} a_{11}b_{12}+a_{12}b_{22}\\ a_{21}b_{12}+a_{22}b_{22} \end{pmatrix}= b_{12}\textbf{A}_1+b_{22}\textbf{A}_2\] <p>Which is the second column of \(\textbf{A}\textbf{B}\).</p> <p>Thus, we see that</p> \[\textbf{A}\textbf{B}= \bigg( \textbf{A}\textbf{B}_1\quad\textbf{A}\textbf{B}_2 \bigg)\] <p>And more generally,</p> \[\begin{equation} \textbf{A} \bigg( \textbf{B}_1\quad\cdots\quad\textbf{B}_n \bigg)= \bigg( \textbf{A}\textbf{B}_1\quad\cdots\quad\textbf{A}\textbf{B}_n \bigg) \end{equation}\] <p>That is to say, each column of the product, is just the left matrix times the individual column. Hence, we can use the column perspective for each column individually.</p> <h2 id="column-perspective-examples">Column Perspective Examples</h2> <p>This can GREATLY speed up certain computations.</p> <p>For example,</p> \[\textbf{A}\begin{pmatrix} 1\\0\\0 \end{pmatrix}=\textbf{A}_1\] <p>will just give us the first column of \(\textbf{A}\). And</p> \[\textbf{A}\begin{pmatrix} 1\\1\\0 \end{pmatrix}=\textbf{A}_1+\textbf{A}_2\] <p>is just the first column and the second column added together.</p> <p>This can also help us in the reverse direction! Take the example of solving</p> \[\left( \begin{array}{ccc|c} 1&amp;0&amp;3&amp;0\\ 0&amp;1&amp;-1&amp;0\\ 0&amp;0&amp;0&amp;0 \end{array} \right)\] <p>We can see that if we take the third column as it is (that is to say, taking exactly \(1\) of it), we can cancel out the nonzero entries by taking \(-3\) of column one, and \(1\) of column two. Thus, a solution will be \((-3,1,1)\). Since any scalar multiple of this vector will also yield the zero vector, the general solution will be</p> \[\textbf{x}=c\begin{pmatrix} -3\\1\\1 \end{pmatrix}\] <p>since taking \(-3\) of the first column, \(1\) of the second column, and \(1\) of the third column, will cancel everything out. No need to turn it back into equations and solve or whatever.</p> <p>Let’s use an example of matrix multiplication</p> \[\begin{pmatrix}-2&amp;1&amp;1\\1&amp;-2&amp;1\\1&amp;1&amp;-2\end{pmatrix} \begin{pmatrix}0&amp;1&amp;1\\1&amp;0&amp;1\\1&amp;1&amp;0\end{pmatrix}\] <p>The first column of the right matrix tells us to add up the second and third columns: \((1,-2,1)+(1,1,-2)=(2,-1,-1)\). The second column tells us to add the first and third columns: \((-2,1,1)+(1,1,-2)=(-1,2,-1)\). And the third column tells us to add up the first and second columns: \((-2,1,1)+(1,-2,1)=(-1,-1,2)\). Therefore, the product will be</p> \[\begin{pmatrix}-2&amp;1&amp;1\\1&amp;-2&amp;1\\1&amp;1&amp;-2\end{pmatrix} \begin{pmatrix}0&amp;1&amp;1\\1&amp;0&amp;1\\1&amp;1&amp;0\end{pmatrix}= \begin{pmatrix}2&amp;-1&amp;-1\\-1&amp;2&amp;-1\\-1&amp;-1&amp;2\end{pmatrix}\] <p>Personally, I can say that I prefer adding up the columns as opposed to doing nine three-dimensional dot products.</p> <hr/> <h1 id="row-perspective">Row Perspective</h1> <p>Row perspective, while not <em>quite</em> as useful as column perspective, still has its share of uses and applications. It is essentially the transpose of column perspective. Then, we will define our notation for the rows of an \(m\times n\) matrix \(A\) as \(\textbf{a}_1^T,\ldots,\textbf{a}_m^T\).</p> <p>Row perspective, is as follows.</p> \[\begin{equation} \begin{pmatrix} \textbf{a}_1^T\\\textbf{a}_2^T\\\vdots\\\textbf{a}_m^T \end{pmatrix}\textbf{B}= \begin{pmatrix} \textbf{a}_1^T\textbf{B}\\\textbf{a}_2^T\textbf{B}\\\vdots\\\textbf{a}_m^T\textbf{B} \end{pmatrix} \end{equation}\] <p>Where the entries of \(\textbf{a}_i^T\) tell you how many of each <strong>row</strong> of \(B\) to take.</p> <p>To see this for a \(2\times2\),</p> \[\begin{pmatrix} \textbf{a}_1^T\\ \textbf{a}_2^T\\ \end{pmatrix}\textbf{B} =\begin{pmatrix} a_{11}&amp;a_{12}\\ a_{21}&amp;a_{22} \end{pmatrix} \begin{pmatrix} b_{11}&amp;b_{12}\\ b_{21}&amp;b_{22} \end{pmatrix}\] \[\begin{pmatrix} \textbf{a}_1^T\\ \textbf{a}_2^T\\ \end{pmatrix}\textbf{B} =\begin{pmatrix} a_{11}b_{11}+a_{12}b_{21}&amp;a_{11}b_{12}+a_{12}b_{22}\\ a_{21}b_{11}+a_{22}b_{21}&amp;a_{21}b_{12}+a_{22}b_{22} \end{pmatrix}\] \[\begin{pmatrix} \textbf{a}_1^T\\ \textbf{a}_2^T\\ \end{pmatrix}\textbf{B} =\begin{pmatrix} a_{11}(b_{11}\quad b_{12})+a_{12}(b_{21}\quad b_{22})\\ a_{21}(b_{11}\quad b_{12})+a_{22}(b_{21}\quad b_{22})\\ \end{pmatrix}\] \[\begin{pmatrix} \textbf{a}_1^T\\ \textbf{a}_2^T\\ \end{pmatrix}\textbf{B} =\begin{pmatrix} (a_{11}\quad a_{12})\begin{pmatrix} b_{11}&amp;b_{12}\\ b_{21}&amp;b_{22} \end{pmatrix}\\ (a_{21}\quad a_{22})\begin{pmatrix} b_{11}&amp;b_{12}\\ b_{21}&amp;b_{22} \end{pmatrix}\\ \end{pmatrix}\] \[\begin{pmatrix} \textbf{a}_1^T\\ \textbf{a}_2^T\\ \end{pmatrix}\textbf{B} =\begin{pmatrix} \textbf{a}_1^T\textbf{B}\\ \textbf{a}_2^T\textbf{B}\\ \end{pmatrix}\] <p>So an example of this would be</p> \[\begin{pmatrix}0&amp;1&amp;1\\1&amp;1&amp;0\\1&amp;0&amp;1\end{pmatrix} \begin{pmatrix}-2&amp;1&amp;1\\1&amp;-2&amp;1\\1&amp;1&amp;-2\end{pmatrix}\] <p>The first row of the left matrix tells us we want \(1\) of the second row and \(1\) of the third row: \((1,-2,1)+(1,1,-2)=(2,-1,-1)\). The second row tells us to add up the first and second rows: \((-2,1,1)+(1,-2,1)=(-1,-1,2)\). And the third rows tells us to add the first and third rows: \((-2,1,1)+(1,1,-2)=(-1,2,-1)\). Therefore, the product will be</p> \[\begin{pmatrix}0&amp;1&amp;1\\1&amp;1&amp;0\\1&amp;0&amp;1\end{pmatrix} \begin{pmatrix}-2&amp;1&amp;1\\1&amp;-2&amp;1\\1&amp;1&amp;-2\end{pmatrix}= \begin{pmatrix}2&amp;-1&amp;-1\\-1&amp;-1&amp;2\\-1&amp;2&amp;-1\end{pmatrix}\] <h2 id="application-to-row-reduction">Application to row reduction</h2> <p>I use row perspective often when row reducing matrices. It helps me do multiple steps at the same time. So, let’s say I am trying to row reduce</p> \[\textbf{A}=\begin{pmatrix} 1&amp;1&amp;3&amp;2\\-1&amp;0&amp;-2&amp;-3\\2&amp;2&amp;6&amp;7 \end{pmatrix}\] <p>My thought process is as follows:</p> <p>First, the second row would be better if its negative was the first row, because then we would have a pivot in the first column and a zero in the second entry. Thus, the first row of my row reduction matrix should be \((0,-1,0)\), since we want \(-1\) of the second row.</p> <p>Next, if we add up the first two rows, then we get a pivot in the second column and a zero in the first entry. So, the second row of our row reduction matrix will be \((1,1,0)\), since we want to add up the first and second row.</p> <p>Finally, we can cancel out the first three entries of the third row by adding \(-2\) of row one. The third column should then be \((-2,0,1)\), since we want to take the third row and subtract \(2\) times the first row.</p> <p>Putting it all together, our row reduction matrix is</p> \[\textbf{R}=\begin{pmatrix} 0&amp;-1&amp;0\\1&amp;1&amp;0\\-2&amp;0&amp;1 \end{pmatrix}\] <p>And if we multiply \(\textbf{A}\) by our row reduction matrix, we get</p> \[\textbf{R}\textbf{A}= \begin{pmatrix} 0&amp;-1&amp;0\\1&amp;1&amp;0\\-2&amp;0&amp;1 \end{pmatrix}\begin{pmatrix} 1&amp;1&amp;3&amp;2\\-1&amp;0&amp;-2&amp;-3\\2&amp;2&amp;6&amp;7 \end{pmatrix}\] \[\textbf{R}\textbf{A}= \begin{pmatrix} 1&amp;0&amp;2&amp;3\\ 0&amp;1&amp;1&amp;-1\\ 0&amp;0&amp;0&amp;3 \end{pmatrix}\] <p>The final steps of row reduction are then very simple. Divide row three by \(3\), and then add the correct multiples of it to the first two rows to get the final pivot column.</p> <hr/> <h1 id="finding-inverses-by-inspection">Finding inverses by inspection</h1> <p>We can also use these perspectives to find inverses relatively easily (depending on the matrix). Of course, this is relatively pointless for a \(2\times2\) since there is the very easy formula. But I try to do this when possible for larger matrices.</p> <p>Let’s say we want to invert our row reduction matrix from before</p> \[\textbf{R}=\begin{pmatrix} 0&amp;-1&amp;0\\1&amp;1&amp;0\\-2&amp;0&amp;1 \end{pmatrix}\] <p>This is a <em>good</em> candidate for the perspectives, because there are lots of zeros! The more zeros, the easier it is to use this. If a matrix has no nonzero entries, unless there is some amazingly obvious pattern, I would just use either the adjugate matrix if it’s \(3\times3\) or row reduction.</p> <p>So, to do this, we want to find combinations of the rows and columns to get the identity matrix.</p> <p>The most obvious one to me is that if we take just \(1\) of the third column, we get the third column of the identity matrix. So that means the third column of \(\textbf{R}^{-1}\) will be \((0,0,1)\)</p> \[\textbf{R}^{-1}=\begin{pmatrix} *&amp;*&amp;0\\**&amp;*&amp;0\\**&amp;*&amp;1 \end{pmatrix}\] <p>Also, if we take \(-1\) of the first row, we will get the second row of the identity matrix. So the second row should be \((-1,0,0)\)</p> \[\textbf{R}^{-1}=\begin{pmatrix} *&amp;*&amp;0\\-1&amp;0&amp;0\\**&amp;*&amp;1 \end{pmatrix}\] <p>I can also see that to get the second column of the identity matrix we can take \(1\) of column one and \(2\) of column three: \((0,1,-2)+2(0,0,1)=(0,1,0)\). So the second column of our inverse will be \((1,0,2)\)</p> \[\textbf{R}^{-1}=\begin{pmatrix} *&amp;1&amp;0\\-1&amp;0&amp;0\\**&amp;2&amp;1 \end{pmatrix}\] <p>Finally, to get the first column, we need a combination of columns one and three to get \((1,0,0)\), given that we have \(-1\) of column two (from the 2,1 entry), \(-1(-1,1,0)\). To cancel out the second entry, it looks like we need \(1\) of column one:</p> \[1(0,1,-2)-1(-1,1,0)=(1,0,-2)\] <p>So, to cancel out that \(-2\), we can take \(2\) of column three. Thus, the first column of the inverse is \((1,-1,2)\).</p> \[\textbf{R}^{-1}=\begin{pmatrix} 1&amp;1&amp;0\\-1&amp;0&amp;0\\2&amp;2&amp;1 \end{pmatrix}\] <p>We could have also looked at rows one and three individually! I chose to do the whole column at once because that was my personal preference. You can do it in whatever order you like. I just do whatever is most obvious to me first.</p>]]></content><author><name>Taylor F.</name></author><category term="linear-algebra"/><category term="best"/><summary type="html"><![CDATA[How to simplify matrix multiplication with the best perspectives (and also find certain inverse matrices fast!)]]></summary></entry></feed>