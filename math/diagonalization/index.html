<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>A Different Perspective of Diagonalization | smashmath</title> <meta name="author" content=" "> <meta name="description" content="An (atttempt at an) intuitive approach to similar matrix decomposition."> <meta name="keywords" content="math, blog"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://smashmath.github.io/math/diagonalization/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "A Different Perspective of Diagonalization",
      "description": "An (atttempt at an) intuitive approach to similar matrix decomposition.",
      "published": "November 2, 2020",
      "authors": [
        {
          "author": "Taylor Grant",
          "authorURL": "",
          "affiliations": [
            {
              "name": "None",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">smashmath</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>A Different Perspective of Diagonalization</h1> <p>An (atttempt at an) intuitive approach to similar matrix decomposition.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#diagonalization">Diagonalization</a></div> <div><a href="#general-decomposition">General Decomposition</a></div> <ul> <li><a href="#complex-eigendecomposition-of-real-matrices">Complex Eigendecomposition of Real Matrices</a></li> </ul> </nav> </d-contents> <p>\(\newcommand{\Re}{\operatorname{Re}}\) \(\newcommand{\Im}{\operatorname{Im}}\)</p> <h1 id="diagonalization">Diagonalization</h1> <p>While this works for any dimension matrix, this will be of the example where the matrix is a \(2\times2\).</p> <p>Suppose \(A\) is a \(2\times2\) matrix with eigenvectors \(\textbf{v}_1,\textbf{v}_2\) associated with eigenvalues \(\lambda_1,\lambda_2\) respectively. By definition of an eigenvector, this would imply that for any scalars \(c_1,c_2\).</p> <p>\begin{equation} A(c_1\textbf{v}_1+c_2\textbf{v}_2)=\lambda_1c_1\textbf{v}_1+\lambda_2c_2\textbf{v}_2 \end{equation}</p> <p>Say we apply \(A\) a second time…</p> \[\begin{gather*} A(A(c_1\textbf{v}_1+c_2\textbf{v}_2))=A((\lambda_1c_1)\textbf{v}_1+(\lambda_2c_2)\textbf{v}_2)\\ A^2(c_1\textbf{v}_1+c_2\textbf{v}_2)=\lambda_1(\lambda_1c_1)\textbf{v}_1+\lambda_2(\lambda_2c_2)\textbf{v}_2\\ A^2(c_1\textbf{v}_1+c_2\textbf{v}_2)=\lambda_1^2c_1\textbf{v}_1+\lambda_2^2c_2\textbf{v}_2\\ \end{gather*}\] <p>We could easily repeat this as many times as we wanted. So it is also the case that</p> <p>\begin{equation}\label{exp} A^n(c_1\textbf{v}_1+c_2\textbf{v}_2)=\lambda_1^nc_1\textbf{v}_1+\lambda_2^nc_2\textbf{v}_2 \end{equation}</p> <p>Since for those vectors, the transformation \(A\) is just simple scaling, in the situation where we have to repeatedly apply \(A\) to some vector \(w\), it would clearly be preferable to get that vector in terms of \(\textbf{v}_1\) and \(\textbf{v}_2\). This seems like a job for change of basis…</p> <p>So let us define the eigenbasis \(\begin{equation} B=\{\textbf{v}_1,\textbf{v}_2\} \end{equation}\)</p> <p>The change of basis matrix \(P_{\varepsilon\leftarrow B}\) (where \(\varepsilon\) is the standard basis \(\varepsilon=\{e_1,e_2\}\)), then is</p> \[\begin{equation} P_{\varepsilon\leftarrow B}= \bigg( \textbf{v}_1\quad \textbf{v}_2 \bigg) \end{equation}\] <p>By extension,</p> <p>\begin{equation}\label{inv change} P_{B\leftarrow\varepsilon}=(P _{\varepsilon\leftarrow B})^{-1} \end{equation}</p> <p>So now suppose that</p> \[\textbf{w}=c_1\textbf{v}_1+c_2\textbf{v}_2\] <p>This tells us that the coordinate vector of \(w\) with respect to the eigenbasis \(B\), \((\textbf{w})_B\), is</p> \[\begin{equation} \textbf{w}=c_1\textbf{v}_1+c_2\textbf{v}_2 \implies (\textbf{w})_B=\begin{pmatrix}c_1\\c_2\end{pmatrix} \end{equation}\] <p>We know from \eqref{exp} then that</p> \[A^n\textbf{w}=A^n(c_1\textbf{v}_1+c_2\textbf{v}_2)=\lambda_1^nc_1\textbf{v}_1+\lambda_2^nc_2\textbf{v}_2\] <p>So</p> \[(A^n\textbf{w})_B=\begin{pmatrix}\lambda_1^nc_1\\\lambda_2^nc_2\end{pmatrix}\] <p>But that’s also the transformation</p> \[(A^n\textbf{w})_B=\begin{pmatrix}\lambda_1^nc_1\\\lambda_2^nc_2\end{pmatrix}=\begin{pmatrix}\lambda_1^n&amp;0\\0&amp;\lambda_2^n\end{pmatrix}\begin{pmatrix}c_1\\c_2\end{pmatrix}\] <p>If we call the diagonal matrix \(D=\begin{pmatrix}\lambda_1&amp;0\\0&amp;\lambda_2\end{pmatrix}\), then we get</p> <p>\begin{equation} (A^n\textbf{w})_B=D^n(\textbf{w})_B \end{equation}</p> <p>We’re so close now. Next, we change back to the standard basis by applying \(P_{\varepsilon\leftarrow B}\) to both sides,</p> \[P_{\varepsilon\leftarrow B}(A^n\textbf{w}) _{B} = P _{\varepsilon\leftarrow B}D^n (\textbf{w}) _B\] <p>\begin{equation}\label{close} A^n\textbf{w}=P _{\varepsilon\leftarrow B}D^n(\textbf{w}) _B \end{equation}</p> <p>To get things entirely in terms of the standard basis, we start by rewriting</p> \[(\textbf{w}) _B=P _{B\leftarrow\varepsilon}(\textbf{w}) _{\varepsilon}\] <p>Using \eqref{inv change} and the fact that \((\textbf{w}) _{\varepsilon}=w\) (by definition of the standard basis),</p> <p>\begin{equation} (\textbf{w}) _B=(P _{\varepsilon\leftarrow B})^{-1}\textbf{w} \end{equation}</p> <p>we substitute into \eqref{close}</p> \[A^n\textbf{w}=P _{\varepsilon\leftarrow B}D^n(P _{\varepsilon\leftarrow B})^{-1}\textbf{w}\] <p>If we just denote \(P _{\varepsilon\leftarrow B}\) as \(P\), then we finally get</p> <p>\begin{equation} A^n\textbf{w}=PD^nP^{-1}\textbf{w} \end{equation}</p> <h1 id="general-decomposition">General Decomposition</h1> <p>In general, if we know how a matrix \(A\) acts on a basis, then we may either construct or decompose it via a \(PDP^{-1}\) decomposition.</p> <h2 id="complex-eigendecomposition-of-real-matrices">Complex Eigendecomposition of Real Matrices</h2> <p>Let’s take the example of a real \(2\times2\) with complex eigenvalues. Suppose \(A\) is such a matrix with a complex eigenvalue \(a+bi\), where \(b\neq0\), associated with a complex eigenvector \(v\).</p> <p>\begin{equation} \label{complex eigenvector} A\textbf{v}=(a+bi)\textbf{v} \end{equation}</p> <p>Let’s get a bit more specific by decomposing \(v\) into its real and imaginary parts.</p> \[\begin{gather*} A(\Re(\textbf{v})+i\Im(\textbf{v}))=(a+bi)(\Re(\textbf{v})+i\Im(\textbf{v}))\\ A\Re(\textbf{v})+i(A\Im(\textbf{v}))=(a\Re(\textbf{v})-b\Im(\textbf{v}))+i(b\Re(\textbf{v})+a\Im(\textbf{v})) \end{gather*}\] <p>We can equate the real and imaginary parts on both sides,</p> <p>\begin{equation} A\Re(\textbf{v})=a\Re(\textbf{v})-b\Im(\textbf{v}),\quad A\Im(\textbf{v})=b\Re(\textbf{v})+a\Im(\textbf{v}) \end{equation}</p> <p>Now we know that \(\{\Re(\textbf{v}),\Im(\textbf{v})\}\) is a basis of \(\mathbb{R}^2\) because if they were linearly dependent, then \(\Im(\textbf{v})=k\Re(\textbf{v})\). That would change \eqref{complex eigenvector} into</p> \[A(1+ki)\Re(\textbf{v})=(a+bi)(1+ki)\Re(\textbf{v})\] <p>Dividing by the common \(1+ki\),</p> \[A\Re(\textbf{v})=(a+bi)\Re(\textbf{v})\] <p>This implies a contradiction since the imaginary part of the left side is zero due to \(A\) being real, but the right is not if \(b\neq0\). Therefore,</p> \[\begin{equation} \label{complex basis} B=\{\Re(\textbf{v}),\Im(\textbf{v})\} \text{ is a basis for } \mathbb{R}^2 \end{equation}\] <p>It then follows that</p> \[\begin{equation} P= \bigg( \Re(\textbf{v})\quad\Im(\textbf{v}) \bigg) \end{equation}\] <p>is invertible.</p> <p>Well, we know how \(A\) acts on the basis \(B\) \eqref{complex basis}:</p> \[(A\Re(\textbf{v}))_B=\begin{pmatrix}a\\-b\end{pmatrix},\quad (A\Im(\textbf{v}))_B=\begin{pmatrix}b\\a\end{pmatrix}\] <p>So our matrix \(D\) is then,</p> \[\begin{equation} D=\begin{pmatrix}a&amp;b\\-b&amp;a\end{pmatrix} \end{equation}\] <p>And our eigendecomposition of \(A\) is</p> \[\begin{equation} A=\bigg( \Re(\textbf{v})\quad\Im(\textbf{v}) \bigg) \begin{pmatrix}a&amp;b\\-b&amp;a\end{pmatrix} \bigg( \Re(\textbf{v})\quad\Im(\textbf{v}) \bigg)^{-1} \end{equation}\] <p>It can be verified that the eigenvalues of this particular \(D\) are \(\lambda=a\pm bi\).</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 09, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>