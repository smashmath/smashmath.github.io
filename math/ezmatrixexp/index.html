<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Matrix Exponential Formulas for 2x2 Matrices Using Laplace Transforms | smashmath</title> <meta name="author" content=" "> <meta name="description" content="this has been rewritten"> <meta name="keywords" content="math, blog"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://smashmath.github.io/math/ezmatrixexp/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Matrix Exponential Formulas for 2x2 Matrices Using Laplace Transforms",
      "description": "this has been rewritten",
      "published": "April 26, 2021",
      "authors": [
        {
          "author": "Taylor F.",
          "authorURL": "",
          "affiliations": [
            {
              "name": "None",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">smashmath</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Matrix Exponential Formulas for 2x2 Matrices Using Laplace Transforms</h1> <p>this has been rewritten</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#formulas">Formulas</a></div> <div><a href="#proofs">Proofs</a></div> <ul> <li><a href="#intro">Intro</a></li> <li><a href="#distinct-eigenvalues">Distinct Eigenvalues</a></li> <li><a href="#complex-conjugate-eigenvalues">Complex Conjugate Eigenvalues</a></li> <li><a href="#one-defective-eigenvalue">One Defective Eigenvalue</a></li> <li><a href="#one-nondefective-eigenvalue">One Nondefective Eigenvalue</a></li> </ul> <div><a href="#applications">Applications</a></div> <div><a href="#closing-remarks">Closing Remarks</a></div> </nav> </d-contents> <p><strong>WARNING</strong>: If you are in a differential equations class right now, turn back. This is black magic that your professor <em>will not</em> want you to memorize or use on their tests. These are just cool and/or for the convenience of those who evaluate a lot of matrix exponentials, as I do.</p> <h1 id="formulas">Formulas</h1> <p>Let \(A\) be a \(2\times 2\) matrix.</p> <p>If \(A\) has two distinct eigenvalues \(\lambda_1,\lambda_2\), then</p> \[\begin{equation} \label{form1} e^{At}=\frac{e^{\lambda_2t}(A-\lambda_1I)-e^{\lambda_1t}(A-\lambda_2I)}{\lambda_2-\lambda_1} \end{equation}\] <p>If \(A\) has a real determinant, \(\operatorname{tr}(A)=0\), and \(\det(A)&lt;0\), then an eigenvalue of \(A\) is \(\lambda=\sqrt{-\det(A)}\), and</p> \[\begin{equation} \label{form1b} e^{At}=\cosh(\lambda t)I+\frac{\sinh(\lambda t)}{\lambda}A \end{equation}\] <p>If \(A\) is singular and has a nonzero trace,</p> \[\begin{equation}\label{form1c} e^{At}=\frac{e^{\operatorname{tr}(A)t}A-(A-\operatorname{tr}(A)I)}{\operatorname{tr}(A)} \end{equation}\] <p>If \(A\) is real and has complex conjugate eigenvalues \(a \pm bi\), then</p> \[\begin{equation} e^{At}=e^{a t}\left(\cos(b t)I+\frac{\sin(b t)}{b }(A-a I)\right) \end{equation}\] \[\begin{equation} \label{form2} e^{At}=\frac{e^{at}}{b}\left(b\cos(b t)I+\sin(b t)(A-a I)\right) \end{equation}\] <p>If \(A\) has one defective eigenvalue \(\lambda\), then</p> \[\begin{equation} \label{form3} e^{At}=e^{\lambda t}\left(I+t(A-\lambda I)\right) \end{equation}\] <p>And if \(A\) has only one eigenvalues \(\lambda\) which is not defective (meaning \(A=\lambda I\)) then</p> \[\begin{equation} \label{form4} e^{At}=e^{\lambda t}I \end{equation}\] <h1 id="proofs">Proofs</h1> <p>To prove these formulas, we are going to use the Laplace Transform <img class="emoji" title=":eyes:" alt=":eyes:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f440.png" height="20" width="20"></p> <p>If solving <em>systems</em> of differential equations with the Laplace Transform seems strange, then the intro is probably worth reading. If this is nothing new to you, you can most likely skim the labeled equations.</p> <h2 id="intro">Intro</h2> <p>We first use the fact that</p> \[\begin{equation} (sI-A)^{-1}=\mathcal{L}\{e^{At}\} \end{equation}\] <p>I encourage trying to prove this using the inverse Laplace Transform and the geometric series for matrices. It’s cool.</p> <p>But just to get an idea of where this comes from, observe that if \(A\) is an \(n\times n\) constant matrix, then the solution to the system of differential equations</p> \[x'=Ax,\quad x(0)=x_0\] <p>will be</p> \[x=e^{At}x_0\] <p>Which is beautifully consistent with the solution to \(y'=ay,\; y(0)=y_0\) being \(y=y_0e^{at}\).</p> <p>If we were to attempt to solve the original system of differential equations using the Laplace Transform (just transforming each entry individually), we would get</p> \[\begin{gather*} sX-x_0=AX\\ (sI-A)X=x_0\\ X=(sI-A)^{-1}x_0\\ x=\mathcal{L}^{-1}\{(sI-A)^{-1}\}x_0 \end{gather*}\] <p>And we can see that \(\mathcal{L}^{-1}\{(sI-A)^{-1}\}\) is in the place of \(e^{At}\), so we may use the existence and uniqueness theorem to verify that they are in fact equal.</p> <p>The second fact we use is that for \(2\times2\) matrices, \((sI-A)^{-1}\) is <em>really</em> easy to calculate.</p> \[(sI-A)^{-1}=\frac{1}{p(s)}\operatorname{adj}(sI-A)\] \[\begin{equation} \label{firstlaplace} (sI-A)^{-1}=\frac{1}{p(s)}(sI+A-\operatorname{tr}(A)I) \end{equation}\] <p>where \(p(s)\) is the characteristic polynomial. The above can be verified by first seeing that \(\operatorname{adj}(sI-A)=sI-\operatorname{adj}(A)\), and then using</p> \[A+\operatorname{adj}(A)=\operatorname{tr}(A)I\implies \operatorname{adj}(A)=\operatorname{tr}(A)I-A\] <p>From this point on, we will abbreviate \(\operatorname{tr}(A)\) as \(\Sigma\), due to the fact that the trace is the sum of the eigenvalues \(\operatorname{tr}(A)=\sum\lambda_i\).</p> \[\begin{equation} \Sigma=\lambda_1+\lambda_2 \end{equation}\] <p>We can then rewrite our original laplace transform \eqref{firstlaplace} as</p> \[\begin{equation} (sI-A)^{-1}=\frac{1}{p(s)}(sI+A-\Sigma I) \end{equation}\] <p>How we proceed depends on the form of \(p(s)\).</p> <h2 id="distinct-eigenvalues">Distinct Eigenvalues</h2> <p>If the eigenvalues \(\lambda_1\neq\lambda_2\), then \(p(s)=(s-\lambda_1)(s-\lambda_2)\). Calculating the matrix exponential then only requires partial fractions and simplification.</p> <p>Observe that the solution to</p> \[\begin{equation} \frac{b_0+b_1s}{(s-\lambda_1)(s-\lambda_2)}=\frac{x_1}{s-\lambda_1}+\frac{x_2}{s-\lambda_2} \end{equation}\] <p>is</p> \[\begin{equation} x_1=-\frac{b_0+\lambda_1b_1}{\lambda_2-\lambda_1},\quad x_2=\frac{b_0+\lambda_2b_1}{\lambda_2-\lambda_1} \end{equation}\] <p>Is there any reason we can’t apply this to our matrix equation above? No! For a hand-wavy justification, just imagine we’re doing the above equation for each entry.</p> \[\begin{equation} \frac{A-\Sigma I+sI}{(s-\lambda_1)(s-\lambda_2)}=\frac{X_1}{s-\lambda_1}+\frac{X_2}{s-\lambda_2} \end{equation}\] <p>We have that \(b_0=A-\Sigma I\) and \(b_1=I\). Plugging this into our formula,</p> \[\begin{equation} X_1=\frac{\Sigma I-A-\lambda_1 I}{\lambda_2-\lambda_1},\quad X_2=\frac{A-\Sigma I+\lambda_2I}{\lambda_2-\lambda_1} \end{equation}\] <p>By our definition of \(\Sigma\),</p> \[\begin{equation} X_1=\frac{\lambda_2 I-A}{\lambda_2-\lambda_1},\quad X_2=\frac{A-\lambda_1I}{\lambda_2-\lambda_1} \end{equation}\] <p>Leading us to</p> \[\begin{equation} (sI-A)^{-1}=\frac{1}{s-\lambda_1}\left(\frac{\lambda_2 I-A}{\lambda_2-\lambda_1}\right)+\frac{1}{s-\lambda_2}\left(\frac{A-\lambda_1I}{\lambda_2-\lambda_1}\right) \end{equation}\] \[\begin{equation} \mathcal{L}\{e^{At}\}=\frac{1}{s-\lambda_2}\left(\frac{A-\lambda_1I}{\lambda_2-\lambda_1}\right)-\frac{1}{s-\lambda_1}\left(\frac{A-\lambda_2 I}{\lambda_2-\lambda_1}\right) \end{equation}\] <p>This is a trivial Inverse Laplace Transform.</p> \[\begin{equation} e^{At}=e^{\lambda_2t}\left(\frac{A-\lambda_1I}{\lambda_2-\lambda_1}\right)-e^{\lambda_1t}\left(\frac{A-\lambda_2 I}{\lambda_2-\lambda_1}\right) \end{equation}\] <p>We may then simplify to \eqref{form1}.</p> \[e^{At}=\frac{e^{\lambda_2t}(A-\lambda_1I)-e^{\lambda_1t}(A-\lambda_2I)}{\lambda_2-\lambda_1}\quad \blacksquare\] <p>Note that if \(A\) is a complex matrix with complex eigenvalues that <em>are not</em> conjugates of each other, then \eqref{form1} will still work and be the best option.</p> <p>Now for some bonus equations… First, if \(\det(A)\) is real and strictly less than zero, and \(\operatorname{tr}(A)=0\), then \(\lambda_1=-\lambda_2=\sqrt{-\det(A)}\). This simplifies \eqref{form1} to \eqref{form1b}.</p> \[\begin{equation} e^{At}=\cosh(\lambda t)I+\frac{\sinh(\lambda t)}{\lambda}A \end{equation}\] <p>Bonus: If \(A\) is singular and the other eigenvalue is not also zero, this simplifies to \eqref{form1c}</p> \[\begin{equation} e^{At}=I+\frac{e^{\operatorname{tr}(A)t}-1}{\operatorname{tr}(A)}A \end{equation}\] <p>The best part is that this formula actually holds for all square rank-one matrices regardless of size (as long as \(\operatorname{tr}(A)\neq0\;\)!) Bonus bonus: Since \(e^{(A+kI)t}=e^{kt}e^{At}\), then if \(B=A+kI\) where \(A\) is a rank one matrix with a nonzero trace, then you can still use the above formula, just multiply by \(e^{kt}\). That is to say, <em>any</em> matrix which is a scalar matrix (\(kI\)) away from a rank-one matrix (with a nonzero trace) can use this above formula, no matter the size.</p> <h2 id="complex-conjugate-eigenvalues">Complex Conjugate Eigenvalues</h2> <p>If \(A\) is real and the eigenvalues of \(A\) are complex conjugates \(a\pm bi\), where \(b\neq0\), then \(p(s)=(s-a)^2+b^2\) and \(\Sigma=2a\).</p> <p>\eqref{firstlaplace} is then</p> \[\begin{equation} (sI-A)^{-1}=\frac{sI+A-2a I}{(s-a)^2+b^2} \end{equation}\] <p>With foresight into inverse Laplace transforms, we can rewrite this as</p> \[\begin{equation} (sI-A)^{-1}=\frac{(s-a)I+A-a I}{(s-a)^2+b^2} \end{equation}\] <p>And we can already take the Laplace transform, actually, but for the sake of completeness we will go one more step.</p> \[\begin{equation} \mathcal{L}\{e^{At}\}=\left(\frac{s-a}{(s-a)^2+b^2}\right)I+\frac{1}{b}\left(\frac{b}{(s-a)^2+b^2}\right)(A-a I) \end{equation}\] <p>Finally arriving at \eqref{form2} when we factor out \(e^{at}\) from both terms.</p> \[\begin{equation} e^{At}=e^{at}\left(\cos(bt)I+\frac{\sin(bt)}{b}(A-a I)\right)\quad \blacksquare \end{equation}\] <h2 id="one-defective-eigenvalue">One Defective Eigenvalue</h2> <p>If \(A\) has only one eigenvalue of multiplicity two \(\lambda\), then \(p(s)=(s-\lambda)^2\) and \(\Sigma=2\lambda\).</p> <p>If \(A\) is just a scalar matrix, \(A=\lambda I\), then \(e^{At}=e^{\lambda t}I\), which isn’t very interesting. If \(A\) is not a scalar matrix, however, then</p> <p>Substituting into \eqref{firstlaplace},</p> \[\begin{equation} (sI-A)^{-1}=\frac{sI+A-2\lambda I}{(s-\lambda)^2} \end{equation}\] <p>This simplifies quite nicely into</p> \[\begin{equation} (sI-A)^{-1}=\frac{1}{s-\lambda}I+\frac{1}{(s-\lambda)^2}(A-\lambda I) \end{equation}\] <p>The inverse Laplace yields exactly \eqref{form3} when we factor out \(e^{\lambda t}\)</p> \[\begin{equation} e^{At}=e^{\lambda t}\left(I+t(A-\lambda I)\right)\quad \blacksquare \end{equation}\] <h3 id="one-nondefective-eigenvalue">One Nondefective Eigenvalue</h3> <p>This only occurs when \(A\) is a scalar matrix, but it’s techinically a case so it’s worth mentioning. So let’s say that \(A=\lambda I\). Then</p> \[e^{At}=\sum_{n=0}^\infty \frac{(\lambda tI)^n}{n!}\] \[e^{At}=\sum_{n=0}^\infty \frac{(\lambda t)^n}{n!}I^n\] \[e^{At}=\left(\sum_{n=0}^\infty \frac{(\lambda t)^n}{n!}\right)I=e^{\lambda t}I\] <h1 id="applications">Applications</h1> <p>The matrix exponential is incredibly useful in solving systems differential equations, and especially initial value problems.</p> <p>As we mentioned in the intro, if \(A\) is an \(n\times n\) constant matrix, then the solution to the system of differential equations is as such</p> \[\begin{equation} x'=Ax,\quad x(0)=x_0\implies x=e^{At}x_0 \end{equation}\] <p>So to use our formulas, we need only solve the characteristic polynomial. For \(2\times 2\) matrices, there is the convenient formula</p> \[\begin{equation} p(s)=s^2-\operatorname{tr}(A)s+\det(A) \end{equation}\] <p>And if you are doing differential equations, you should be able to solve quadratic equations with little effort. Then we use whichever formula applies to the roots. This skips the work of computing eigenvectors entirely. As a bonus, the columns will be the normalized solutions.</p> <p>That said If one wishes to go back to the something vaguely of the form</p> \[x=c_1e^{\lambda_1t}v_1+c_2e^{\lambda_2t}v_2\] <p>For \eqref{form1}, you can let \(v_1\) be any basis for the column space of \(A-\lambda_2I\), and similarly let \(v_2\) be any basis for the column space of \(A-\lambda_1I\). This coming directly from the matrices multiplying \(e^{\lambda_it}\) in \eqref{form1}.</p> <p>In \eqref{form2} you can let \(v_1\) and \(v_2\) be the column of \(e^{At}\).</p> <p>The eigenvector \(v_1\) of \(A\) in \eqref{form3} can be any basis for the column space of \(A-\lambda I\). For \(v_2\), any column of \(e^{At}\) which has a nonzero term with a \(t\) on it should suffice.</p> <h2 id="closing-remarks">Closing Remarks</h2> <p>I enjoyed verifying that taking the derivative of the first three formulas is the same as multiplying by \(A\).</p> \[\begin{equation} \frac{d}{dt}e^{At}=Ae^{At} \end{equation}\] <p>If you choose to attempt to do so as well, you may need the Cayley-Hamilton theorem.</p> <p>Even more fun than that is verifying that substituting \(-t\) for \(t\) does in fact give the inverse.</p> \[\begin{equation} e^{At}e^{-At}=I\implies e^{-At}=\left(e^{At}\right)^{-1} \end{equation}\] </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 16, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>