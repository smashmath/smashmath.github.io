<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Introduction to Least Squares Part 2 (Electric Boogaloo) | smashmath</title> <meta name="author" content=" "> <meta name="description" content="Why the heck do we multiply by the transpose"> <meta name="keywords" content="math, blog"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://smashmath.github.io/blog/leastsquarespart2/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Introduction to Least Squares Part 2 (Electric Boogaloo)",
      "description": "Why the heck do we multiply by the transpose",
      "published": "April 15, 2024",
      "authors": [
        {
          "author": "Taylor F.",
          "authorURL": "",
          "affiliations": [
            {
              "name": "None",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">smashmath</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Introduction to Least Squares Part 2 (Electric Boogaloo)</h1> <p>Why the heck do we multiply by the transpose</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#why-is-it-consistent">Why is it consistent</a></div> <div><a href="#why-is-it-the-closest-solution">Why is it the closest solution</a></div> </nav> </d-contents> <p>This is a very short sequel to my previous post on <a href="../leastsquares/" target="_blank">least squares</a>.</p> <p>These are the questions I left at the end of that post (in the more general complex form).</p> <ol> <li>Why is \(\mathbf{A}^*\mathbf{A}\mathbf{x}=\mathbf{A}^*\mathbf{b}\) guaranteed a solution?</li> <li>Why would the solution to the normal equation actually be the “closest” solution?</li> </ol> <p>When I say “dot product” here, I mean the general euclidean inner product on \(\mathbb{C}^n\), where we multiply by the <em>conjugate</em> transpose (adjoint) rather than the regular transpose.</p> \[\langle w,v\rangle=v^*w=\left(\overline{v}\right)^Tw\] <p>Note that if we talk about \(\mathbb{R}^n\), then the adjoint is just, of course, the regular transpose.</p> <p>If you need a reminder for why we need the <em>conjugate</em>, it’s because complex numbers get weird when you just square them:</p> \[\begin{pmatrix}1&amp;i\end{pmatrix}\begin{pmatrix}1\\i\end{pmatrix}=1^2+i^2=0\] <p>So we can’t just say \(w\cdot v=v^Tw\) and call it a day if we want to be as general as possible. But you are more than welcome to just interpret \(v^*\) as the transpose, and assume we’re talking in terms of real vectors (my complex compulsion prevents me from doing so).</p> <h2 id="why-is-it-consistent">Why is it consistent</h2> <p>Recall that a system of equations \({A}{x}={b}\) is consistent when \(b\) is in the image or column space of \(A\). So the column space, \(W\), is going to be the main character in our story today.</p> <p>We know that every vector space can be decomposed as the direct sum of any subspace and its orthogonal complement. That is,</p> \[\mathbb{C}^n=W\oplus W^\perp\] <p>This means that \(b\) can be written <em>uniquely</em> as \(b=w+w_\perp\), where \(w\) is \(W\) and \(w_\perp\) is orthogonal to everything in \(W\). Conceptually this can be understood as writing \(b\) as a sum of a vector which does make a consistent system, and a vector which can knock us out of being consistent (i.e. the system is consistent if and only if \(w_\perp=0\)).</p> <p>But what does it mean to be orthogonal in \(\mathbb{C}^n\)? What is the inner product? Well, like we said before, it’s not as simple as just \(w\cdot v=v^Tw\). Instead, it is \(w\cdot v=v^*w\), which means the complex dot product is not commutative, but conjugate commutative (\(v\cdot w=\overline{w\cdot v}\)). Though, since we are only interested in orthogonality and dot products of zero, we get a sort of orthogonal symmetry/commutativity (\(v\cdot w=0\iff w\cdot v=0\)).</p> <p>This does add one complication. While in \(\mathbb{R}^n\), \(An=0\) implies that \(n\) is orthogonal to the rows of \(A\), that isn’t quite so in \(\mathbb{C}^n\), since the inner product requires one vector to be conjugated. That is, while in \(\mathbb{R}^n\), the row space is the orthogonal complement of the null space, in \(\mathbb{C}^n\), the null space is the orthogonal complement of the conjugated row space (\(\ker(B)^\perp=\operatorname{row}(\overline{B})\)). Note that if \(B\) is real, we do recover the result that the kernel is orthogonal to the row space.</p> <p>But if we let \(B=A^*\) in the expression above, this means the orthogonal complement of the null space of \(A^*\) is the column space of \(A\). That is, if a vector \(u\) is orthogonal to the column space of \(A\), then \(A^*u=0\).</p> <p>Thus, \(A^*w_\perp=0\) and \(A^*b=A^*w\). Essentially, this means that multiplying by the adjoint <em>sort of</em> projects us into the column space of \(A\) (or, at least, zeros out the part that isn’t in the column space).</p> <p>Since \(w\) is by definition in the column space of \(A\), we know \(Ax=w\) is consistent. Let \(x=c\) be the solution (that is, \(w=Ac\)).</p> <p>So let’s write out what happens when we multiply \(Ax=b\) by \(A^*\).</p> \[A^*Ax=A^*b=A^*w=A^*Ac\] <p>This clearly has a solution: \(x=c\), which actually implies that the solution to \(A^*Ax=A^*b\) is the same as the solution to \(Ax=w\).</p> <p>That is, the least squares solution is the solution to the system projected onto the column space. Which actually makes some sense intuitively. If we want the closest solution, we want it to be the solution to the system where the vector is projected orthogonally to the column space. Which actually answers both our questions.</p> <h2 id="why-is-it-the-closest-solution">Why is it the closest solution</h2> <p>Now, I said it answers both of our questions, but perhaps you aren’t quite convinced that just because it’s the solution to the system where \(b\) has been projected into the column space it actually minimizes our error. Let’s see if I can change your mind.</p> <p>We measure the “closest” solution using \(\left\lVert b-Ax \right\rVert^2\) (minimizing the squares of error: hence, ‘least squares’).</p> <p>Note that by orthogonality, we can say that \(\left\lVert w+w_\perp \right\rVert^2=\left\lVert w \right\rVert^2+\left\lVert w_\perp \right\rVert^2\). But, since \(w\) and \(Ax\) are both in \(W\), we can rewrite</p> \[\left\lVert b-Ax \right\rVert^2=\left\lVert w+w_\perp-Ax \right\rVert^2=\left\lVert w-Ax \right\rVert^2+\left\lVert w_\perp \right\rVert^2\] <p>Notice that no matter <em>what</em> \(x\) is, our squared error will always be \(\geq\left\lVert w_\perp \right\rVert^2\). This is hopefully somewhat intuitive. Since \(w_\perp\) in a sense tells us <em>how</em> inconsistent our system is, its size acts as a lower bound for our error. Cool, right?</p> <p>Thus, the only thing we <em>can</em> do to minimize error, is to minimize \(\left\lVert w-Ax \right\rVert^2\). The best we can do is make it zero. But… like we said before, \(w=Ac\) for some \(c\). Then, \(x=c\) will give us zero. So \(x=c\), the solution to \(Ax=w\), really <em>is</em> the solution that minimizes the error!</p> <p>One might object: what if \(A\) has dependent columns and there are multiple solutions to \(Ax=w\)? Well, we can see that <em>all</em> of those solutions will minimize the error. That is, even if the least squares solution is not unique, every single one will minimize the error! This is because the error is in terms of the magnitude of \(w-Ax\). And \(w-Ax=0\) if and only if \(x\) is a preimage of \(w\) of \(A\) (there is no restriction on <em>which preimage</em>).</p> <p>If you’re very picky, and you want to pick one least squares solution, then you <em>could</em> theoretically pick the least squares solution with the minimum magnitude. If \(c\) is one least squares solution, then any other least squares solution will be of the form \(c+u\) where \(u\in\ker(A)\). Then, we can play a similar game in trying to minimize</p> \[\left\lVert c+u \right\rVert^2=\left\lVert k+k_\perp+u \right\rVert^2=\left\lVert k+u \right\rVert^2+\left\lVert k_\perp \right\rVert^2\] <p>where we are decomposing \(c\) in terms of \(k\in\ker(A)\) and \(k_\perp\in\ker(A)^\perp\). Since our degree of freedom is in choosing \(u\), we can pick \(u=-k\), and then we find that the ‘least squares least squares solution’ is the solution in the orthogonal complement of \(\ker(A)\).</p> \[x_{least}=\operatorname{proj}_{\ker(A)^\perp}(c)\] <p><a href="https://youtu.be/M5CeQG1YfEQ?si=2J5M9Tdyq01GVAsc" target="_blank" rel="external nofollow noopener">hyperlink</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 15, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>