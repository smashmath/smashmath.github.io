<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Matrix Exponential Formulas for 2x2 Matrices | smashmath</title> <meta name="author" content=" "> <meta name="description" content="Who needs eigenvectors?"> <meta name="keywords" content="math, blog"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://smashmath.github.io/blog/2x2ezmatrixexp/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Matrix Exponential Formulas for 2x2 Matrices",
      "description": "Who needs eigenvectors?",
      "published": "December 13, 2021",
      "authors": [
        {
          "author": "Taylor F.",
          "authorURL": "",
          "affiliations": [
            {
              "name": "None",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">smashmath</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Matrix Exponential Formulas for 2x2 Matrices</h1> <p>Who needs eigenvectors?</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#formulas">Formulas</a></div> <div><a href="#proofs">Proofs</a></div> <ul> <li><a href="#one-defective-eigenvalue">One Defective Eigenvalue</a></li> <li><a href="#complex-conjugate-eigenvalues">Complex Conjugate Eigenvalues</a></li> <li><a href="#zero-trace-and-negative-determinant">Zero trace and negative determinant</a></li> <li><a href="#one-nondefective-eigenvalue">One Nondefective Eigenvalue</a></li> <li><a href="#rank-1-matrix">Rank 1 matrix</a></li> <li><a href="#distinct-eigenvalues">Distinct Eigenvalues</a></li> </ul> <div><a href="#another-approach">Another approach</a></div> <div><a href="#applications">Applications</a></div> <div><a href="#closing-remarks">Closing Remarks</a></div> </nav> </d-contents> <p><strong>WARNING</strong>: If you are in a differential equations class right now, turn back. This is black magic that your professor <em>will not</em> want you to memorize or use on their tests. These are just cool and/or for the convenience of those who evaluate a lot of matrix exponentials, as I do.</p> <h1 id="formulas">Formulas</h1> <p>Let \(A\) be a \(2\times 2\) matrix.</p> <ul> <li>If \(A\) has one defective eigenvalue \(\lambda\), then</li> </ul> \[\begin{equation} \label{defect} e^{At}=e^{\lambda t}\left(I+t(A-\lambda I)\right) \end{equation}\] <ul> <li>If \(A\) is real and has complex conjugate eigenvalues \(a \pm bi\), then</li> </ul> \[\begin{equation}\label{complex1} e^{At}=e^{a t}\left(\cos(b t)I+\frac{\sin(b t)}{b }(A-a I)\right) \end{equation}\] \[\begin{equation} \label{complex2} e^{At}=\frac{e^{at}}{b}\bigg(b\cos(b t)I+\sin(b t)(A-a I)\bigg) \end{equation}\] <ul> <li>If \(A\) has a real determinant, \(\operatorname{tr}(A)=0\), and \(\det(A)&lt;0\), then an eigenvalue of \(A\) is \(\lambda=\sqrt{-\det(A)}\), and</li> </ul> \[\begin{equation} \label{0trace} e^{At}=\cosh(\lambda t)I+\frac{\sinh(\lambda t)}{\lambda}A \end{equation}\] <ul> <li>If \(A\) is singular and has a nonzero trace,</li> </ul> \[\begin{equation}\label{formsingular} e^{At}=\frac{e^{\operatorname{tr}(A)t}A-(A-\operatorname{tr}(A)I)}{\operatorname{tr}(A)} \end{equation}\] \[\begin{equation}\label{formsingularb} e^{At}=I+\frac{e^{\operatorname{tr}(A)t}-1}{\operatorname{tr}(A)}A \end{equation}\] <ul> <li>If \(A\) has two distinct eigenvalues \(\lambda_1,\lambda_2\), then</li> </ul> \[\begin{equation} \label{distinct} e^{At}=\frac{e^{\lambda_2t}(A-\lambda_1I)-e^{\lambda_1t}(A-\lambda_2I)}{\lambda_2-\lambda_1} \end{equation}\] \[\begin{equation} \label{distinct2} e^{At}=\frac{\lambda_2e^{\lambda_1t}-\lambda_1e^{\lambda_2t}}{\lambda_2-\lambda_1}I + \frac{e^{\lambda_2t}-e^{\lambda_1t}}{\lambda_2-\lambda_1}A \end{equation}\] <p>And if \(A\) has only one eigenvalues \(\lambda\) which is not defective (meaning \(A=\lambda I\)) then</p> \[\begin{equation} \label{scalar} e^{\lambda It}=e^{\lambda t}I \end{equation}\] <h1 id="proofs">Proofs</h1> <p>For these proofs, we are going to be using the Cayley-Hamilton theorem quite a bit. Without getting into all the details, subtleties, and its proof, the basic idea is</p> <blockquote> <p>A matrix satisfies its own characteristic polynomial</p> </blockquote> <p>Maybe that makes intuitive sense, maybe it doesn’t. But if you want to see for yourself, take the matrix \(\begin{pmatrix}a&amp;b\\c&amp;d\end{pmatrix}\), and plug it into its characteristic polynomial</p> \[\det(A-\lambda I)=\lambda^2-(a+d)\lambda+(ad-bc)=0\] \[\begin{pmatrix} a&amp;b\\c&amp;d \end{pmatrix}^2-(a+d) \begin{pmatrix} a&amp;b\\c&amp;d \end{pmatrix}+(ad-bc) \begin{pmatrix} 1&amp;0\\0&amp;1 \end{pmatrix}\] <p>You will indeed get the zero matrix. Pretty cool, right?</p> <p>Another fact we will be using constantly is</p> <p>\begin{equation} e^{kt}e^{At}=e^{(A+kI)t} \end{equation}</p> <p>This is because \(e^{(A+kI)t}=e^{At}e^{kIt}=e^{At}e^{kt}\).</p> <p>Note: you cannot always split a matrix exponential this way. The two matrices must commute. Since scalar matrices always commute with every matrix of appropriate size, we can always use this sort of exponential shifty thing.</p> <p>The reason this is so useful is that it may be difficult to compute the matrix exponential of \(A\). But as it happens, there is always a value of \(k\) for any \(2\times2\)s that gives \(A+kI\) a nifty property which makes it easier to compute \(e^{(A+kI)t}\).</p> <p>In short, if its easy to calculate \(e^{(A+kI)t}\), then its easy to calculate \(e^{At}\).</p> <h2 id="one-defective-eigenvalue">One Defective Eigenvalue</h2> <p>This one happens to be the simplest, in my opinion.</p> <p>So let us say that \(A\) is a \(2\times2\) matrix with one defective eigenvalue \(\lambda\). Then its characteristic polynomial is \(s^2-2\lambda s+\lambda^2=0\). Therefore, by the Cayley-Hamilton theorem,</p> \[A^2-2\lambda A+\lambda^2I=0\] \[(A-\lambda I)^2=0\] <p>This makes it easy to directly calculate the matrix exponential of \(A-\lambda I\) by plugging it directly into the series \(I+tX+\frac{t^2}{2!}X^2+\ldots\)</p> \[e^{(A-\lambda I)t}=I+t(A-\lambda I)+(A-\lambda I)^2\left(\frac{t^2}{2!}I+\ldots\right)\] \[e^{(A-\lambda I)t}=I+t(A-\lambda I)\] \[e^{\lambda t}e^{(A-\lambda I)t}=e^{\lambda t}\bigg(I+t(A-\lambda I)\bigg)\] \[e^{At}=e^{\lambda t}\bigg(I+t(A-\lambda I)\bigg)\quad\blacksquare\] <p>And that’s it!</p> <hr> <h2 id="complex-conjugate-eigenvalues">Complex Conjugate Eigenvalues</h2> <p>If \(A\in\mathbb{R}^{2\times2}\) has complex conjugate eigenvalues \(a\pm bi\), then the characteristic polynomial of \(A\) is \(t^2-2at+a^2+b^2=0\). Therefore, again by the Cayley-Hamilton theorem,</p> \[A^2-2aA+(a^2+b^2)I=0\] \[(A-aI)^2+b^2I=0\] \[(A-aI)^2=-b^2I\] <p>For convenience, let us call \(B=A-aI\) (note that this implies that \(e^{At}=e^{at}e^{Bt}\))</p> \[B^2=-b^2I\] <p>Now this is interesting. Calculating even power of \(B\) will have the <em>nifty</em> formula \(B^{2k}=(-b^2)^kI\)</p> \[B^{2k}=(-1)b^{2k}I\] <p>To get odd powers, we just multiply by \(B\).</p> \[B^{2k+1}=(-1)b^{2k}B\] <p>Sweet. So if we split the series of \(e^{Bt}\) into even and odd powers, we will just get the power series of a function multiplying \(I\), and the power series of a function multiplying \(A\). That is to say, no need to compute matrix powers at all!</p> \[e^{Bt}=\sum_{k=0}^\infty\frac{(-1)^kb^{2k}t^{2k}}{(2k)!}I+\sum_{k=0}^\infty\frac{(-1)^kb^{2k}t^{2k+1}}{(2k+1)!}B\] <p>We can combine the all of the \(b\) terms with the \(t\) terms using \(b^{2k}=\frac{b^{2k+1}}{b}\), and dividing out/multiplying a \(b\) into the sum.</p> \[e^{Bt}=\sum_{k=0}^\infty\frac{(-1)^k(bt)^{2k}}{(2k)!}I+\frac{1}{b}\sum_{k=0}^\infty\frac{(-1)^k(bt)^{2k+1}}{(2k+1)!}B\] <p>And look at that! It’s our old pals, \(\cos\) and \(\sin\).</p> \[e^{Bt}=\cos(bt)I+\frac{1}{b}\sin(bt)B\] <p>Now we’re good to go in writing things in terms of \(A\).</p> \[e^{(A-aI)t}=\cos(bt)+\frac{1}{b}\sin(bt)(A-aI)\] <p>Now we need only multiply by \(e^{at}\) to obtain \(e^{At}\) in \eqref{complex1}.</p> \[e^{At}=e^{at}\left(\cos(bt)+\frac{1}{b}\sin(bt)(A-aI)\right)\] <p>We get \eqref{complex2} when we factor out \(\frac{1}{b}\).</p> \[e^{At}=\frac{e^{at}}{b}\bigg(b\cos(bt)I+\sin(bt)(A-aI)\bigg)\quad\blacksquare\] <hr> <p>Now we can do something very similar to get \eqref{0trace}.</p> <h2 id="zero-trace-and-negative-determinant">Zero trace and negative determinant</h2> <p>If \(A\) has a trace of zero and a real determinant strictly less than zero, (let us say \(\det(A)=-\lambda^2\)), then the characteristic polynomial is</p> \[A^2-\lambda^2I=0\] \[A^2=\lambda^2I\] <p>You can repeat the process detailed above, and it will actually be easier since there’s no negative to worry about. But you will get to</p> \[e^{At}=\sum_{k=0}^\infty\frac{(-1)^k\lambda^{2k}t^{2k}}{(2k)!}I+\sum_{k=0}^\infty\frac{(-1)^k\lambda^{2k}t^{2k+1}}{(2k+1)!}A\] \[e^{At}=\sum_{k=0}^\infty\frac{(\lambda t)^{2k}}{(2k)!}I+\frac{1}{\lambda}\sum_{k=0}^\infty\frac{(\lambda t)^{2k+1}}{(2k+1)!}A\] <p>And this time we have our acquaintances who we had a class with a while back and we kind of forgot their names but they remember ours so things are a bit awkward. That being \(\cosh\) and \(\sinh\), naturally.</p> \[e^{At}=\cosh(\lambda t)I+\frac{1}{\lambda}\sinh(\lambda t)A\] <p>Placing the \(\sinh\) in the numerator gives us \eqref{0trace}.</p> <h2 id="one-nondefective-eigenvalue">One Nondefective Eigenvalue</h2> <p>This only occurs when \(A\) is a scalar matrix, but it’s also technically a case so it’s worth mentioning. So let’s say that \(A=\lambda I\). Then</p> \[e^{At}=\sum_{n=0}^\infty \frac{(\lambda tI)^n}{n!}\] \[e^{At}=\sum_{n=0}^\infty \frac{(\lambda t)^n}{n!}I^n\] \[e^{At}=\left(\sum_{n=0}^\infty \frac{(\lambda t)^n}{n!}\right)I=e^{\lambda t}I\] <p>Thus, \eqref{scalar}</p> \[e^{\lambda It}=e^{\lambda t}I\] <h2 id="rank-1-matrix">Rank 1 matrix</h2> <p>Now, if \(A\) has a rank of one, and does not have a trace of zero, there is a nice way to obtain \(e^{At}\), and this works for any rank one \(n\times n\) matrix! The first key fact is to observe that if \(A\) is rank one, then</p> \[A^2=\operatorname{tr}(A)A\] <p>You can prove this with the Cayley-Hamilton theorem! Anyway, this also implies that for \(n\geq1\),</p> \[A^n=\operatorname{tr}(A)^{n-1}A\] <p>We may then compute \(e^{At}\).</p> \[e^{At}=I+\sum_{n=1}^\infty\frac{\operatorname{tr}(A)^{n-1}t^n}{n!}A\] \[e^{At}=I+\frac{1}{\operatorname{tr}(A)}\sum_{n=1}^\infty\frac{(\operatorname{tr}(A)t)^n}{n!}A\] \[e^{At}=I+\frac{1}{\operatorname{tr}(A)}\bigg(e^{\operatorname{tr}(A)t}-1\bigg)A\] <p>Which is \eqref{formsingularb}. You can rearrange the terms to get \eqref{formsingular}.</p> \[e^{At}=\frac{e^{\operatorname{tr}(A)t}A-(A-\operatorname{tr}(A)I)}{\operatorname{tr}(A)}\] <h2 id="distinct-eigenvalues">Distinct Eigenvalues</h2> <p>Now you, the nonexistent hypothetical reader, may be thinking to yourself, “Hey, nix, why are you doing the most common case <em>last</em>?” The reason is that the formulas… kind of suck? These ones are just not as good.</p> <p>So for this, we’re assuming that \(A\) does not have an eigenvalue of zero. Then our nifty trick for this is to subtract a scalar matrix such that it <em>is</em> singular. If we suppose \(A\) has eigenvalues \(\lambda_1,\lambda_2\), then, by definition of an eigenvalue, \(A-\lambda_1I\) is singular. Thus, we may use \eqref{formsingular}.</p> <p>Again for convinience, we denote \(B=A-\lambda_1I\implies e^{At}=e^{\lambda_1t}e^{Bt}\). One can also verify that the trace of \(B\) is \(\lambda_2-\lambda_1\). Hence,</p> \[e^{Bt}=\frac{e^{(\lambda_2-\lambda_1)t}B-(B-(\lambda_2-\lambda_1)I)}{\lambda_2-\lambda_1}\] <p>There’s not a lot to simplify, but observe that \(B-(\lambda_2-\lambda_1)I=A-\lambda_2I\).</p> \[e^{Bt}=\frac{e^{(\lambda_2-\lambda_1)t}(A-\lambda_1I)-(A-\lambda_2I)}{\lambda_2-\lambda_1}\] <p>And thus we get \eqref{distinct}</p> \[e^{At}=\frac{e^{\lambda_2t}(A-\lambda_1I)-e^{\lambda_1t}(A-\lambda_2I)}{\lambda_2-\lambda_1}\] <p>You can get \eqref{distinct2} by isolating \(A\) and \(I\).</p> <hr> <h2 id="another-approach">Another approach</h2> <p>You may have noticed that so far every formula can be rearranged to be in the form</p> \[e^{At}=x_1(t)I+x_2(t)A\] <p>And, in fact, yes. This will be true for any \(2\times2\) matrix. More generally, for any \(n\times n\) matrix \(A\), you can write \(e^{At}\) in the form</p> \[e^{At}=x_1(t)I+\ldots+x_n(t)A^{n-1}\] <p>See <a href="../matrixexpwde" target="_blank">this post</a> to learn more about that… But I’m going to present <em>another</em> kind of similar and totally equivalent method here.</p> <p>Now, there are a few ways you can go about this. One way is to get a first order initial value problem for which \(e^{At}\) is the solution.</p> \[\Phi'=A\Phi, \quad \Phi(0)=I\] \[x_1'(t)I+x_2'(t)A=x_1(t)A+x_2(t)A^2\] <p>Using Cayley-Hamilton, if the characteristic polynomial of \(A\) is \(t^2-pt+q=0\), then \(A^2=pA-qI\).</p> \[x_1'(t)I+x_2'(t)A=x_1(t)A+x_2(t)(pA-qI)\] <p>Setting the terms on \(I\) and \(A\) equal gives,</p> \[\begin{array}{ccccc} x_1'&amp;=&amp;&amp;&amp;-qx_2\\ x_2'&amp;=&amp;x_1&amp;+&amp;px_2 \end{array}\] \[\textbf{x}'= \begin{pmatrix} 0&amp;-q\\1&amp;p \end{pmatrix} \textbf{x},\quad \textbf{x}(0)= \begin{pmatrix} 1\\0 \end{pmatrix}\] <p>Now, we can use the fact that \(p=\operatorname{tr}(A)\) and \(q=\det(A)\).</p> \[\textbf{x}'= \begin{pmatrix} 0&amp;-\det(A)\\1&amp;\operatorname{tr}(A) \end{pmatrix} \textbf{x},\quad \textbf{x}(0)= \begin{pmatrix} 1\\0 \end{pmatrix}\] <p>A neat observation is that this coefficient matrix has the same trace and determinant as \(A\). It is, in fact, similar to \(A\). So we already know the eigenvalues going in (assuming you found them before).</p> <p>We can use the fact that \(\operatorname{tr}(A)=\lambda_1+\lambda_2\) and \(\det(A)=\lambda_1\lambda_2\).</p> \[\begin{equation}\label{1stsys} \textbf{x}'= \begin{pmatrix} 0&amp;-\lambda_1\lambda_2\\1&amp;\lambda_1+\lambda_2 \end{pmatrix} \textbf{x},\quad \textbf{x}(0)= \begin{pmatrix} 1\\0 \end{pmatrix} \end{equation}\] <p>And at this point, we have an initial value problem, and we can solve for \(x_1,x_2\) by finding the eigenvectors.</p> <p>If you feel inclined, I do recommend trying to solve \eqref{1stsys} for all the different cases of eigenvalues! I found it satisfying to get the same formulas derived above.</p> <hr> <p><em>Another</em> another option is to convert \eqref{1stsys} to a linear second order differential equation.</p> \[x_1=x_2'-(\lambda_1+\lambda_2)x_2\] \[\implies x_1'=x_2''-(\lambda_1+\lambda_2)x_2'=-\lambda_1\lambda_2x_2\] \[x_2''-(\lambda_1+\lambda_2)x_2'+\lambda_1\lambda_2x_2=0\] <p>ᵒʰ ʰᵉʸ ˡᵒᵒᵏ ⁱᵗ’ˢ ᵗʰᵉ ᶜʰᵃʳᵃᶜᵗᵉʳⁱˢᵗⁱᶜ ᵖᵒˡʸⁿᵒᵐⁱᵃˡ</p> <p>Then you could solve it like a normal initial value problem. If you want, I guess.</p> <h1 id="applications">Applications</h1> <p>For ways to apply matrix exponentials to solve \(2\times2\) systems of differential equations, check out <a href="../firstordersystemsquick" target="_blank">this post</a> on solving them like a baller.</p> <p>It’s also possible to find very similar formulas for \(A^n\) using these methods. The formulas for the \(2\times2\) cases are <em>extremely</em> similar to the formulas derived here.</p> <p>For example, if \(A\in\mathbb{R}^{2\times2}\) has complex eigenvalues \(a\pm bi=re^{\pm i\theta}\), then, as mentioned above</p> \[e^{At}=e^{at}\left(\cos(bt)I+\frac{\sin(bt)}{b}\bigg(A-aI\bigg)\right)\] <p>But in addition,</p> \[A^n=r^n\left(\cos(n\theta)I+\frac{\sin(n\theta)}{b}\bigg(A-aI\bigg)\right)\] <p>The method discussed in <a href="#another-approach">Another Approach</a> is my preferred choice for finding these formulas. But the system is not a differential equation, and instead a discrete analog.</p> \[\textbf{x}(n+1)=A\textbf{x}(n)\] <p>And while \(e^{At}\textbf{x}(0)\) is the solution to \(\textbf{x}'=A\textbf{x}\), the solution to the equation above is \(A^n\textbf{x}(0)\)</p> <p>I may choose to write a post about these in the future… EDIT: <a href="../discretesystems" target="_blank">I totally did. I couldn’t wait.</a></p> <h2 id="closing-remarks">Closing Remarks</h2> <p>I enjoyed verifying that taking the derivative of these formulas is indeed the same as multiplying by \(A\).</p> \[\begin{equation} \frac{d}{dt}e^{At}=Ae^{At} \end{equation}\] <p>Possibly even more fun than that is verifying that substituting \(-t\) for \(t\) does in fact give the inverse.</p> \[\begin{equation} e^{At}e^{-At}=I\implies e^{-At}=\left(e^{At}\right)^{-1} \end{equation}\] </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 02, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>