---
layout: distill
title: Jordan Canonical Form Made Easier
date: 2025-6-21
description: not easy. just like... easier...
comments: true
importance: 3
tags:
category: linear-algebra
authors:  
  - name: Taylor F.
    url: ""
    affiliations:
      name: None
toc:
  - name: The Abstract Algebra
    subsections:
      - name: The Structure Theorem
  - name: c2
---

Okay, so Jordan Canonical Form is a very strange topic. It's usually relegated to advanced linear algebra (which it is), but it's also presented in a very strange way. You see, JCF is actually part of a much deeper result found in Abstract Algebra! Specifically: Ring Theory about finitely generated modules over a PID (principal ideal domain). That's a lot of words!

The idea is that finitely generated Modules over a PID have a very predictable and clear structure. And we can actually identify every linear operator on some vector space $$V$$ (over the field $$F$$) with a finitely generated torsion module $$V$$ over the ring $$F[x]$$ (all polynomials with coefficients in $$F$$ with variable $$x$$).

Now, you're probably thinking, "Taylor, can you just explain Jordan forms to me?" And to that I say, "yeah, shortly". But the idea I really want to get across is that the *reason* Jordan forms are so darn confusing and sound like the ravings of a lunatic is *because* it's actually an extension of a much deeper result in Abstract Algebra. And, I think, with that context, things just come out of nowhere and there's no rhyme or reason to it. So my plan is to set the groundwork by briefly explaining some of the abstract algebra, effectively offloading the existence of the Jordan form, so that we can focus on using our knowledge of [change of basis](../changeofbasis/){:target="_blank"} to actually *find* the Jordan form.

## The Abstract Algebra

Basically, linear algebra is done on vector spaces which are over a *field*. A field is stuff that can be added and multiplied, and all the nonzero stuff can be divided out (like the real numbers or complex numbers). A ring is like a field except you can't always divide stuff (and multiplication isn't always commutative). Rings are a little weirder (and more interesting in my opinion!). 

An ideal of a commutative ring is kind of like a subspace for a ring: closed under addition (by other things in the ideal) and multiplication (by anything else in the ring). The even numbers are an example of an ideal of the ring of integer. And a PID is just a commutative ring where every ideal is generated by one element (like every even number is a multiple of 2). For our purposes, we just need to understand the PID $$F[x]$$.

$$F[x]$$ is the ring of single variable polynomials (in $$x$$) with coefficients in the field $$F$$. This is, in fact, a PID! The proof is not really relevant, but it comes down to the existence of a division algorithm for polynomials.

You can do linear algebra over a ring instead of a field, and then we call it a module. Without all the nice field properties, modules can be SUPER weird. For example, some modules can have linearly independent spanning (generating) sets of ANY size. That is, the same module has a linearly independent generating set of size 1 and also of size 2901457!. But I'm getting too off topic (point is, you should study modules if you like linear algebra!).

We're specifically going to talk about the structure of finitely generated *torsion* modules over $$F[x]$$. What does that mean? Well, basically, we're going to assume that we have an operator $$T$$ on $$V$$ (finite dimensional). Then the module $$V$$ over $$F[x]$$ is basically kind of like a vector space where we have vectors $$v$$ but our scalars are in $$F[x]$$. And we define "scalar multiplication" as

$$
\begin{equation}
p(x)\cdot v=p(T)v
\end{equation}
$$

We are going to assume $$T$$ has a minimal polynomial (we'll call $$m(x)$$. note $$m$$ is nonzero), then $$m(T)=0$$, so $$m(x)\cdot v=0$$ for all $$v$$. Remember that the minimal polynomial is just the polynomial of least degree such that $$m(T)=0$$. If it sounds weird that *any* polynomial with that property should exist, the Cayley Hamilton theorem tells us that the characteristic polynomial does have that property, in fact! The minimal polynomial is always going to be something that divides the characteristic polynomial.
Anyway, since we have a nonzero scalar (polynomial $$m$$) that multiplies ALL vectors to $$0$$, we say this is a "torsion module".

### The Structure Theorem

We're just going to write the structure theorem for our specific case of $$V$$ as a $$F[x]$$ torsion module. Since $$V$$ is finite dimensional (meaning it has a finite basis), then it's finitely generated. This means the module can be decomposed as

$$V\cong F[x]/(p_1(x))\oplus\ldots\oplus F[x]/(p_g(x))$$

Meaning $$V$$ can be decomposed as being isomorphic to separate cyclic submodules $$F[x]/(p_i(x))$$, but what are they? Well, for one thing, we know they must divide the minimal polynomial. But it turns out, one can show that they will be decomposed into

$$V\cong \bigoplus F[x]/(x-\lambda_i)^{k_i}$$

where $$\lambda_1,\ldots,\lambda_s$$ are the eigenvalues of $$T$$. And it turns out each of these submodules $$F[x]/(x-\lambda_i)^{k_i}$$ correspond to Jordan blocks. More specifically, these make up the generalized eigenspaces.

Why is this intuitive? Well, it isn't, really. But I can try my best to give you a sense.

Eigenspaces are invariant and independent. It's elementary to show that eigenvectors with different eigenvalues are linearly independent. So the idea that we can decompose $$V$$ into a direct sum of eigenspaces isn't entirely radical. This is what we do when $$T$$ is diagonalizable. And it turns out, the diagonalizable case is when all the $$k_i$$'s are $$1$$. It's only when $$V$$ gets decomposed into a term like $$F[x]/(x-\lambda)^2$$ (we call this a non-simple submodule) that it becomes nondiagonalizable. 

Okay, but why? What the heck is $$F[x]/(q(x))$$ anyway? Basically, it's the set of polynomials of the form

$$r(x)+(q(x))$$

where we define 

And what does it mean to have a subspace corresponding to $$F[x]/(x-\lambda)^2$$ or $$F[x]/(x-\lambda)^k$$ ($$k>1$$)? It means that all the vectors in that space are annihilated by $$(x-\lambda)^k$$. That is:

$$(T-\lambda I)^kv=0$$

for all $$v$$ in the submodule. But the submodule is cyclic. So we have some generator $$v_k$$.

This gives us a new way to think about nondiagonalizability.

$$
\begin{equation}
\begin{pmatrix}1\\1\end{pmatrix}
\end{equation}
$$

[hyperlink](../eigentricks/){:target="_blank"}

[Section c2](#c2)


[hyperlink](https://youtu.be/g2VYkc-MtC8?si=7KvPWXyo4wPEhZ7f){:target="_blank"}
